{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"welcome/","text":"Welcome \u00b6 Hello \ud83d\udc4b This is the Turing Commons\u2014a home for resources and tools to help you reflect, discuss, and take responsibility for the design, development, and use of data-driven technologies. Confused by that last sentence? Don't worry you're still in the right place. We have a range of resources for those curious about the ethical and responsible use of data, regardless of whether you're a curious citizen looking to have greater control over local matters, or a data scientist looking to take more responsibility over the consequences of your research. This site is a living resource, however, which means the content is constantly being updated, revised, and refined. At present, the site is a home for several guidebooks, which have been designed for specific training courses we are in the process of delivering. These guidebooks are themselves being updated and added to, so may be in different states of completeness and readiness. You can access them via the links below, or using the menu on the left. Responsible Research and Innovation (RRI) Public Engagement of Data Science and AI (PED) AI Ethics & Governance (AEG) Citizen's Guide to Data: Ethical, Social and Legal Issues Contributing \u00b6 If you'd like to get involved and help contribute to these resources, you can either: Contribute on GitHub: Comment on issues , join a discussion, collaborate on an ongoing task and exchange your thoughts with others. Can't find your idea being discussed anywhere? Open a new issue ! Just want to get in touch? Feel free to send us an email: cburr@turing.ac.uk .","title":"Welcome"},{"location":"welcome/#welcome","text":"Hello \ud83d\udc4b This is the Turing Commons\u2014a home for resources and tools to help you reflect, discuss, and take responsibility for the design, development, and use of data-driven technologies. Confused by that last sentence? Don't worry you're still in the right place. We have a range of resources for those curious about the ethical and responsible use of data, regardless of whether you're a curious citizen looking to have greater control over local matters, or a data scientist looking to take more responsibility over the consequences of your research. This site is a living resource, however, which means the content is constantly being updated, revised, and refined. At present, the site is a home for several guidebooks, which have been designed for specific training courses we are in the process of delivering. These guidebooks are themselves being updated and added to, so may be in different states of completeness and readiness. You can access them via the links below, or using the menu on the left. Responsible Research and Innovation (RRI) Public Engagement of Data Science and AI (PED) AI Ethics & Governance (AEG) Citizen's Guide to Data: Ethical, Social and Legal Issues","title":"Welcome"},{"location":"welcome/#contributing","text":"If you'd like to get involved and help contribute to these resources, you can either: Contribute on GitHub: Comment on issues , join a discussion, collaborate on an ongoing task and exchange your thoughts with others. Can't find your idea being discussed anywhere? Open a new issue ! Just want to get in touch? Feel free to send us an email: cburr@turing.ac.uk .","title":"Contributing"},{"location":"aeg/","text":"About this Course \u00b6 This course is designed to help you understand the fundamentals of AI Ethcis and Governance. The course begings with an introduction to metaethcis and normative theories. It then follows with the practical ways AI systems can produce diverse harms to individuals, society, and even the biosphere, as well as the values that should be upheld when thinking about AI ethics. The course then goes into a deeper dive on the following topics: AI Sustainability through Stakeholder engagement and impact assessment, AI fairness and bias mitigation, accountability and governance, explainability and transparency, and the CARE & Act principles. Thoughout the course we will have time for Q&A, group discussions, case studies, and structured activities to further the discussion and understanding of these concepts. Who is this Guidebook For? \u00b6 Primarily, this guidebook is for researchers with an active interest in the ethics and governance of data science and AI. This doesn't mean you have to be a data scientist or develop machine learning algorithms. You could also be an ethicist, sociologist, or someone with an interest in law and public policy. This course has practical, and sometimes hands-on activities that are designed to a) encourage critical reflection and b) help you build practical understanding of the processes associated with effective and responsible engagement with AI systems. While they can be carried out as part of individual and self-directed learning, they are most suited to group discussion. Learning Objectives \u00b6 This guidebook has the following learning objectives: Get familiar with some key concepts in practical ethics. In particular, understand the metaethical motivation behind our course, as well as the main families of normative ethical theories. Understand the different kinds of harms that AI systems can create. Explore the different values that underpin the thinking and reflection behind issues in AI ethics. Understand the importance of AI sustainability and anticipatory reflection through a comprehension of the stakeholder engagement process (SEP) and stakeholder impact assessment (SIA). Explore some of the ethical issues around AI systems, such as: fairness & bias mitigation, explainability & transparency, and accountability & governance. Learn the importance and use of the CARE & ACT principles in developing AI systems where ethics is considered throughout the process and in an iterative manner. Table of Contents \u00b6 :material-chat-question:{ .lg .middle } Introduction to Practical Ethics This chapter looks at foundational concepts of practical ethics, through two broad-brush introductions to: (i) metaethics and (ii) normative ethical theories. :octicons-arrow-right-24: Go to chapter :material-scale-balance:{ .lg .middle } AI Harms and Values This chapter looks at the different kinds of harms AI systems may cause, as well as the values that should be used as goals and objectives when thinking about the ethics of AI systems. :octicons-arrow-right-24: Go to chapter :fontawesome-solid-people-group:{ .lg .middle } AI Sustainability and Stakeholder Engagement This chapter introduces the concepts of AI Sustainability and the importance of anticipatory reflection. We will develop sustainability by looking at the Stakeholder Engagement Process (SEP) and the Stakeholder Impact Assessment (SIA). :octicons-arrow-right-24: Go to chapter :material-chat-processing:{ .lg .middle } Fairness, Bias Mitigation, Accountability, and Governance This chapter addresses various issues that arise from the use of AI systems: AI fairness and bias mitigation, as well as accountability and governance. :octicons-arrow-right-24: Go to chapter :fontawesome-solid-handshake:{ .lg .middle } Transparency & Explainability and CARE & ACT Principles The concluding chapter starts with the importance of transparency and explainability in AI systems. We then go through what we call CARE & ACT Principles: (i) consider context, (ii) anticipate impacts, (iii) reflect on purpose, positionality, and power, (iv) engage inclusively, and (v) act responsibly and transparently. :octicons-arrow-right-24: Go to chapter","title":"(AEG) About this course"},{"location":"aeg/#about-this-course","text":"This course is designed to help you understand the fundamentals of AI Ethcis and Governance. The course begings with an introduction to metaethcis and normative theories. It then follows with the practical ways AI systems can produce diverse harms to individuals, society, and even the biosphere, as well as the values that should be upheld when thinking about AI ethics. The course then goes into a deeper dive on the following topics: AI Sustainability through Stakeholder engagement and impact assessment, AI fairness and bias mitigation, accountability and governance, explainability and transparency, and the CARE & Act principles. Thoughout the course we will have time for Q&A, group discussions, case studies, and structured activities to further the discussion and understanding of these concepts.","title":"About this Course"},{"location":"aeg/#who-is-this-guidebook-for","text":"Primarily, this guidebook is for researchers with an active interest in the ethics and governance of data science and AI. This doesn't mean you have to be a data scientist or develop machine learning algorithms. You could also be an ethicist, sociologist, or someone with an interest in law and public policy. This course has practical, and sometimes hands-on activities that are designed to a) encourage critical reflection and b) help you build practical understanding of the processes associated with effective and responsible engagement with AI systems. While they can be carried out as part of individual and self-directed learning, they are most suited to group discussion.","title":"Who is this Guidebook For?"},{"location":"aeg/#learning-objectives","text":"This guidebook has the following learning objectives: Get familiar with some key concepts in practical ethics. In particular, understand the metaethical motivation behind our course, as well as the main families of normative ethical theories. Understand the different kinds of harms that AI systems can create. Explore the different values that underpin the thinking and reflection behind issues in AI ethics. Understand the importance of AI sustainability and anticipatory reflection through a comprehension of the stakeholder engagement process (SEP) and stakeholder impact assessment (SIA). Explore some of the ethical issues around AI systems, such as: fairness & bias mitigation, explainability & transparency, and accountability & governance. Learn the importance and use of the CARE & ACT principles in developing AI systems where ethics is considered throughout the process and in an iterative manner.","title":"Learning Objectives"},{"location":"aeg/#table-of-contents","text":":material-chat-question:{ .lg .middle } Introduction to Practical Ethics This chapter looks at foundational concepts of practical ethics, through two broad-brush introductions to: (i) metaethics and (ii) normative ethical theories. :octicons-arrow-right-24: Go to chapter :material-scale-balance:{ .lg .middle } AI Harms and Values This chapter looks at the different kinds of harms AI systems may cause, as well as the values that should be used as goals and objectives when thinking about the ethics of AI systems. :octicons-arrow-right-24: Go to chapter :fontawesome-solid-people-group:{ .lg .middle } AI Sustainability and Stakeholder Engagement This chapter introduces the concepts of AI Sustainability and the importance of anticipatory reflection. We will develop sustainability by looking at the Stakeholder Engagement Process (SEP) and the Stakeholder Impact Assessment (SIA). :octicons-arrow-right-24: Go to chapter :material-chat-processing:{ .lg .middle } Fairness, Bias Mitigation, Accountability, and Governance This chapter addresses various issues that arise from the use of AI systems: AI fairness and bias mitigation, as well as accountability and governance. :octicons-arrow-right-24: Go to chapter :fontawesome-solid-handshake:{ .lg .middle } Transparency & Explainability and CARE & ACT Principles The concluding chapter starts with the importance of transparency and explainability in AI systems. We then go through what we call CARE & ACT Principles: (i) consider context, (ii) anticipate impacts, (iii) reflect on purpose, positionality, and power, (iv) engage inclusively, and (v) act responsibly and transparently. :octicons-arrow-right-24: Go to chapter","title":"Table of Contents"},{"location":"aeg/chapter1/","text":"AI Ethics: Introduction to Practical Ethics \u00b6 Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk)) Chapter Outline \u00b6 Introduction to Metaethics Introduction to Normative Ethical Theories Chapter Summary This chapter introduces the basic theoretical building blocks for learning AI ethics. We start with a brief metaethical motivation, looking at different positions in metaethics and reflecting on how these affect the way we do practical ethics. Additionally, we will delve into the some of the main normative theories in practical ethics: consequentialist theories, deontological theories, virtue ethics, and biocentric ethics. These concepts will be useful to paint the ethical landscape against which the sub-field of AI (practical) ethics has developed. Learning Objectives In this chapter, you will: Learn what practical ethics is. Familiarise yourself with the main positions in metaethics, as well as learn about why we favour a procedural approach to ethics. Learn about some of the main normative ethical theories: consequentialist theories, deontological theories, virtue ethics, and biocentric ethics. Reflect on how these concepts in practical ethics create the wider ethical landscape on which AI ethics develops.","title":"Introduction"},{"location":"aeg/chapter1/#ai-ethics-introduction-to-practical-ethics","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))","title":"AI Ethics: Introduction to Practical Ethics"},{"location":"aeg/chapter1/#chapter-outline","text":"Introduction to Metaethics Introduction to Normative Ethical Theories Chapter Summary This chapter introduces the basic theoretical building blocks for learning AI ethics. We start with a brief metaethical motivation, looking at different positions in metaethics and reflecting on how these affect the way we do practical ethics. Additionally, we will delve into the some of the main normative theories in practical ethics: consequentialist theories, deontological theories, virtue ethics, and biocentric ethics. These concepts will be useful to paint the ethical landscape against which the sub-field of AI (practical) ethics has developed. Learning Objectives In this chapter, you will: Learn what practical ethics is. Familiarise yourself with the main positions in metaethics, as well as learn about why we favour a procedural approach to ethics. Learn about some of the main normative ethical theories: consequentialist theories, deontological theories, virtue ethics, and biocentric ethics. Reflect on how these concepts in practical ethics create the wider ethical landscape on which AI ethics develops.","title":"Chapter Outline"},{"location":"aeg/chapter1/metaethics/","text":"Introduction to Metaethics \u00b6 What is practical ethics? \u00b6 Practical ethics is a branch of philosophy, in particular a branch of ethics concerned with examining the aims and principles of moral behaviour and applying them to various problems of our everyday lives or real-world scenarios in general.[@uehiro] In this course, we will look at issues in practical ethics which arise from our use of AI systems and the prevalence of data in our lives. It is important to note that these harms are not exclusive to the use of AI systems, but can be characteristic of them. To do so, we will introduce some key concepts in metaethics and some of the most relevant ethical theories in use today. What is Metaethics? \u00b6 Metaethics, as its name suggest, attempts to understand the various aspects (ontological, epistemological, semantic, and psychological) that make up the presuppositions and commitments of moral thought, talk and practice.[@sep-metaethics2014] What does this mean? Metaethics deals with a variety of quite complex, yet relevant questions such as the following: Is there anything such an objective moral truth? Or is morality more a matter of taste? Are all cultural moral standards equally valid or true? Are there moral facts? And if they exist, what is their origin? How are moral facts (if they exist) related to other kinds of facts about our world?[@sep-metaethics2014] Clearly, none of these questions will have straightforward or easy answers, and we will not be dealing with them extensively in this course. However, in this brief introduction to practical ethics, we will look at different ways that some of these questions have been tried to answer. Quote What is morality? Are there moral facts about the world? If there are, can we know them? If there are not, how do we decide on right and wrong in our actions? 1. Universalist Perspectives \u00b6 1.1 Moral realism \u00b6 Moral realists, roughly speaking, accept the following two statements: There are such things as moral facts or properties, and; These are independent of human attitudes towards them (this is called attitude independence). In practice, this means that moral realists believe that objective moral facts do exist, and there is only one \"set\" of these facts. Objective moral truths also imply that things like subjectivity, or culture are not relevant when evaluating whether an act is good or bad. Moral realism is, in and of itself, neutral on what it is exactly that makes something good or bad; it just is prepared to state that moral facts exist . Of course, this does not mean that subjectivity or culture play no role in what people think is good or bad. However, the moral realist is prepared to say that those differences, and any human attitude over what makes something good or bad, is irrelevant when evaluating moral facts. That is, the reason why some things are good or bad do not hinge on whether people, culture, or society evaluate them as good or bad. The existence of objective moral truths clearly begs the question: where do these facts come from? This is what is known as the 'grounding problem'. One way of summing up moral realism is to say that epistemically they believe that \"there are moral facts, in the same way that there are planets and spoons\".[@rachels2019] 1.2 Procedular ethics \u00b6 In procedural ethics, moral validity is bound by practices of giving and asking for reasons. In this way, a moral claim that is justified is one that can convince someone else by the \u201cunforced force\u201d of the better argument. Legitimate moral decision-making thus requires persuasion through rational justification and compelling argumentation. This means that procedular ethics begins by trying to reconstruct the requirements for rationality. These requirements include assuming an impartial perspective, which considers the interests of all affected by an action equally, and making universalizable moral judgements that are applicable in all like situations. Does this mean that procedural ethicists believe that moral facts exist? Yes and no. While processes of rational justification are supposed to yield a basis for valid moral claims, many procedural ethicists abstain from making substantive claims about moral reality. In their view, the proper role of moral theory is not to posit the fixed, and mind- or language-independent moral values and properties of the universe, but rather to facilitate and to normatively justify the continuation of ongoing rational dialogue. 2. Non-universalist Perspectives \u00b6 2.1 Cultural Relativism \u00b6 Cultural Relativism says, in effect, that there is no such thing as universal truth in ethics; there are only the various cultural codes that derive from social approval, and nothing more. Cultural relativism challenges our belief in the objectivity and universality of moral truth.[@rachels2019] Values and moral beliefs form an interconnected and coherent whole in a given cultural field or system of meanings, but this symbolic whole is relative to the historical background of the group it is derived from. There are no objective moral standards that go beyond the locality of cultures, and hence, no universal standards that could help adjudicate between them. Some examples of claims made by cultural relativists are: Different societies have different moral codes, and the moral code of a society determines what is right within that society. There is no objective standard that can be used to judge one society\u2019s code as better than another\u2019s. There are no moral truths that hold for all people at all times. The moral code of our own society has no special status. It is arrogant for us to judge other cultures. We should always be tolerant of them. 2.2 Moral Subjectivism \u00b6 Moral subjectivists take relativism to a next level; instead of basing it on codes shared by whole cultures, they believe morality is an inherently individual matter. That is, what is morally right to do is up to the individual\u2019s desires, wants, tastes, or preferences. Therefore, in this view, moral language is not fact-stating language and thus does not contain mind-independent ethical truths. Instead, moral judgments are subjective expressions of feelings or attitudes, and so more like judgments of taste than statements of fact or rational justifications.","title":"Introduction to Metaethics"},{"location":"aeg/chapter1/metaethics/#introduction-to-metaethics","text":"","title":"Introduction to Metaethics"},{"location":"aeg/chapter1/metaethics/#what-is-practical-ethics","text":"Practical ethics is a branch of philosophy, in particular a branch of ethics concerned with examining the aims and principles of moral behaviour and applying them to various problems of our everyday lives or real-world scenarios in general.[@uehiro] In this course, we will look at issues in practical ethics which arise from our use of AI systems and the prevalence of data in our lives. It is important to note that these harms are not exclusive to the use of AI systems, but can be characteristic of them. To do so, we will introduce some key concepts in metaethics and some of the most relevant ethical theories in use today.","title":"What is practical ethics?"},{"location":"aeg/chapter1/metaethics/#what-is-metaethics","text":"Metaethics, as its name suggest, attempts to understand the various aspects (ontological, epistemological, semantic, and psychological) that make up the presuppositions and commitments of moral thought, talk and practice.[@sep-metaethics2014] What does this mean? Metaethics deals with a variety of quite complex, yet relevant questions such as the following: Is there anything such an objective moral truth? Or is morality more a matter of taste? Are all cultural moral standards equally valid or true? Are there moral facts? And if they exist, what is their origin? How are moral facts (if they exist) related to other kinds of facts about our world?[@sep-metaethics2014] Clearly, none of these questions will have straightforward or easy answers, and we will not be dealing with them extensively in this course. However, in this brief introduction to practical ethics, we will look at different ways that some of these questions have been tried to answer. Quote What is morality? Are there moral facts about the world? If there are, can we know them? If there are not, how do we decide on right and wrong in our actions?","title":"What is Metaethics?"},{"location":"aeg/chapter1/metaethics/#1-universalist-perspectives","text":"","title":"1. Universalist Perspectives"},{"location":"aeg/chapter1/metaethics/#11-moral-realism","text":"Moral realists, roughly speaking, accept the following two statements: There are such things as moral facts or properties, and; These are independent of human attitudes towards them (this is called attitude independence). In practice, this means that moral realists believe that objective moral facts do exist, and there is only one \"set\" of these facts. Objective moral truths also imply that things like subjectivity, or culture are not relevant when evaluating whether an act is good or bad. Moral realism is, in and of itself, neutral on what it is exactly that makes something good or bad; it just is prepared to state that moral facts exist . Of course, this does not mean that subjectivity or culture play no role in what people think is good or bad. However, the moral realist is prepared to say that those differences, and any human attitude over what makes something good or bad, is irrelevant when evaluating moral facts. That is, the reason why some things are good or bad do not hinge on whether people, culture, or society evaluate them as good or bad. The existence of objective moral truths clearly begs the question: where do these facts come from? This is what is known as the 'grounding problem'. One way of summing up moral realism is to say that epistemically they believe that \"there are moral facts, in the same way that there are planets and spoons\".[@rachels2019]","title":"1.1 Moral realism"},{"location":"aeg/chapter1/metaethics/#12-procedular-ethics","text":"In procedural ethics, moral validity is bound by practices of giving and asking for reasons. In this way, a moral claim that is justified is one that can convince someone else by the \u201cunforced force\u201d of the better argument. Legitimate moral decision-making thus requires persuasion through rational justification and compelling argumentation. This means that procedular ethics begins by trying to reconstruct the requirements for rationality. These requirements include assuming an impartial perspective, which considers the interests of all affected by an action equally, and making universalizable moral judgements that are applicable in all like situations. Does this mean that procedural ethicists believe that moral facts exist? Yes and no. While processes of rational justification are supposed to yield a basis for valid moral claims, many procedural ethicists abstain from making substantive claims about moral reality. In their view, the proper role of moral theory is not to posit the fixed, and mind- or language-independent moral values and properties of the universe, but rather to facilitate and to normatively justify the continuation of ongoing rational dialogue.","title":"1.2 Procedular ethics"},{"location":"aeg/chapter1/metaethics/#2-non-universalist-perspectives","text":"","title":"2. Non-universalist Perspectives"},{"location":"aeg/chapter1/metaethics/#21-cultural-relativism","text":"Cultural Relativism says, in effect, that there is no such thing as universal truth in ethics; there are only the various cultural codes that derive from social approval, and nothing more. Cultural relativism challenges our belief in the objectivity and universality of moral truth.[@rachels2019] Values and moral beliefs form an interconnected and coherent whole in a given cultural field or system of meanings, but this symbolic whole is relative to the historical background of the group it is derived from. There are no objective moral standards that go beyond the locality of cultures, and hence, no universal standards that could help adjudicate between them. Some examples of claims made by cultural relativists are: Different societies have different moral codes, and the moral code of a society determines what is right within that society. There is no objective standard that can be used to judge one society\u2019s code as better than another\u2019s. There are no moral truths that hold for all people at all times. The moral code of our own society has no special status. It is arrogant for us to judge other cultures. We should always be tolerant of them.","title":"2.1 Cultural Relativism"},{"location":"aeg/chapter1/metaethics/#22-moral-subjectivism","text":"Moral subjectivists take relativism to a next level; instead of basing it on codes shared by whole cultures, they believe morality is an inherently individual matter. That is, what is morally right to do is up to the individual\u2019s desires, wants, tastes, or preferences. Therefore, in this view, moral language is not fact-stating language and thus does not contain mind-independent ethical truths. Instead, moral judgments are subjective expressions of feelings or attitudes, and so more like judgments of taste than statements of fact or rational justifications.","title":"2.2 Moral Subjectivism"},{"location":"aeg/chapter1/normative/","text":"Introduction to Normative Ethical Theories \u00b6 In contrast with metaethics, which is concerned with the nature of ethics, normative ethics is concerned with the content of ethical theories. Normative ethical theories attempt to give an answer to the question of what makes someone ethical, or what makes certain actions morally permissible. Ethical theories are varied and they come in different flavours. Here we will group them into four families of theories. Consequentialist Theories \u00b6 Consequence-based theories are normative theories which, as the name suggests, define whether an action is morally permissible in terms of the consequences it brings about. In simple terms, they define morality as a matter of maximising the best possible consequences in any given situation. This then begs the question, what are the best consequences to be maximised? Consequentialists have given different answers to this question. One very influential flavour of consequentialism is utilitarianism, which specifies that the way we should rank consequences is by how much total (aggregate) utlity they bring about (where more utlity is better). 1 This means that when confronted with a decision, utilitarianism will tell you to choose whichever action maximises overall utility. This theory's practical impact is not to be discounted. It has served as the basis for classical economics and is widely taught to and applied by economists today. Other examples of consequentialist theories are hedonism, act conseuquentialism, and rule consequentialism. 2 In essence, what makes a normative ethical theory consequentialist is that, given a metric for what good consequences are (as well as a way of ranking better and worse consequences), it then states that in any situation, the morally aproppriate way to act is to choose the action (among the existing possibilities) that brings about the best consequence. Deontological Theories (or principles-based theories) \u00b6 Deontological theories in contrast, take a different approach. Morality is not to be measured by the consequences actions bring about, but instead morality dictates that one should follow a set of rules or principles regardless of what the consequences are. That is, the rightness of an action is determined by an agent\u2019s application of some universal standard, rule, or maxim of rightness irrespective of its consequences and independent of the interests or ends of the agent whose conduct the standard, rule, or maxim is guiding. The most famous proponent of deontological theories is Immanuel Kant, who was a firm believer in rule or duty-based morality, and who went as far as to define the maxim or principle to be followed in order to act morally which he called the Categorical Imperative. This rule roughly states that one must always act as if one\u2019s actions could be willed into a universal rule that everyone in society follows. In principle-based theories, morality is about following a set of rules or principles, and different theories will give different answers as to what principles one should act upon. Virtue Ethics \u00b6 Virtue ethics is not concerned with how people should act in a particular situation, but instead on what kind of people they are. Its roots can be found in Confucius and Mencius in the East and in Plato and Aristotle in the West. Instead of beginning with the question: \u201cWhat should I do?\u201d, it asks the question: \u201cWhat sort of person should I strive to become in order to live an ethical life?\u201d. Virtue ethicists emphasise that morality has to do with having a certain moral character which is achieved by cultivating virtues. A virtue is an excellent trait of character[@sep-ethics-virtue], and it comes in degrees. These traits may derive from natural tendencies, but they must also be nurtured. Under these theories, a person who acts in a kind or benevolent way (these would be the virtues) does not do so because doing so would maximise the outcome of her actions (as a consequentialist would), or because it is her duty to do so (as deontology would advise), but instead because acting in that way cultivates said virtues. The development of moral virtues through practice, discipline, and repetition is the purpose of the human form of life. For this ethical view to work, there must be some reserve of objective moral values available for agents to draw on to form virtuous habits and dispositions. Some of the virtues stressed in antiquity but also justified in modern secular ethics: wisdom (ability to exercise practical reason to determine right action); temperance (ability to remain cool-headed and guided by reason instead of emotion); courage (ability to confront danger boldly and with self-assurance); justice (ability to act impartially and with fairness). Bio-centric ethics \u00b6 Biocentric ethics attempts to justify moral responsibility and the rightness of action not in a human-centered way but in terms of the intrinsic value of all life forms and ecologies. It treats \u201cnature\u201d (the living biosphere) not as an instrument or resource available to be used to achieve the purposes of human industry, but rather as an entity that makes a moral claim on us. From a biocentric perspective, moral actions are those that preserve the flourishing and the diversity of all living beings and aim to secure the sustenance of the biosphere as a whole. For the purposes of the rest of the course we will focus mostly on the first two families of theories. Example Let's take a classic example to help draw the distinction between consequentialism and deontology. Suppose an assassin comes to your door and asks if you have seen your friend Peter. And suppose you are indeed hiding Peter in your house as he is fleeing this very assassin. Should you tell the assassin the truth, or should you lie and tell him you don't know where Peter is? From a consequentialist perspective, it is clear what one should do. Telling the truth will result in Peter dying and lying will result in him living. The first consequence is a lot worse than the second, so you should lie to the assassin. However, from a deontological perspective it is not so obvious. If morality is about duty, and one of our duties is to be honest, then perhaps we also have a moral duty to tell the truth, even in this case. 3 Most people would find the second conclusion morally abhorrent. This shows us that in some cases we seem to take a more consequentialist approach to ethics. However, the converse can also be the case. Example Imagine you are a surgeon and Peter, your patient, is peacefully recuperating after an appendicitis operation. He should shortly be back home. And imagine that five people in need of different organ transplants come into the hospital. If these people do not get a transplant soon they will die. Suppose Peter is a match for all of these people. A strict consequentialist doctor might decide that we should kill Peter in order to save the other five via organ transplants. After all the consequences of one life lost and five gained are a lot better than one life gained and five lost. Most people would find this conclusion morally abhorrent. A deontologist would reply that it is never permissible to use people as means to an end, since people are always ends in themselves. Therefore our duty to respect Peter's dignity and not use him as a means to saving the other five overrides any consequentialist considerations we might have. The question as to what exactly utility is and how one goes about measuring it poses all sorts of complications which are beyond the scope of this introduction. \u21a9 For a list of different kinds of consequentialist theories see the Stanford Encyclopedia of Philosophy's entry on Consequentialism[@sep-consequentialism]. \u21a9 There is a lot more nuance as to what a deontological theory would say in the example just described. However, this simplification helps illustrate a core difference between the two families of theories. \u21a9","title":"Introduction to Normative Theories"},{"location":"aeg/chapter1/normative/#introduction-to-normative-ethical-theories","text":"In contrast with metaethics, which is concerned with the nature of ethics, normative ethics is concerned with the content of ethical theories. Normative ethical theories attempt to give an answer to the question of what makes someone ethical, or what makes certain actions morally permissible. Ethical theories are varied and they come in different flavours. Here we will group them into four families of theories.","title":"Introduction to Normative Ethical Theories"},{"location":"aeg/chapter1/normative/#consequentialist-theories","text":"Consequence-based theories are normative theories which, as the name suggests, define whether an action is morally permissible in terms of the consequences it brings about. In simple terms, they define morality as a matter of maximising the best possible consequences in any given situation. This then begs the question, what are the best consequences to be maximised? Consequentialists have given different answers to this question. One very influential flavour of consequentialism is utilitarianism, which specifies that the way we should rank consequences is by how much total (aggregate) utlity they bring about (where more utlity is better). 1 This means that when confronted with a decision, utilitarianism will tell you to choose whichever action maximises overall utility. This theory's practical impact is not to be discounted. It has served as the basis for classical economics and is widely taught to and applied by economists today. Other examples of consequentialist theories are hedonism, act conseuquentialism, and rule consequentialism. 2 In essence, what makes a normative ethical theory consequentialist is that, given a metric for what good consequences are (as well as a way of ranking better and worse consequences), it then states that in any situation, the morally aproppriate way to act is to choose the action (among the existing possibilities) that brings about the best consequence.","title":"Consequentialist Theories"},{"location":"aeg/chapter1/normative/#deontological-theories-or-principles-based-theories","text":"Deontological theories in contrast, take a different approach. Morality is not to be measured by the consequences actions bring about, but instead morality dictates that one should follow a set of rules or principles regardless of what the consequences are. That is, the rightness of an action is determined by an agent\u2019s application of some universal standard, rule, or maxim of rightness irrespective of its consequences and independent of the interests or ends of the agent whose conduct the standard, rule, or maxim is guiding. The most famous proponent of deontological theories is Immanuel Kant, who was a firm believer in rule or duty-based morality, and who went as far as to define the maxim or principle to be followed in order to act morally which he called the Categorical Imperative. This rule roughly states that one must always act as if one\u2019s actions could be willed into a universal rule that everyone in society follows. In principle-based theories, morality is about following a set of rules or principles, and different theories will give different answers as to what principles one should act upon.","title":"Deontological Theories (or principles-based theories)"},{"location":"aeg/chapter1/normative/#virtue-ethics","text":"Virtue ethics is not concerned with how people should act in a particular situation, but instead on what kind of people they are. Its roots can be found in Confucius and Mencius in the East and in Plato and Aristotle in the West. Instead of beginning with the question: \u201cWhat should I do?\u201d, it asks the question: \u201cWhat sort of person should I strive to become in order to live an ethical life?\u201d. Virtue ethicists emphasise that morality has to do with having a certain moral character which is achieved by cultivating virtues. A virtue is an excellent trait of character[@sep-ethics-virtue], and it comes in degrees. These traits may derive from natural tendencies, but they must also be nurtured. Under these theories, a person who acts in a kind or benevolent way (these would be the virtues) does not do so because doing so would maximise the outcome of her actions (as a consequentialist would), or because it is her duty to do so (as deontology would advise), but instead because acting in that way cultivates said virtues. The development of moral virtues through practice, discipline, and repetition is the purpose of the human form of life. For this ethical view to work, there must be some reserve of objective moral values available for agents to draw on to form virtuous habits and dispositions. Some of the virtues stressed in antiquity but also justified in modern secular ethics: wisdom (ability to exercise practical reason to determine right action); temperance (ability to remain cool-headed and guided by reason instead of emotion); courage (ability to confront danger boldly and with self-assurance); justice (ability to act impartially and with fairness).","title":"Virtue Ethics"},{"location":"aeg/chapter1/normative/#bio-centric-ethics","text":"Biocentric ethics attempts to justify moral responsibility and the rightness of action not in a human-centered way but in terms of the intrinsic value of all life forms and ecologies. It treats \u201cnature\u201d (the living biosphere) not as an instrument or resource available to be used to achieve the purposes of human industry, but rather as an entity that makes a moral claim on us. From a biocentric perspective, moral actions are those that preserve the flourishing and the diversity of all living beings and aim to secure the sustenance of the biosphere as a whole. For the purposes of the rest of the course we will focus mostly on the first two families of theories. Example Let's take a classic example to help draw the distinction between consequentialism and deontology. Suppose an assassin comes to your door and asks if you have seen your friend Peter. And suppose you are indeed hiding Peter in your house as he is fleeing this very assassin. Should you tell the assassin the truth, or should you lie and tell him you don't know where Peter is? From a consequentialist perspective, it is clear what one should do. Telling the truth will result in Peter dying and lying will result in him living. The first consequence is a lot worse than the second, so you should lie to the assassin. However, from a deontological perspective it is not so obvious. If morality is about duty, and one of our duties is to be honest, then perhaps we also have a moral duty to tell the truth, even in this case. 3 Most people would find the second conclusion morally abhorrent. This shows us that in some cases we seem to take a more consequentialist approach to ethics. However, the converse can also be the case. Example Imagine you are a surgeon and Peter, your patient, is peacefully recuperating after an appendicitis operation. He should shortly be back home. And imagine that five people in need of different organ transplants come into the hospital. If these people do not get a transplant soon they will die. Suppose Peter is a match for all of these people. A strict consequentialist doctor might decide that we should kill Peter in order to save the other five via organ transplants. After all the consequences of one life lost and five gained are a lot better than one life gained and five lost. Most people would find this conclusion morally abhorrent. A deontologist would reply that it is never permissible to use people as means to an end, since people are always ends in themselves. Therefore our duty to respect Peter's dignity and not use him as a means to saving the other five overrides any consequentialist considerations we might have. The question as to what exactly utility is and how one goes about measuring it poses all sorts of complications which are beyond the scope of this introduction. \u21a9 For a list of different kinds of consequentialist theories see the Stanford Encyclopedia of Philosophy's entry on Consequentialism[@sep-consequentialism]. \u21a9 There is a lot more nuance as to what a deontological theory would say in the example just described. However, this simplification helps illustrate a core difference between the two families of theories. \u21a9","title":"Bio-centric ethics"},{"location":"aeg/chapter2/","text":"AI Harms and Values \u00b6 Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk)) Chapter Outline \u00b6 AI Harms AI Values Chapter Summary Following an brief introduction into the world of practical ethics, we now turn to AI Ethics in particular. We start this chapter by identifying the different kinds of harms AI systems can create, be it to particular individuals, communities, society as a whole, or even to the biosphere. Additionally, we then turn to the values that drive issues behind AI ethics. In particular, we focus on what we call the SUM values, as they support, underwrite, and motivate a responsible innovation ecosystem. They are: protect, respect, connect, and care. Learning Objectives In this chapter, you will: Familiarise yourself with the different kinds of harms AI systems can create, as well as understand the level of harm they pose (individual, social, planetary). Look at the SUM Values and how they relate to AI ethics.","title":"Introduction"},{"location":"aeg/chapter2/#ai-harms-and-values","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))","title":"AI Harms and Values"},{"location":"aeg/chapter2/#chapter-outline","text":"AI Harms AI Values Chapter Summary Following an brief introduction into the world of practical ethics, we now turn to AI Ethics in particular. We start this chapter by identifying the different kinds of harms AI systems can create, be it to particular individuals, communities, society as a whole, or even to the biosphere. Additionally, we then turn to the values that drive issues behind AI ethics. In particular, we focus on what we call the SUM values, as they support, underwrite, and motivate a responsible innovation ecosystem. They are: protect, respect, connect, and care. Learning Objectives In this chapter, you will: Familiarise yourself with the different kinds of harms AI systems can create, as well as understand the level of harm they pose (individual, social, planetary). Look at the SUM Values and how they relate to AI ethics.","title":"Chapter Outline"},{"location":"aeg/chapter2/harms/","text":"AI Harms \u00b6 After that short introduction into metaethics and normative ethical theories, we will now start thinking about the specific context of artificial intelligence. What considerations are relevant when thinking about the ethics of using AI and data science in our everyday lives? As we saw in chapter 1, there are many different ways to think about what morality is and where it comes from, and the debate is far from settled (if one believes it ever can be settled in the first place). The question then becomes: how can we talk about AI ethics if we do not have a precise definition of what morality is or what it requires from us? This is no small barrier. To overcome it, we will draw on two traditions of moral thinking: a) bioethics and b) human rights discourse. Bioethics is the study of the ethical impacts of biomedicine and the applied life sciences. Human rights discourse draws its original inspiration from the UN Declaration of Human Rights . It is anchored in a set of universal principles that build upon the idea that all humans have an equal moral status as bearers of intrinsic human dignity. Most generally, human rights are the basic rights and freedoms that are possessed by every person in the world from cradle to grave and that preserve and protect the inviolable dignity of each individual regardless of their race, ethnicity, gender, age, sexual orientation, class, religion, disability status, language, nationality, or any other ascribed characteristic. These fundamental rights and freedoms create obligations that bind civil servants and governments to respecting, protecting, and fulfilling human rights. In the absence of the fulfilment of these duties, individuals are entitled to legal remedies that allow for the redress of any human rights violations. Human rights and freedoms, as have been codified in the Human Rights Act (1998) and in the European Convention on Human Rights (1953) can be then applied to the different aspects of an AI or data-driven system. Whereas bioethics largely stresses the normative values that underlie the safeguarding of individuals in instances where technological practices affect their interests and wellbeing, human rights discourse mainly focuses on the set of social, political, and legal entitlements that are due to all human beings under a universal framework of judicial protection and the rule of law. The main principles of bioethics include respecting the autonomy of the individual, protecting people from harm, looking after the well-being of others, and treating all individuals equitably and justly. The main tenets of human rights include the entitlement to equal freedom and dignity under the law, the protection of civil, political, and social rights, the universal recognition of personhood, and the right to free and unencumbered participation in the life of the community. Key Concept: Bioethics and Human Rights in Context The principles that have emerged from both traditions found their origins in moral claims that have responded directly to tangible, technologically inflicted harms and atrocities. That is, both traditions emerged out of concerted public acts of resistance against violence done to disempowered or vulnerable people. Whereas human rights has its origins in efforts to redress the well-known barbarisms and genocides of the mid-twentieth century, in the case of bioethics, its emergence tracked the public exposure in the 1960s and 1970s of several atrocities of human experimentation (such as the infamous Tuskegee syphilis experiment). In the latter instances, it was discovered that members of vulnerable or marginalised social groups had been subjected to the injurious effects of institutionally run biomedical experiments without having knowledge of or giving consent to their participation. It is drawing on these points of departure that we now turn to AI and data science ethics. Anchoring the foundation of AI ethics in real-world social injuries has been a useful strategy. It has enabled the scope of the values and ethical concerns that underwrite responsible practices in the design, development and deployment of AI and data- driven systems to be informed by the actual risks posed by their use. In this course we will focus on six main kinds of AI harms. Abstract Loss of autonomy, interpersonal connection, and empathy Poor qualities and dangerous outcomes Bias, injustice, and discrimination Widening global and digital divides Breaches in data integrity, privacy and security Biospheric harm 1. Loss of autonomy, interpersonal connection, and empathy \u00b6 Automated AI systems with the power to make decisions about people can have potentially dehumanising consequences for those subject to them. Obviously, not all automated decisions are created equal. It is one thing if an automated system decides whether an email should be classified as spam or not, and quite another if an AI is in charge of allocating scarce social services, or deciding who gets hired for a job. Individuals may feel disempowered in the face of unstoppable automation, especially when these decisions are relevant to their sense of personal autonomy. This feeling can be compounded as well if there are few or no avenues in place to dispute or contest the automated decision. People may also feel as if they are being \"reduced to a statistic\" by these systems, or that the use of their personal data violates their privacy. Finally, but no less importantly, automation may also result in a loss of empathy and crucial human connection. For example, if AI is used to make or assist in making decision which impact people's lives, such as AI-assisted hiring tools there can be a loss of autonomy and interpersonal connection. People might want to know why they were not called to an interview, and knowing that there's no one to ask might be frustrating. Similarly, there is a loss of autonomy if the person does not then have a clear avenue where they can ask about the reasons about why they did not get called, and how they might improve in the future. 2. Poor qualities and dangerous outcomes \u00b6 Algorithmic models are only as good as the data on which they are trained, tested, and validated (commnonly called \u2018Garbage in, garbage out\u2019). Inaccuracies, measurement errors, and sampling biases across data collection and recording can taint datasets. Using poor quality data could have grave consequence for individual wellbeing and the public welfare. This problem exist wherever datasets are used. The question of the quality of a dataset obviously comes in degrees, and it can be more or less dangerous depending on the context in which the data is used. It can also stem from multiple sources. Some can be honest mistakes, like typos or missing data points, while others derive from more profound problems, such as the structural biases that our society has which are then replicated in the datasets. But the main insight remains: as long as the data used for a model is not appropriate and does not accurately represent the underlying phenomenon it is studying, the outcomes of said AI model will not be accurate either. 3. Bias, injustice, and discrimination \u00b6 Supervised machine learning models draw insights (learn) from the existing data patterns on which they are trained. When they are working reliably, they can make accurate, out-of-sample predictions from what they inferred from the training data. However, the problem is that these patterns may not be equitable or fair. In fact, this is very often not the case, and cases where data is biased, unjust or discriminatory sadly abound. Most (if not all) machine learning models are trained on historial data, and this means that as long as they do, they will be embedded with past and present injustices, forms of discrimination, and multiple biases which the data itself contains. Not only that, the machine learning algorithms will then reproduce, and maybe even amplify said biases. In day 4 we will look at the various different biases which can creep up in the ML/AI lifecycle. One crucial thing to keep in mind though, is that there is no fairness without awareness. When designing, implementing, and using these systems, conscious awareness of biases and explicit mitigation strategies must be discussed and implemented. Examples of this kind of harm come from different sectors of society. A 2019 paper published in Science showed how an algorithm used in US healthcare to predict patients' needs was producing racist results.[@obermeyer2019] The bias was introduced because the algorithm used past health costs as a proxy for health needs, which inadvertently favoured White patients. Less money was spent on black patients with the same level of need as their white counterparts, and the algorithm thus falsely concluded that black patients are healthier than equally sick white patients. Another famous example is Amazon's hiring algorithm which turned out to be biased against women. In an attempt to automate their hiring practices, Amazon developed an experimental hiring tool which used artificial intelligence to give job candidates scores ranging from one to five stars.[@martin2022] The algorithm quickly taught itself to discriminate against women candidates, penalising resumes which included the word women, (in 'women's chess club' for instance), as well as downgrading resumes which came from all-women colleges (ibid). Although the algorithm is not used by the company, (it was actually taken down precisely because of concerns about its sexism[@dastinjeffrey]), it serves as a powerful example of how AI can perpetuate and amplify historical biases (such as learning from the fact that traditionally Amazon has not hired many women, and extrapolating that to mean that women are not good employees). 4. Widening global digital divides \u00b6 The use of AI systems is not distributed uniformally across different countries, or even within regions in the same country. The varying levels of access and use of these technologies can reinforce and amplify already existing digital divides and data inequities. It can also exacerbate exploitative data appropriation from less rich countries and institutions to more well-resourced researchers and companies in richer countries or in better-funded universities within one country. Long-standing dynamics of global inequality, for instance, may undermine reciprocal sharing between research collaborators from high-income countries (HICs) and those from low-/middle-income countries (LMICs).[@leslie2020a] Given asymmetries in resources, infrastructure, and research capabilities, data sharing between LMICs and HICs, and transnational research collaboration, can lead to inequity and exploitation.[@bezuidenhout2017]-[@leonelli2021]-[@shrum2005] That is, data originators from LMICs may put immense amounts of effort and time into developing useful datasets (and openly share them) only to have their countries excluded from the benefits derived by researchers from HICs who have capitalised on such data in virtue of greater access to digital resources and compute infrastructure.[@worldhealthorganization2022] In this way, the benefits of data production and research are not necessarily accrued fairly to originating researchers and research subjects, widening the already wide gaps between the more and less advantaged groups of researchers or communities. An example of this is what happened after the Omicron variant of the Sars-Covid-19 virus was first reported in South Africa. Researchers in the country conducted excelent research and were actually quicker in detecting the variant and sequencing its genome than other countries. However, instead of being lauded by the international research community, the country was rewarded with a travel ban from most of the world, even though it was unlikely to be useful (and actually it was later discovered that Omicron had already been present in Europe before it was detected in South Africa). Even though it wasn't the researchers who were punished in this case, the benefits of the research certainly did not get distributed fairly across the globe.[@bbc2021] These gaps in research resources and capabilities go beyond gaps between HIC's and LMIC's. They can also exist within the same country, between large research universities and technology corporations which are better positioned to advance data research given their access to data and compute infrastructures when compared to less well-resourced universities or institutions.[@ahmed2020] Another example comes from what is called \"parachute research\": researchers from the Global North conduct research in the Global South and then go back to their home countries with the data, without necessary regarding the interests of the researchers or data subjects the data was taken from. In a systematic review that examined African authorship proportions in the biomedical literature published between 1980 \u2013 2016 where research was originally done in Africa, scholars found that African researchers are significantly under-represented in the global health community, even when the data originates from Africa.[@mbaye2019] 5. Breaches in data integrity, privacy, and security \u00b6 The ways we measure, collect, use, and store data points can lead to a multplicity of harms to individuals. However, individual harms can also expand and bleed into wider society. Issues can arise by data points being appropriately and fairly measured, as well as with acquiring data sets with informed consent from the data subjects. Once the data is collected, harms may arise in terms of how the data is used. In these cases the notion of contextual privacy[@nissenbaum2009] can be very enlighting. The idea is that a data point (or a piece of information more generally) is not public or private per se, but instead it depends on the context and purpose for which it is being used. For example, one may consent to a fitness tracker collecting one's data for personal purposes, but one has not then automatically consented to the company using the data in other ways (for example, sell it to health insurance companies). This problem is compounded by the fact that users either have no information that this is being done, and even when they do, as mentioned in (1), they often have few or no avenues to contest these practices. 1 Additionally, many potential harms can occur in the way information is stored in the medium and long run where security considerations are not always at the forefront of data managers. 6. Biospheric harms \u00b6 A final kind of AI harm is not done to people directly, but to the environment we all live in. The explosion of computing power (which has partly driven the \u201cbig data revolution\u201d) has had significant environmental cost. Algorithms require data, and as they become more complex, they require increasing amounts of data and computation, which translates into increasing levels of energy consumption. For example, training Google's large language model BERT produces emissions equivalent to one transatlantic flight.[@strubell2019] Many of these models ingest abundant amounts of data for training. As models increase in size and complexity they need more training data, but this does not necessarily lead to an equally large increase in model accuracy. Quite the contrary. In many cases, the gains in accuracy are only modest. For example, between 2013 and 2019 the amount of compute needed to train complex algorithmic models has increased 300,000 times. This results in energy expenditures (from increases in training the model) doubling every six months. As a result, a significant amount of costly resources are used, even when the benefits of improvements in the model are small at best. Additionally, the costs of these resources are burdened upon everyone on the planet (in the form of negative externalities), while the modest gain in model performace is most likely accrued to the propietary owner(s) of the model. These models contribute to emissions which are partly responsible of biospheric harm and climate change. Additionally, the benefits and risks of the use of data-intensive models is not uniformly distributed among the population or among the world\u2019s regions. If anything, the allocations of benefit and risk have closely tracked the existing patterns of environmental racism, coloniality, and \u201cslow violence\u201d[@nixon2013] that have typified the disproportionate exposure of marginalised communities (especially those who inhabit what has conventionally been referred to as \u201cthe Global South\u201d) to the pollution and destruction of local ecosystems and to involuntary displacement. The European Union's General Data Protection Regulation (GDPR) is probably the most advanced online privacy regulation and attempts to give consumers a lot more control over who can access their data and how it can be used. \u21a9","title":"AI Harms"},{"location":"aeg/chapter2/harms/#ai-harms","text":"After that short introduction into metaethics and normative ethical theories, we will now start thinking about the specific context of artificial intelligence. What considerations are relevant when thinking about the ethics of using AI and data science in our everyday lives? As we saw in chapter 1, there are many different ways to think about what morality is and where it comes from, and the debate is far from settled (if one believes it ever can be settled in the first place). The question then becomes: how can we talk about AI ethics if we do not have a precise definition of what morality is or what it requires from us? This is no small barrier. To overcome it, we will draw on two traditions of moral thinking: a) bioethics and b) human rights discourse. Bioethics is the study of the ethical impacts of biomedicine and the applied life sciences. Human rights discourse draws its original inspiration from the UN Declaration of Human Rights . It is anchored in a set of universal principles that build upon the idea that all humans have an equal moral status as bearers of intrinsic human dignity. Most generally, human rights are the basic rights and freedoms that are possessed by every person in the world from cradle to grave and that preserve and protect the inviolable dignity of each individual regardless of their race, ethnicity, gender, age, sexual orientation, class, religion, disability status, language, nationality, or any other ascribed characteristic. These fundamental rights and freedoms create obligations that bind civil servants and governments to respecting, protecting, and fulfilling human rights. In the absence of the fulfilment of these duties, individuals are entitled to legal remedies that allow for the redress of any human rights violations. Human rights and freedoms, as have been codified in the Human Rights Act (1998) and in the European Convention on Human Rights (1953) can be then applied to the different aspects of an AI or data-driven system. Whereas bioethics largely stresses the normative values that underlie the safeguarding of individuals in instances where technological practices affect their interests and wellbeing, human rights discourse mainly focuses on the set of social, political, and legal entitlements that are due to all human beings under a universal framework of judicial protection and the rule of law. The main principles of bioethics include respecting the autonomy of the individual, protecting people from harm, looking after the well-being of others, and treating all individuals equitably and justly. The main tenets of human rights include the entitlement to equal freedom and dignity under the law, the protection of civil, political, and social rights, the universal recognition of personhood, and the right to free and unencumbered participation in the life of the community. Key Concept: Bioethics and Human Rights in Context The principles that have emerged from both traditions found their origins in moral claims that have responded directly to tangible, technologically inflicted harms and atrocities. That is, both traditions emerged out of concerted public acts of resistance against violence done to disempowered or vulnerable people. Whereas human rights has its origins in efforts to redress the well-known barbarisms and genocides of the mid-twentieth century, in the case of bioethics, its emergence tracked the public exposure in the 1960s and 1970s of several atrocities of human experimentation (such as the infamous Tuskegee syphilis experiment). In the latter instances, it was discovered that members of vulnerable or marginalised social groups had been subjected to the injurious effects of institutionally run biomedical experiments without having knowledge of or giving consent to their participation. It is drawing on these points of departure that we now turn to AI and data science ethics. Anchoring the foundation of AI ethics in real-world social injuries has been a useful strategy. It has enabled the scope of the values and ethical concerns that underwrite responsible practices in the design, development and deployment of AI and data- driven systems to be informed by the actual risks posed by their use. In this course we will focus on six main kinds of AI harms. Abstract Loss of autonomy, interpersonal connection, and empathy Poor qualities and dangerous outcomes Bias, injustice, and discrimination Widening global and digital divides Breaches in data integrity, privacy and security Biospheric harm","title":"AI Harms"},{"location":"aeg/chapter2/harms/#1-loss-of-autonomy-interpersonal-connection-and-empathy","text":"Automated AI systems with the power to make decisions about people can have potentially dehumanising consequences for those subject to them. Obviously, not all automated decisions are created equal. It is one thing if an automated system decides whether an email should be classified as spam or not, and quite another if an AI is in charge of allocating scarce social services, or deciding who gets hired for a job. Individuals may feel disempowered in the face of unstoppable automation, especially when these decisions are relevant to their sense of personal autonomy. This feeling can be compounded as well if there are few or no avenues in place to dispute or contest the automated decision. People may also feel as if they are being \"reduced to a statistic\" by these systems, or that the use of their personal data violates their privacy. Finally, but no less importantly, automation may also result in a loss of empathy and crucial human connection. For example, if AI is used to make or assist in making decision which impact people's lives, such as AI-assisted hiring tools there can be a loss of autonomy and interpersonal connection. People might want to know why they were not called to an interview, and knowing that there's no one to ask might be frustrating. Similarly, there is a loss of autonomy if the person does not then have a clear avenue where they can ask about the reasons about why they did not get called, and how they might improve in the future.","title":"1. Loss of autonomy, interpersonal connection, and empathy"},{"location":"aeg/chapter2/harms/#2-poor-qualities-and-dangerous-outcomes","text":"Algorithmic models are only as good as the data on which they are trained, tested, and validated (commnonly called \u2018Garbage in, garbage out\u2019). Inaccuracies, measurement errors, and sampling biases across data collection and recording can taint datasets. Using poor quality data could have grave consequence for individual wellbeing and the public welfare. This problem exist wherever datasets are used. The question of the quality of a dataset obviously comes in degrees, and it can be more or less dangerous depending on the context in which the data is used. It can also stem from multiple sources. Some can be honest mistakes, like typos or missing data points, while others derive from more profound problems, such as the structural biases that our society has which are then replicated in the datasets. But the main insight remains: as long as the data used for a model is not appropriate and does not accurately represent the underlying phenomenon it is studying, the outcomes of said AI model will not be accurate either.","title":"2. Poor qualities and dangerous outcomes"},{"location":"aeg/chapter2/harms/#3-bias-injustice-and-discrimination","text":"Supervised machine learning models draw insights (learn) from the existing data patterns on which they are trained. When they are working reliably, they can make accurate, out-of-sample predictions from what they inferred from the training data. However, the problem is that these patterns may not be equitable or fair. In fact, this is very often not the case, and cases where data is biased, unjust or discriminatory sadly abound. Most (if not all) machine learning models are trained on historial data, and this means that as long as they do, they will be embedded with past and present injustices, forms of discrimination, and multiple biases which the data itself contains. Not only that, the machine learning algorithms will then reproduce, and maybe even amplify said biases. In day 4 we will look at the various different biases which can creep up in the ML/AI lifecycle. One crucial thing to keep in mind though, is that there is no fairness without awareness. When designing, implementing, and using these systems, conscious awareness of biases and explicit mitigation strategies must be discussed and implemented. Examples of this kind of harm come from different sectors of society. A 2019 paper published in Science showed how an algorithm used in US healthcare to predict patients' needs was producing racist results.[@obermeyer2019] The bias was introduced because the algorithm used past health costs as a proxy for health needs, which inadvertently favoured White patients. Less money was spent on black patients with the same level of need as their white counterparts, and the algorithm thus falsely concluded that black patients are healthier than equally sick white patients. Another famous example is Amazon's hiring algorithm which turned out to be biased against women. In an attempt to automate their hiring practices, Amazon developed an experimental hiring tool which used artificial intelligence to give job candidates scores ranging from one to five stars.[@martin2022] The algorithm quickly taught itself to discriminate against women candidates, penalising resumes which included the word women, (in 'women's chess club' for instance), as well as downgrading resumes which came from all-women colleges (ibid). Although the algorithm is not used by the company, (it was actually taken down precisely because of concerns about its sexism[@dastinjeffrey]), it serves as a powerful example of how AI can perpetuate and amplify historical biases (such as learning from the fact that traditionally Amazon has not hired many women, and extrapolating that to mean that women are not good employees).","title":"3. Bias, injustice, and discrimination"},{"location":"aeg/chapter2/harms/#4-widening-global-digital-divides","text":"The use of AI systems is not distributed uniformally across different countries, or even within regions in the same country. The varying levels of access and use of these technologies can reinforce and amplify already existing digital divides and data inequities. It can also exacerbate exploitative data appropriation from less rich countries and institutions to more well-resourced researchers and companies in richer countries or in better-funded universities within one country. Long-standing dynamics of global inequality, for instance, may undermine reciprocal sharing between research collaborators from high-income countries (HICs) and those from low-/middle-income countries (LMICs).[@leslie2020a] Given asymmetries in resources, infrastructure, and research capabilities, data sharing between LMICs and HICs, and transnational research collaboration, can lead to inequity and exploitation.[@bezuidenhout2017]-[@leonelli2021]-[@shrum2005] That is, data originators from LMICs may put immense amounts of effort and time into developing useful datasets (and openly share them) only to have their countries excluded from the benefits derived by researchers from HICs who have capitalised on such data in virtue of greater access to digital resources and compute infrastructure.[@worldhealthorganization2022] In this way, the benefits of data production and research are not necessarily accrued fairly to originating researchers and research subjects, widening the already wide gaps between the more and less advantaged groups of researchers or communities. An example of this is what happened after the Omicron variant of the Sars-Covid-19 virus was first reported in South Africa. Researchers in the country conducted excelent research and were actually quicker in detecting the variant and sequencing its genome than other countries. However, instead of being lauded by the international research community, the country was rewarded with a travel ban from most of the world, even though it was unlikely to be useful (and actually it was later discovered that Omicron had already been present in Europe before it was detected in South Africa). Even though it wasn't the researchers who were punished in this case, the benefits of the research certainly did not get distributed fairly across the globe.[@bbc2021] These gaps in research resources and capabilities go beyond gaps between HIC's and LMIC's. They can also exist within the same country, between large research universities and technology corporations which are better positioned to advance data research given their access to data and compute infrastructures when compared to less well-resourced universities or institutions.[@ahmed2020] Another example comes from what is called \"parachute research\": researchers from the Global North conduct research in the Global South and then go back to their home countries with the data, without necessary regarding the interests of the researchers or data subjects the data was taken from. In a systematic review that examined African authorship proportions in the biomedical literature published between 1980 \u2013 2016 where research was originally done in Africa, scholars found that African researchers are significantly under-represented in the global health community, even when the data originates from Africa.[@mbaye2019]","title":"4. Widening global digital divides"},{"location":"aeg/chapter2/harms/#5-breaches-in-data-integrity-privacy-and-security","text":"The ways we measure, collect, use, and store data points can lead to a multplicity of harms to individuals. However, individual harms can also expand and bleed into wider society. Issues can arise by data points being appropriately and fairly measured, as well as with acquiring data sets with informed consent from the data subjects. Once the data is collected, harms may arise in terms of how the data is used. In these cases the notion of contextual privacy[@nissenbaum2009] can be very enlighting. The idea is that a data point (or a piece of information more generally) is not public or private per se, but instead it depends on the context and purpose for which it is being used. For example, one may consent to a fitness tracker collecting one's data for personal purposes, but one has not then automatically consented to the company using the data in other ways (for example, sell it to health insurance companies). This problem is compounded by the fact that users either have no information that this is being done, and even when they do, as mentioned in (1), they often have few or no avenues to contest these practices. 1 Additionally, many potential harms can occur in the way information is stored in the medium and long run where security considerations are not always at the forefront of data managers.","title":"5. Breaches in data integrity, privacy, and security"},{"location":"aeg/chapter2/harms/#6-biospheric-harms","text":"A final kind of AI harm is not done to people directly, but to the environment we all live in. The explosion of computing power (which has partly driven the \u201cbig data revolution\u201d) has had significant environmental cost. Algorithms require data, and as they become more complex, they require increasing amounts of data and computation, which translates into increasing levels of energy consumption. For example, training Google's large language model BERT produces emissions equivalent to one transatlantic flight.[@strubell2019] Many of these models ingest abundant amounts of data for training. As models increase in size and complexity they need more training data, but this does not necessarily lead to an equally large increase in model accuracy. Quite the contrary. In many cases, the gains in accuracy are only modest. For example, between 2013 and 2019 the amount of compute needed to train complex algorithmic models has increased 300,000 times. This results in energy expenditures (from increases in training the model) doubling every six months. As a result, a significant amount of costly resources are used, even when the benefits of improvements in the model are small at best. Additionally, the costs of these resources are burdened upon everyone on the planet (in the form of negative externalities), while the modest gain in model performace is most likely accrued to the propietary owner(s) of the model. These models contribute to emissions which are partly responsible of biospheric harm and climate change. Additionally, the benefits and risks of the use of data-intensive models is not uniformly distributed among the population or among the world\u2019s regions. If anything, the allocations of benefit and risk have closely tracked the existing patterns of environmental racism, coloniality, and \u201cslow violence\u201d[@nixon2013] that have typified the disproportionate exposure of marginalised communities (especially those who inhabit what has conventionally been referred to as \u201cthe Global South\u201d) to the pollution and destruction of local ecosystems and to involuntary displacement. The European Union's General Data Protection Regulation (GDPR) is probably the most advanced online privacy regulation and attempts to give consumers a lot more control over who can access their data and how it can be used. \u21a9","title":"6. Biospheric harms"},{"location":"aeg/chapter2/values/","text":"AI Values \u00b6 SUM Values: Respect, Connect, Care, and Protect \u00b6 In keeping with this hazards-responsive approach, the SUM Values incorporate conceptual elements from both bioethics and human rights discourse, but they do so with an eye to applying the most critical of these elements to the specific social and ethical problems raised by the potential misuse, abuse, poor design, or harmful unintended consequences of AI systems. The SUM Values are values that support, underwrite, and motivate a responsible innovation ecosystem. Their role is to provide an accessible framework of ethical criteria for considering, assessing, and deliberating on the ethical permissibility of a prospective AI project and its ethical impacts. They are meant to be utilised as guiding values throughout the innovation lifecycle: from the preliminary steps of project evaluation, planning, and problem formulation, through processes of design, development, and testing, to the stages of implementation and reassessment. Adopting common values from the outset enables reciprocally respectful, sincere, and open dialogue about ethical challenges by helping to create a shared vocabulary for informed dialogue and impact assessment. Such a common starting point also facilitates discussion and deliberation about how to balance values when they come into conflict. The SUM Values encompass a range of values that are distilled in the following four (corresponding ethical concerns are indicated in the circle):","title":"AI Values"},{"location":"aeg/chapter2/values/#ai-values","text":"","title":"AI Values"},{"location":"aeg/chapter2/values/#sum-values-respect-connect-care-and-protect","text":"In keeping with this hazards-responsive approach, the SUM Values incorporate conceptual elements from both bioethics and human rights discourse, but they do so with an eye to applying the most critical of these elements to the specific social and ethical problems raised by the potential misuse, abuse, poor design, or harmful unintended consequences of AI systems. The SUM Values are values that support, underwrite, and motivate a responsible innovation ecosystem. Their role is to provide an accessible framework of ethical criteria for considering, assessing, and deliberating on the ethical permissibility of a prospective AI project and its ethical impacts. They are meant to be utilised as guiding values throughout the innovation lifecycle: from the preliminary steps of project evaluation, planning, and problem formulation, through processes of design, development, and testing, to the stages of implementation and reassessment. Adopting common values from the outset enables reciprocally respectful, sincere, and open dialogue about ethical challenges by helping to create a shared vocabulary for informed dialogue and impact assessment. Such a common starting point also facilitates discussion and deliberation about how to balance values when they come into conflict. The SUM Values encompass a range of values that are distilled in the following four (corresponding ethical concerns are indicated in the circle):","title":"SUM Values: Respect, Connect, Care, and Protect"},{"location":"aeg/chapter3/","text":"AI Sustainability and Stakeholder Engagement \u00b6 Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk)) Chapter Outline \u00b6 Stakeholder Engagement Process Stakeholder Impact Assessment Chapter Summary In this chapter we look at the importance of stakeholders in sustainability through what is known as the stakeholder engagement processes (SEP) as well as the stakeholder impact asssessment (SIA). The former is a three-staged process with the following core steps: (i) stakeholder analysis and salience, (ii) positionality reflection, and (iii) stakeholder engagement objective and method selection. The latter goes beyond the SEP and is designed to ensure a project can not only remain sustainable, but also support the sustainability of impacted communities throughout time. Beyond development and initial deployment, an AI system requires continuous monitoring to ensure that the long-term transformative effects and impacts of the project are not only well-understood, but accepted and beneficial for impacted individuals and communities. Learning Objectives In this chapter, you will: Familiarise yourself with the concept of sustainability in the context of AI and the importance of anticipatory reflection. Learn about the AI project lifecycle and how it can be a useful tool for AI sustainability. Understand the importance of an appropriate engagement with stakeholders, as well as how to conduct a stakeholder engagement process (SEP). Familiarise yourself with impact assessments, in particular a stakeholder impact assessment (SIA) as well as its main components.","title":"Introduction"},{"location":"aeg/chapter3/#ai-sustainability-and-stakeholder-engagement","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))","title":"AI Sustainability and Stakeholder Engagement"},{"location":"aeg/chapter3/#chapter-outline","text":"Stakeholder Engagement Process Stakeholder Impact Assessment Chapter Summary In this chapter we look at the importance of stakeholders in sustainability through what is known as the stakeholder engagement processes (SEP) as well as the stakeholder impact asssessment (SIA). The former is a three-staged process with the following core steps: (i) stakeholder analysis and salience, (ii) positionality reflection, and (iii) stakeholder engagement objective and method selection. The latter goes beyond the SEP and is designed to ensure a project can not only remain sustainable, but also support the sustainability of impacted communities throughout time. Beyond development and initial deployment, an AI system requires continuous monitoring to ensure that the long-term transformative effects and impacts of the project are not only well-understood, but accepted and beneficial for impacted individuals and communities. Learning Objectives In this chapter, you will: Familiarise yourself with the concept of sustainability in the context of AI and the importance of anticipatory reflection. Learn about the AI project lifecycle and how it can be a useful tool for AI sustainability. Understand the importance of an appropriate engagement with stakeholders, as well as how to conduct a stakeholder engagement process (SEP). Familiarise yourself with impact assessments, in particular a stakeholder impact assessment (SIA) as well as its main components.","title":"Chapter Outline"},{"location":"aeg/chapter3/engagement/","text":"Stakeholder Engagement Process \u00b6 The Sustainability, engagement, and reflection cycle A crucial component of AI Sustainability is conducting an appropriate Stakeholder Engagement Process (SEP). But before we delve into the process itself, we must ask, what is a stakeholder? Key Concept: Stakeholder Stakeholders are individuals or groups that: (1) have interests or rights that may be affected by the past, present, and future decisions and activities of an organisation; (2) may have the power or authority to influence the outcome of such decisions and activities; (3) possess relevant characteristics that put them in positions of advantage or vulnerability with regard to those decisions and activities. The SEP is an iterative process with three core steps: Each of these activities should be documented and utilised to create a Project Summary (PS) report. A Project Summary Report is comprised of four components reflecting the SEP process: (1) a preliminary project scoping and stakeholder analysis, (2) a positionality reflection (3) an overview of established stakeholder objectives and methods and (4) a governance workflow map. The fourth component of the PS Report will be discussed later in the course when we look into accountability and governance of AI systems. We will now analyise each of the relevant steps within the process and look at how they relate to thinking about the sustainability of the project as a whole. Preliminary Project Scoping and Stakeholder Analysis \u00b6 This is the first activity within the SEP process. It involves four sub-steps: Outlining project use context, domain, and data: In this step, a high-level description of the prospective system is outlined. This includes the domain in which it will operate, the contexts in which it will be used, and the data on which it will be trained. During this initial project scoping activity, information should be drawn from organisational documents (i.e., the project business case, proof of concept, or project charter), project team collaboration, as well as desk research (if necessary) to complete the description. Identifying stakeholders: Once the first step is completed and building on this contextual understanding, this step involves identifying who may be affected by, or may affect the project. Scoping potential stakeholder impacts: This involves carrying out a preliminary evaluation of the potential impacts of the prospective AI system on affected individuals and communities. At this initial stage of reflection, members of the project team should review the SUM values, and the corresponding ethical concerns, rights and freedoms, and then consider which of these could be impacted by the design, development and deployment of the prospective AI system and how. Analysing stakeholder salience: The step requires assessing the relevance of each identified stakeholder group to the AI project and to its use contexts. This includes assessing the relative interests, rights, vulnerabilities, and advantages of identified stakeholders as these interests, rights, vulnerabilities, and advantages may be impacted by, or may impact, the AI system in question. When identifying stakeholders, the project team should also consider organisational stakeholders, whose input will likewise strengthen the openness, inclusivity, and diversity of the project. Stakeholder analyses may be carried out in a variety of ways that involve more-or-less stakeholder involvement. This spectrum of options ranges from analyses carried out exclusively by a project team without active community engagement to analyses built around the inclusion of community-led participation and co-design from the earliest stages of stakeholder identification. The degree of stakeholder involvement will vary from project to project based upon a preliminary assessment of the potential risks and hazards of the model or tool under consideration. Low-stakes AI applications that are not safety-critical, do not directly impact the lives of people, and do not process potentially sensitive social and demographic data may need less proactive stakeholder engagement than high-stakes projects. For example, an application which sorts out email and identifies spam may be correctly classified as low impact and low risk. A responsible and thorough initial evaluation of the scope of the possible risks that could arise from the AI system (to be carried by the team responsible for developing said system) will determine the the potential hazards the project poses to affected stakeholdes (be them individuals or communities). A reasonable assessments of the dangers posed to individual wellbeing and public welfare is needed in order to formulate proportionate approaches to stakeholder involvement. Regardless of the potential impacts of a project, involving affected individuals and communities in stakeholder analysis (and, later, in stakeholder impact assessment) should, in all cases, be a significant consideration. Stakeholder involvement ensures that a project will possess an appropriate degree of public accountability, transparency, legitimacy, and democratic governance, and it recognizes the important role played in this by the inclusion of the voices of all affected individuals and communities in decision-making processes. In addition to providing these important supports for building public trust, stakeholder involvement can help to strengthen the objectivity, reflexivity, reasonableness, and robustness of the choices a project team makes across the project lifecycle. This is because the inclusion of a wider range of perspectives (especially of those who are most marginalised) can enlarge a project team\u2019s purview, expand its domain knowledge as well as its understanding of citizens\u2019 needs. It can likewise unearth potential biases that may arises from limiting the standpoints that inform decision-making to those of team members. Public engagement and community involvement, however, are only one part of the measures a team needs to take to ensure the objectivity, reflexivity, reasonableness, and robustness of its stakeholder analysis, impact assessment, and decision-making, more generally. Apart from outward-facing community participation, processes inward-facing reflection should also inform the way the team approaches to these challenges (see the sustainability, engagement, and reflection cycle). Positionality Reflection \u00b6 All individual human beings come from unique places, experiences, and life contexts that have shaped their thinking and perspectives. Reflecting on these is important insofar as it can help team members understand how their viewpoints might differ from those around them and, more importantly, from those who have diverging cultural and socioeconomic backgrounds and life experiences. Identifying and probing these differences can enable individuals to better understand how their own backgrounds, for better or worse, frame the way they see others, the way they approach and solve problems, and the way they carry out research and engage in innovation. By undertaking such efforts to recognise social position and differential privilege, they may gain a greater awareness of their own personal biases and unconscious assumptions. This then can enable them to better discern the origins of these biases and assumption and to confront and challenge them in turn. Social scientists have long referred to this kind of self-locating reflection as \u201cpositionality\u201d. When team members take their own positionalities into account making them explicit, they can better grasp how the influence of their respective social and cultural positions creates strengths and limitations. On the one hand, one\u2019s positionality\u2014with respect to characteristics like ethnicity, race, age, gender, socioeconomic status, education and training levels, values, geographical background, etc.\u2014can have a positive effect on an individual\u2019s contributions to an innovation project; the uniqueness of each person\u2019s lived experience and standpoint can play a constructive role in introducing insights and understandings that other team members do not have. On the other hand, one\u2019s positionality can assume a harmful role when hidden biases and prejudices that derive from a person\u2019s background, and from power imbalances and differential privileges, creep into decision-making processes undetected. When taking positionality into account, team members should reflect on their own positionality matrix. They should ask: To what extent do my personal characteristics, group identifications, socioeconomic status, educational, training, & work background, team composition, & institutional frame represent sources of power and advantage or sources of marginalisation and disadvantage? How does this positionality influence my (and my team\u2019s) ability to identify & understand affected stakeholders and the potential impacts of my project? Several other questions must be asked to respond to these two: How do I identify? How have I been educated and trained? What does my institutional context and team composition look like? What is my socioeconomic history? Stakeholder Engagement Objective and Method \u00b6 Once the initial project scoping, stakeholder analysis, and positionality reflection have been done, a project team can move on to define the objective for their stakeholder enaggement, as well as the method(s) to be used. As we have previously established, stakeholder engagement may be carried out in a variety of ways that involve more-or-less stakeholder involvement. This spectrum of options ranges from analyses carried out exclusively by a project team without active community engagement to analyses built around the inclusion of community-led participation and co-design from the earliest stages of stakeholder identification. The degree of stakeholder involvement will vary from project to project and take into account a preliminary assessment of the potential risks and hazards of the model or tool under consideration. Low-stakes AI applications that are not safety-critical, do not directly impact the lives of people, and do not process potentially sensitive social and demographic data may need less proactive stakeholder engagement than high-stakes projects. Similarly, the project team will need to take into account their assessment of positionality. When weighing these three factors, the team should prioritise the establishment of a clear and explicit stakeholder engagement objective and document this. This is crucial, because all stakeholder engagement processes can run the risk either of being cosmetic tools employed to legitimate projects without substantial and meaningful participation or of being insufficiently participative, i.e. of being one-way information flows or nudging exercises that serve as public relations instruments. The purpose of stakeholder involvement in sustainable AI projects is just the opposite: to amplify the participatory agency of affected individuals and organisations in impact assessment, risk management, and assurance processes. To avoid such hazards of superficiality, the team should shore up its proportionate approach to stakeholder engagement with deliberate and precise goal-setting. There are a range of engagement options that can help a project obtain a level of public participation which meets team-based assessments of impact and positionality as well as practical considerations and stakeholder needs. Defining a Stakeholder Objective \u00b6 Selecting a Stakeholder Engagement Method \u00b6 The following table summarises a wide range of salient methods: Mode of Engagement Description Degree of Engagement Practical Strengths Practical Weaknesses :material-email: Newsletters (email) Regular emails (e.g.: fortnightly or monthly) that contain updates, relevant news, and calls to action in an inviting format. INFORM Can reach many people; can contain large amount of relevant information; can be made accessible and visually engaging. Might not reach certain portions of the population; can be demanding to design and produce with some periodicity; easily forwarded to spam/junk folders without project team knowing (leading to overinflated readership stats). :material-mailbox: Letters (post) Regular letters (e.g.: monthly) that contain the latest updates, relevant news and calls to action. INFORM Can reach parts of the population with no internet or digital access; can contain large amount of relevant information; can be made accessible and visually engaging. Might not engage certain portions of the population; Slow delivery and interaction times hampers the effective flow of information and the organisation of further engagement. :material-checkbox-blank-badge: App notifications Projects can rely on the design of apps that are pitched to stakeholders who are notified on their phone with relevant updates. INFORM Easy and cost-effective to distribute information to large numbers of people; Rapid information flows bolster the provision of relevant and timely news and updates. More significant initial investment in developing an app; will not be available to people without smartphones. :fontawesome-solid-people-group: Community fora Events in which panels of experts share their knowledge on issues and then stakeholders can ask questions. INFORM Can inform people with more relevant information by providing them with the opportunity to ask questions; brings community together in a shared space of public communication. More time-consuming and resource intensive to organise; might attract smaller numbers of people and self-selecting groups rather than representative subsets of the population; effectiveness is constrained by forum capacity. :fontawesome-solid-list-check: Online Surveys Survey sent via email, embedded in a website, shared via social media, etc. CONSULT Cost-effective; simple mass- distribution. Risk of pre-emptive evaluative framework when designing questions; Does not reach those without internet connection or computer/smartphone access. :material-phone-in-talk: Phone Interviews Structured or semi-structured interviews held over the phone. CONSULT PARTNER Opportunity for stakeholders to voice concerns more openly. Risk of pre-emptive evaluative framework when designing questions; Might exclude portions of the populations without phone access or with habits of infrequent phone use. :fontawesome-solid-door-closed: Door-to-door interviews Structured or semi-structured interviews held in-person at people\u2019s houses. CONSULT PARTNER Opportunity for stakeholders to voice concerns more openly; can allow participants the opportunity to form connections through empathy and face-to- face communication. Potential for limited interest to engage with interviewers; time-consuming; can be seen by interviewees as intrusive or burdensome. :fontawesome-solid-people-arrows-left-right: In-person interviews Short interviews conducted in- person in public spaces. CONSULT PARTNER Can reach many people and a representative subset of the population if stakeholders are appropriately defined and sortition is used. Less targeted; pertinent stakeholders must be identified by area; little time/interest to engage with interviewer; can be viewed by interviewees as time- consuming and burdensome. :material-selection-search: Focus Groups A group of stakeholders brought together and asked their opinions on a particular issue. Can be more or less formally structured. CONSULT PARTNER Can gather in-depth information; Can lead to new insights and directions that were not anticipated by the project team. Subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. :fontawesome-solid-users-between-lines: Online Workshops Workshops using digital tools such as collaborative platforms. CONSULT Opportunity to reach stakeholders across regions, increased accessibility depending on digital access. Potential barriers to accessing tools required for participation, potential for disengagement. :material-crowd: Crowdsourcing (Online) Well-designed tasks that can be undertaken by a distributed collective, with individuals working on separate components. CONSULT PARTNER Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access. Supports increased potential for diverse forms of expertise and experience. Can be misused as a method for outsourcing cheap labour; potential barriers to accessing tools required for participation; potential for disengagement; difficult to ensure accuracy and validity of input from participants. :material-github: Distributed Project Collaboration (Online) Online digital platforms, such as GitHub, enable new forms of citizen science and collaborative development on diverse projects (e.g., open source software, open science). CONSULT PARTNER EMPOWER Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access; increased potential for diverse forms of expertise and experience; empowers new communities to actively participate in shaping and building tools that have real value for their communities. Potential barriers to accessing digital tools required for participation, including high levels of digital literacy. :fontawesome-solid-building-columns: Citizen panel or assembly Large groups of people (dozens or even thousands) who are representative of a town/region. INFORM CONSULT PARTNER EMPOWER Provides an opportunity for co-production of outputs; can produce insights and directions that were not anticipated by the project team; can provide an information base for conducting further outreach (surveys, interviews, focus groups, etc.); can be broadly representative; can bolster a community\u2019s sense of democratic agency and solidarity. Participant rolls must be continuously updated to ensure panels or assemblies remains representative of the population throughout their lifespan; resource-intensive for establishment and maintenance; subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. :material-account-group: Citizen jury A small group of people (between 12 and 24), representative of the demographics of a given area, come together to deliberate on an issue (generally one clearly framed set of questions), over the period of 2 to 7 days ( Involve.org.uk . INFORM CONSULT PARTNER EMPOWER Can gather in-depth information; can produce insights and directions that were not anticipated by the project team; can bolster participants\u2019 sense of democratic agency and solidarity. Subject to hazards of group think; complex to facilitate; risk of pre-emptive evaluative framework; small sample of citizens involved risks low representativeness of wider range of public opinions and beliefs. As with all forms of engagement, deciding on the best method requires awareness of your audience. Consider the following cases: === \":octicons-log-16: Policymakers\" A research team has released results from an economics study that could have a positive impact on public policy. They decide to share these results with policymakers. The goal is to directly influence policy. Therefore, the results need to be clearly communicated and also connected to the policy goal. This connection is important to help ensure that policy-makers are able to evaluate the wider implications of the scientific findings. **Communication Goal:** to demonstrate how scientific findings can support evidence-based policy impact === \":octicons-people-16: General Public\" As part of an education outreach campaign to improve digital literacy among adolescents, a mental health charity are running workshops with secondary school students. They wish to communicate recent evidence about the impact of over-using social media on mental health. Rather than communicating complicated statistical information about the methods used in their study, the team develop a more accessible form of their findings and link these findings to practical steps that the students can take to protect themselves online. **Communication Goal:** to build awareness of possible risks associated with excessive social media usage and support behavioural change strategies === \":octicons-beaker-16: Researchers\" A PhD student working in a Physics department has results from a recent study that developed and tested a new method for the large-scale data mining of astronomical data. The PhD student wishes to present this new method and the validation study at an upcoming international conference for data science. The audience will be technically literate, but will not have specialist expertise in astronomy. Therefore, the PhD student describes the method in the context of its original study buy also emphasises the generalisability for other sciences (e.g. genomics). **Communication Goal**: to advance academic career by gaining experience of presenting conference papers and also generating interest in a novel data science method.","title":"Stakeholder Engagement Process"},{"location":"aeg/chapter3/engagement/#stakeholder-engagement-process","text":"The Sustainability, engagement, and reflection cycle A crucial component of AI Sustainability is conducting an appropriate Stakeholder Engagement Process (SEP). But before we delve into the process itself, we must ask, what is a stakeholder? Key Concept: Stakeholder Stakeholders are individuals or groups that: (1) have interests or rights that may be affected by the past, present, and future decisions and activities of an organisation; (2) may have the power or authority to influence the outcome of such decisions and activities; (3) possess relevant characteristics that put them in positions of advantage or vulnerability with regard to those decisions and activities. The SEP is an iterative process with three core steps: Each of these activities should be documented and utilised to create a Project Summary (PS) report. A Project Summary Report is comprised of four components reflecting the SEP process: (1) a preliminary project scoping and stakeholder analysis, (2) a positionality reflection (3) an overview of established stakeholder objectives and methods and (4) a governance workflow map. The fourth component of the PS Report will be discussed later in the course when we look into accountability and governance of AI systems. We will now analyise each of the relevant steps within the process and look at how they relate to thinking about the sustainability of the project as a whole.","title":"Stakeholder Engagement Process"},{"location":"aeg/chapter3/engagement/#preliminary-project-scoping-and-stakeholder-analysis","text":"This is the first activity within the SEP process. It involves four sub-steps: Outlining project use context, domain, and data: In this step, a high-level description of the prospective system is outlined. This includes the domain in which it will operate, the contexts in which it will be used, and the data on which it will be trained. During this initial project scoping activity, information should be drawn from organisational documents (i.e., the project business case, proof of concept, or project charter), project team collaboration, as well as desk research (if necessary) to complete the description. Identifying stakeholders: Once the first step is completed and building on this contextual understanding, this step involves identifying who may be affected by, or may affect the project. Scoping potential stakeholder impacts: This involves carrying out a preliminary evaluation of the potential impacts of the prospective AI system on affected individuals and communities. At this initial stage of reflection, members of the project team should review the SUM values, and the corresponding ethical concerns, rights and freedoms, and then consider which of these could be impacted by the design, development and deployment of the prospective AI system and how. Analysing stakeholder salience: The step requires assessing the relevance of each identified stakeholder group to the AI project and to its use contexts. This includes assessing the relative interests, rights, vulnerabilities, and advantages of identified stakeholders as these interests, rights, vulnerabilities, and advantages may be impacted by, or may impact, the AI system in question. When identifying stakeholders, the project team should also consider organisational stakeholders, whose input will likewise strengthen the openness, inclusivity, and diversity of the project. Stakeholder analyses may be carried out in a variety of ways that involve more-or-less stakeholder involvement. This spectrum of options ranges from analyses carried out exclusively by a project team without active community engagement to analyses built around the inclusion of community-led participation and co-design from the earliest stages of stakeholder identification. The degree of stakeholder involvement will vary from project to project based upon a preliminary assessment of the potential risks and hazards of the model or tool under consideration. Low-stakes AI applications that are not safety-critical, do not directly impact the lives of people, and do not process potentially sensitive social and demographic data may need less proactive stakeholder engagement than high-stakes projects. For example, an application which sorts out email and identifies spam may be correctly classified as low impact and low risk. A responsible and thorough initial evaluation of the scope of the possible risks that could arise from the AI system (to be carried by the team responsible for developing said system) will determine the the potential hazards the project poses to affected stakeholdes (be them individuals or communities). A reasonable assessments of the dangers posed to individual wellbeing and public welfare is needed in order to formulate proportionate approaches to stakeholder involvement. Regardless of the potential impacts of a project, involving affected individuals and communities in stakeholder analysis (and, later, in stakeholder impact assessment) should, in all cases, be a significant consideration. Stakeholder involvement ensures that a project will possess an appropriate degree of public accountability, transparency, legitimacy, and democratic governance, and it recognizes the important role played in this by the inclusion of the voices of all affected individuals and communities in decision-making processes. In addition to providing these important supports for building public trust, stakeholder involvement can help to strengthen the objectivity, reflexivity, reasonableness, and robustness of the choices a project team makes across the project lifecycle. This is because the inclusion of a wider range of perspectives (especially of those who are most marginalised) can enlarge a project team\u2019s purview, expand its domain knowledge as well as its understanding of citizens\u2019 needs. It can likewise unearth potential biases that may arises from limiting the standpoints that inform decision-making to those of team members. Public engagement and community involvement, however, are only one part of the measures a team needs to take to ensure the objectivity, reflexivity, reasonableness, and robustness of its stakeholder analysis, impact assessment, and decision-making, more generally. Apart from outward-facing community participation, processes inward-facing reflection should also inform the way the team approaches to these challenges (see the sustainability, engagement, and reflection cycle).","title":"Preliminary Project Scoping and Stakeholder Analysis"},{"location":"aeg/chapter3/engagement/#positionality-reflection","text":"All individual human beings come from unique places, experiences, and life contexts that have shaped their thinking and perspectives. Reflecting on these is important insofar as it can help team members understand how their viewpoints might differ from those around them and, more importantly, from those who have diverging cultural and socioeconomic backgrounds and life experiences. Identifying and probing these differences can enable individuals to better understand how their own backgrounds, for better or worse, frame the way they see others, the way they approach and solve problems, and the way they carry out research and engage in innovation. By undertaking such efforts to recognise social position and differential privilege, they may gain a greater awareness of their own personal biases and unconscious assumptions. This then can enable them to better discern the origins of these biases and assumption and to confront and challenge them in turn. Social scientists have long referred to this kind of self-locating reflection as \u201cpositionality\u201d. When team members take their own positionalities into account making them explicit, they can better grasp how the influence of their respective social and cultural positions creates strengths and limitations. On the one hand, one\u2019s positionality\u2014with respect to characteristics like ethnicity, race, age, gender, socioeconomic status, education and training levels, values, geographical background, etc.\u2014can have a positive effect on an individual\u2019s contributions to an innovation project; the uniqueness of each person\u2019s lived experience and standpoint can play a constructive role in introducing insights and understandings that other team members do not have. On the other hand, one\u2019s positionality can assume a harmful role when hidden biases and prejudices that derive from a person\u2019s background, and from power imbalances and differential privileges, creep into decision-making processes undetected. When taking positionality into account, team members should reflect on their own positionality matrix. They should ask: To what extent do my personal characteristics, group identifications, socioeconomic status, educational, training, & work background, team composition, & institutional frame represent sources of power and advantage or sources of marginalisation and disadvantage? How does this positionality influence my (and my team\u2019s) ability to identify & understand affected stakeholders and the potential impacts of my project? Several other questions must be asked to respond to these two: How do I identify? How have I been educated and trained? What does my institutional context and team composition look like? What is my socioeconomic history?","title":"Positionality Reflection"},{"location":"aeg/chapter3/engagement/#stakeholder-engagement-objective-and-method","text":"Once the initial project scoping, stakeholder analysis, and positionality reflection have been done, a project team can move on to define the objective for their stakeholder enaggement, as well as the method(s) to be used. As we have previously established, stakeholder engagement may be carried out in a variety of ways that involve more-or-less stakeholder involvement. This spectrum of options ranges from analyses carried out exclusively by a project team without active community engagement to analyses built around the inclusion of community-led participation and co-design from the earliest stages of stakeholder identification. The degree of stakeholder involvement will vary from project to project and take into account a preliminary assessment of the potential risks and hazards of the model or tool under consideration. Low-stakes AI applications that are not safety-critical, do not directly impact the lives of people, and do not process potentially sensitive social and demographic data may need less proactive stakeholder engagement than high-stakes projects. Similarly, the project team will need to take into account their assessment of positionality. When weighing these three factors, the team should prioritise the establishment of a clear and explicit stakeholder engagement objective and document this. This is crucial, because all stakeholder engagement processes can run the risk either of being cosmetic tools employed to legitimate projects without substantial and meaningful participation or of being insufficiently participative, i.e. of being one-way information flows or nudging exercises that serve as public relations instruments. The purpose of stakeholder involvement in sustainable AI projects is just the opposite: to amplify the participatory agency of affected individuals and organisations in impact assessment, risk management, and assurance processes. To avoid such hazards of superficiality, the team should shore up its proportionate approach to stakeholder engagement with deliberate and precise goal-setting. There are a range of engagement options that can help a project obtain a level of public participation which meets team-based assessments of impact and positionality as well as practical considerations and stakeholder needs.","title":"Stakeholder Engagement Objective and Method"},{"location":"aeg/chapter3/engagement/#defining-a-stakeholder-objective","text":"","title":"Defining a Stakeholder Objective"},{"location":"aeg/chapter3/engagement/#selecting-a-stakeholder-engagement-method","text":"The following table summarises a wide range of salient methods: Mode of Engagement Description Degree of Engagement Practical Strengths Practical Weaknesses :material-email: Newsletters (email) Regular emails (e.g.: fortnightly or monthly) that contain updates, relevant news, and calls to action in an inviting format. INFORM Can reach many people; can contain large amount of relevant information; can be made accessible and visually engaging. Might not reach certain portions of the population; can be demanding to design and produce with some periodicity; easily forwarded to spam/junk folders without project team knowing (leading to overinflated readership stats). :material-mailbox: Letters (post) Regular letters (e.g.: monthly) that contain the latest updates, relevant news and calls to action. INFORM Can reach parts of the population with no internet or digital access; can contain large amount of relevant information; can be made accessible and visually engaging. Might not engage certain portions of the population; Slow delivery and interaction times hampers the effective flow of information and the organisation of further engagement. :material-checkbox-blank-badge: App notifications Projects can rely on the design of apps that are pitched to stakeholders who are notified on their phone with relevant updates. INFORM Easy and cost-effective to distribute information to large numbers of people; Rapid information flows bolster the provision of relevant and timely news and updates. More significant initial investment in developing an app; will not be available to people without smartphones. :fontawesome-solid-people-group: Community fora Events in which panels of experts share their knowledge on issues and then stakeholders can ask questions. INFORM Can inform people with more relevant information by providing them with the opportunity to ask questions; brings community together in a shared space of public communication. More time-consuming and resource intensive to organise; might attract smaller numbers of people and self-selecting groups rather than representative subsets of the population; effectiveness is constrained by forum capacity. :fontawesome-solid-list-check: Online Surveys Survey sent via email, embedded in a website, shared via social media, etc. CONSULT Cost-effective; simple mass- distribution. Risk of pre-emptive evaluative framework when designing questions; Does not reach those without internet connection or computer/smartphone access. :material-phone-in-talk: Phone Interviews Structured or semi-structured interviews held over the phone. CONSULT PARTNER Opportunity for stakeholders to voice concerns more openly. Risk of pre-emptive evaluative framework when designing questions; Might exclude portions of the populations without phone access or with habits of infrequent phone use. :fontawesome-solid-door-closed: Door-to-door interviews Structured or semi-structured interviews held in-person at people\u2019s houses. CONSULT PARTNER Opportunity for stakeholders to voice concerns more openly; can allow participants the opportunity to form connections through empathy and face-to- face communication. Potential for limited interest to engage with interviewers; time-consuming; can be seen by interviewees as intrusive or burdensome. :fontawesome-solid-people-arrows-left-right: In-person interviews Short interviews conducted in- person in public spaces. CONSULT PARTNER Can reach many people and a representative subset of the population if stakeholders are appropriately defined and sortition is used. Less targeted; pertinent stakeholders must be identified by area; little time/interest to engage with interviewer; can be viewed by interviewees as time- consuming and burdensome. :material-selection-search: Focus Groups A group of stakeholders brought together and asked their opinions on a particular issue. Can be more or less formally structured. CONSULT PARTNER Can gather in-depth information; Can lead to new insights and directions that were not anticipated by the project team. Subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. :fontawesome-solid-users-between-lines: Online Workshops Workshops using digital tools such as collaborative platforms. CONSULT Opportunity to reach stakeholders across regions, increased accessibility depending on digital access. Potential barriers to accessing tools required for participation, potential for disengagement. :material-crowd: Crowdsourcing (Online) Well-designed tasks that can be undertaken by a distributed collective, with individuals working on separate components. CONSULT PARTNER Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access. Supports increased potential for diverse forms of expertise and experience. Can be misused as a method for outsourcing cheap labour; potential barriers to accessing tools required for participation; potential for disengagement; difficult to ensure accuracy and validity of input from participants. :material-github: Distributed Project Collaboration (Online) Online digital platforms, such as GitHub, enable new forms of citizen science and collaborative development on diverse projects (e.g., open source software, open science). CONSULT PARTNER EMPOWER Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access; increased potential for diverse forms of expertise and experience; empowers new communities to actively participate in shaping and building tools that have real value for their communities. Potential barriers to accessing digital tools required for participation, including high levels of digital literacy. :fontawesome-solid-building-columns: Citizen panel or assembly Large groups of people (dozens or even thousands) who are representative of a town/region. INFORM CONSULT PARTNER EMPOWER Provides an opportunity for co-production of outputs; can produce insights and directions that were not anticipated by the project team; can provide an information base for conducting further outreach (surveys, interviews, focus groups, etc.); can be broadly representative; can bolster a community\u2019s sense of democratic agency and solidarity. Participant rolls must be continuously updated to ensure panels or assemblies remains representative of the population throughout their lifespan; resource-intensive for establishment and maintenance; subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. :material-account-group: Citizen jury A small group of people (between 12 and 24), representative of the demographics of a given area, come together to deliberate on an issue (generally one clearly framed set of questions), over the period of 2 to 7 days ( Involve.org.uk . INFORM CONSULT PARTNER EMPOWER Can gather in-depth information; can produce insights and directions that were not anticipated by the project team; can bolster participants\u2019 sense of democratic agency and solidarity. Subject to hazards of group think; complex to facilitate; risk of pre-emptive evaluative framework; small sample of citizens involved risks low representativeness of wider range of public opinions and beliefs. As with all forms of engagement, deciding on the best method requires awareness of your audience. Consider the following cases: === \":octicons-log-16: Policymakers\" A research team has released results from an economics study that could have a positive impact on public policy. They decide to share these results with policymakers. The goal is to directly influence policy. Therefore, the results need to be clearly communicated and also connected to the policy goal. This connection is important to help ensure that policy-makers are able to evaluate the wider implications of the scientific findings. **Communication Goal:** to demonstrate how scientific findings can support evidence-based policy impact === \":octicons-people-16: General Public\" As part of an education outreach campaign to improve digital literacy among adolescents, a mental health charity are running workshops with secondary school students. They wish to communicate recent evidence about the impact of over-using social media on mental health. Rather than communicating complicated statistical information about the methods used in their study, the team develop a more accessible form of their findings and link these findings to practical steps that the students can take to protect themselves online. **Communication Goal:** to build awareness of possible risks associated with excessive social media usage and support behavioural change strategies === \":octicons-beaker-16: Researchers\" A PhD student working in a Physics department has results from a recent study that developed and tested a new method for the large-scale data mining of astronomical data. The PhD student wishes to present this new method and the validation study at an upcoming international conference for data science. The audience will be technically literate, but will not have specialist expertise in astronomy. Therefore, the PhD student describes the method in the context of its original study buy also emphasises the generalisability for other sciences (e.g. genomics). **Communication Goal**: to advance academic career by gaining experience of presenting conference papers and also generating interest in a novel data science method.","title":"Selecting a Stakeholder Engagement Method"},{"location":"aeg/chapter3/impact/","text":"Stakeholder Impact Assessment \u00b6 Designers and users of AI systems should remain aware that these technologies may have transformative and long-term effects on individuals and society. To ensure that the deployment of an AI system remains sustainable and supports the sustainability of the communities it will affect, the project team should proceed with a continuous sensitivity to the real-world impacts that the system will have. The team should come together to evaluate the social impact and sustainability of an AI project through what is known as a Stakeholder Impact Assessment (SIA). The SUM values form the basis of the SIA. They are intended as a launching point for open and inclusive conversations about the individual and societal impacts of data science research and AI innovation projects rather than to provide a comprehensive inventory of moral concerns and solutions. At the very outset of any project, these should provide the normative point of departure for collaborative and anticipatory reflection, while, at the same time, allowing for the respectful and interculturally sensitive inclusion of other points of view. Objectives of a SIA The purpose of carrying out a SIA is multidimensional. SIAs can serve several purposes, some of which include: Help to build public confidence that the design and deployment of the AI system has been done responsibly. Facilitate and strengthen your accountability framework. Bring to light unseen risks that threaten to affect individuals and the public good. Underwrite well-informed decision-making and transparent innovation practices . Demonstrate forethought and due diligence not only within an organisation but also to the wider public. AI projects may require different kinds of impact assessments. For example Data Protection Law requires data protection impact assessments (DPIAs) to be carried out in cases where the processing of personal data is likely to result in a high risk to individuals. DPIAs assess the necessity and proportionality of the processing of personal data, identify risks that may emerge in that processing, and present measures taken to mitigate those risks. Another example are equality impact assessments (EIAs) which aid in fulfiling the requirements of equality legislation. While both DPIAs and EIAs provide relevant insights with respect to the ethical stakes of AI innovation projects, they go only part of the way in identifying and assessing the full range of potential individual and societal impacts of the design, development, and deployment of AI systems. Reaching a comprehensive assessment of these impacts is the purpose of SIAs. Key Concept: Stakeholder Impact Assessment SIAs are tools that create a procedure for, and a means of documenting, the collaborative evaluation and reflective anticipation of the possible harms and benefits of AI innovation projects. SIAs are not intended to replace DPIAs or EIAs, which are obligatory. Rather, SIAs are meant to be integrated into the wider impact assessment regime as a way to demonstrate that sufficient attention has been paid to the ethical permissibility, transparency, accountability, and equity of AI innovation projects. There are three critical points in the AI project lifecycle at which the project team should convene to impact the social impact and sustainability of a project: === \":octicons-log-16: Design\" A SIA should be carried out to determine the ethical permissibility of the project. As a starting point, the team should refer to the SUM Values for the considerations of the possible effects of the project on individual wellbeing and public welfare. This should include a stakeholder engagement and involvement component in the initial SIA through methods established in the initial Project Summary Report (PS Report), so public views can be considered in ways that are proportional to potential project impacts and appropriate to team positionality. This will bolster the inclusion of a diversity of voices and opinions into the design and development process through the participation of a more representative range of stakeholders. The Design Phase SIA includes a revisitation of the Project Summary Report, where engagement objectives and methods for the Development Phase SIA were first established. These, and other relevant project revisions should be reflected in an update to the PS Report. === \":material-developer-board: Develop\" Once a model has been trained, tested, and validated, the project team should revisit the initial SIA to confirm that the AI system to be implemented is still in line with the evaluations and conclusions of the original assessment. This check-in should be logged on the Development Phase section of the SIA with any applicable changes added and discussed. The method of stakeholder engagement that accompanies the SIA process will have been initially established in the PS report and revisited in the Design Phase SIA. This report should be revisited again during the Development Phase SIA and updated where needed. At this point the team must also set a timeframe for re-assessment once the system is in operation. Timeframes for these re-assessments should be decided by the team on a case-by-case basis but should be proportional to the scale of the potential impact of the system on the individuals and communities it will affect. === \":material-network-pos: Deploy\" Once an AI system has gone live, the team should iteratively revisit and re-evaluate the SIA. These checkins should be logged on the Deployment Phase section of the SIA with any applicable changes added and discussed. Deployment-Phase SIAs should focus both on evaluating the existing SIA against real world impacts and on considering how to mitigate the unintended consequences that may have ensued in the wake of the deployment of the system. As with each SIA iteration, the PS report is revisited at this point, when objectives, methods, and timeframes for the next Deployment Phase SIA are established. Skills for Conducting SIA's \u00b6 Weighing the values and considering trade-offs \u00b6 Taking the SUM values as a starting point of conversation and for the SIA, there will come times when these values come into conflict with one another and decisions will have to be made on which value to prioritise. Within a team, discussion should be encouraged on how to weigh the values against one another and how to consider trade-offs should use-case specific circumstances arise when the values come into tension with each other. For instance, there may be circumstances where the use of an AI system could optimally advance the public interest only at the cost of safeguarding the wellbeing or the autonomy of a given individual. In other cases, the use of an AI system could preserve the wellbeing of a particular individual only at the cost of the autonomy of another or of the public welfare more generally. This issue of adjudicating between conflicting values has long been a crucial and thorny dimension of collective life, and the problem of discovering reasonable ways to overcome the disagreements that arise as a result of the plurality of human values has occupied thinkers for just as long. Nonetheless, over the course of the development of modern democratic and plural societies, several useful approaches to managing the tension between conflicting values have emerged. Consequences-based and principles-based approaches to balancing values \u00b6 Let's go back to some concepts introduced in chapter 1 that will be useful when trying to balance the tension between values: consequences-based moral thinking or consequentialism and principles-based moral thinking or deontology. These can be seen as procedural tools for thinking through a given dilemma in weighing values. As a quick reminder, a consequence-based approach asks that, in judging the moral correctness of an action, one prioritise considerations of the goodness produced by an outcome. In other words, because what matters most is the consequences of ones actions, the goodness of these consequences should be maximised. Standards of right and wrong (indicators of what one ought to do) are determined, on this view, by whether the action taken maximises overall goodness of the consequences rather than by the principles or standards one applies when acting. A principles-based approach takes the opposite tack. The rightness of an action is determined, from this standpoint, by the intentional application of a universally applicable standard, maxim, or principle. Rather than basing the morality of conduct on the ends served by it, this approach anchors rightness in the duty or obligation of the individual agent to follow a rationally determined (and therefore \u201cuniversalisable\u201d) principle. For deontological or principles-based ethics, the integrity of the principled action and intention matters most, and so justified constraints, which are rooted in the priority to act according to moral standards, must be put on the pursuit of the achievement of one\u2019s goals. Knowing when to prioritise consequences and when to prioritise principles in moral deliberations is a tricky matter, and applying either a consequentialist or deontological approach (or both) may make sense depending upon the context. To take a familiar example, in deontologically following the principle \u2018Thou shall not lie,\u2019 you would be justifiably constrained from stealthily deceiving and misleading others to get ahead in your job. The principle matters here. But, in a different situation, say, where lying to a murderer, who appears at your front door, would save an innocent victim whom you are concealing in your cellar, the prioritisation of consequences makes more sense. Consider another example: In an overburdened council, the introduction of an automated system for making sites available for development would vastly expedite housing delivery. The implementation of this AI system would thus produce a consequence that could be beneficial to the public. Yet, it may, among other things, simultaneously do damage to the value of Connect (which safeguards interpersonal dialogue, meaningful human connection and social cohesion) by eliminating time intensive consultation processes that facilitate interpersonal communication, trust building, and social bonding between council staff and residents. How then could one go about weighing the value of improving public welfare against the value of respecting the integrity of interpersonal relations? One way would be to place each side of this comparison under the rubric of either consequences or principles and then measure them up against each other accordingly. From one perspective, the publicly beneficial consequences of improving service delivery might outweigh the publicly harmful consequences of impairing social cohesion. On a different view, such a trade-off would be unacceptable, because the principle of respecting the integrity of social cohesion trumps any solidarity-harming but publicly beneficial consequences whatsoever. The answers to these questions will always be tricky, but deliberation between team members and stakeholders should always be a part of arriving to any sort of consensus (which we will focus on in the next section). Getting clear on the consequences and the principles involved in a specific case of conflicting values will allow team members to get a better picture of the practical and moral stakes at play in a particular project. It will also help the team get a sharper idea of the proportionality of using an AI technology to achieve a desired outcome given both its potential ethical impacts and the social needs to which it is responding. When drawn upon for guidance, consequentialism and deontology can provide a kind of procedural scale upon which to place, measure, and weigh conflicting values. They are practical tools that can be used within meaningful deliberation. Ensuring meaningful and inclusive deliberation \u00b6 The most general approach is to encourage mutually respectful, sincere, and well-informed dialogue, so that reasons from all affected voices can be heard and considered. Deliberations that have been inclusive, open, and impartial tend to generate better and more inferentially sound conclusions, so approaching the adjudication of conflicting values in this manner will likely improve mutual understanding of the rationales and perspectives, which inform those values. Whether or not this ends up being the case in each conversational context, the importance of cultivating a culture of innovation, which encourages reciprocally respectful, open, non-coercive, and mutually accountable communication must be stressed. The success of the modern sciences (which have been built on the dynamic foundations of inclusive, rational, and democratic communication) is perhaps evidence enough to support the validity of this emphasis. Here, procedural ethics is crucial. The importance of meaningful dialogue in balancing values is rooted in central role played in it by the rational exchange and assessment of ideas and beliefs. The validity of the claims we make in conversations about values is bound by practices of giving and asking for reasons. A claim about values that is justified is one that convinces by the unforced strength of the better or more compelling argument. Rational justification and persuasive reason-giving are, in fact, central elements of legitimate and consensus-oriented moral decision making. And, along the same lines, claims made about moral values or properties are subject to critical evaluation vis-\u00e0-vis their inferential strengths and weaknesses. Another way to understand the importance of meaningful dialogue in balancing values has to do with the way that such a procedural view of ethical thinking enables open communication about prioritising values without imposing substantive views about the values themselves. Instead, an emphasis on rational communication in deliberations looks to secure a justified and equitable process of exchanging and evaluating reasons. It starts with the question: what are the enabling conditions in the interpersonal communication of values and beliefs that allow interlocutors to come to defensible and rationally acceptable moral judgments and reason-based consensus? To answer this question, moral thinkers over the past century have endeavoured to reconstruct the practical assumptions behind and presuppositions of rational communication (a summary of the most essential of such assumptions and presuppositions is provided below). Creating a reflective and practicable awareness of these assumptions and presuppositions among members of a team can play a crucial role in creating an innovation environment that is optimally conducive meaningful and inclusive deliberation. Preconditions of meaningful deliberation: Impartiality Interlocutors engaging in meaningful deliberation must consider the interests of all those who are affected by their actions equally. Thinking impartially involves taking on the view of others to try to put oneself in their place. Non-coercion Meaningful deliberation must be free from any sort of implicit or explicit coercion, force, or restriction that would prevent the open and unconstrained exchange of reasons. Sincerity Meaningful deliberation must be free from any sort of deception or duplicity that would prevent the authentic exchange of reasons. Interlocutors must mean what they say. Consistency and coherence Arguments and positions offered in meaningful deliberation must be clear, free from contradictions, and hold together collectively in an understandable way. Mutual respect and egalitarian reciprocity All interlocutors must be treated with respect and given equal opportunity to contribute to the conversation. All voices are worthy of equal consideration in processes of exchanging reasons. Inclusivity and publicity Anyone whose interests are affected by an issue and who could make a contribution to better understanding it must not be excluded from participating in deliberation. All relevant voices must be heard and all relevant information considered. Addressing and mitigating power dynamics that may obstruct meaningful and inclusive deliberation \u00b6 The stewardship of meaningful and inclusive dialogue is critical to safeguarding the collective weighing up of values. However, there is an important potential barrier to meaningful deliberation that challenges its feasibility and must be addressed. As guiding assumptions of rational communication, norms like sincerity, impartiality, non-coercion, and inclusiveness may strike some as overly idealistic. In the real world, discussions are rarely fully inclusive, informed, and free of assertive manipulation, coercion, and deception. Rather, deliberation and dialogue are often steered by and crafted to protect the interests of the dominant. Likewise, differential power relationships (for instance, divergent educational backgrounds that derive from differential socioeconomic privileges) create power imbalances that fundamentally challenge the conditions of reciprocity and equal footing that are needed for justified and equitable communication. A team should confront these obstacles to meaningful deliberation head-on through a power-aware approach to facilitating collaborative reflection, dialogue, and engagement. An awareness of and sensitivity to the differential relationships of power that can suppress the full participation of disadvantaged or marginalised voices can better encourage an inclusive, open, and equal opportunity conversation between participants. Clear-headed explorations of power dynamics between civil servants, scientists, citizens, domain experts, and policymakers can assist in avoiding the kind of deficiencies of representation and empowerment that risk reinforcing existing power structures and inequalities. This may involve active mitigation measures like the provision of training, upskilling, and technical resources to those who have lacked access to them. Above all, the norms of meaningful deliberation makes teams aware of the possible distortions of communication (i.e. a lack of egalitarian reciprocity, non-coercion, sincerity, etc.) that must be tackled and rectified for the hurdles of power disparities to be scaled.","title":"Stakeholder Impact Assessment"},{"location":"aeg/chapter3/impact/#stakeholder-impact-assessment","text":"Designers and users of AI systems should remain aware that these technologies may have transformative and long-term effects on individuals and society. To ensure that the deployment of an AI system remains sustainable and supports the sustainability of the communities it will affect, the project team should proceed with a continuous sensitivity to the real-world impacts that the system will have. The team should come together to evaluate the social impact and sustainability of an AI project through what is known as a Stakeholder Impact Assessment (SIA). The SUM values form the basis of the SIA. They are intended as a launching point for open and inclusive conversations about the individual and societal impacts of data science research and AI innovation projects rather than to provide a comprehensive inventory of moral concerns and solutions. At the very outset of any project, these should provide the normative point of departure for collaborative and anticipatory reflection, while, at the same time, allowing for the respectful and interculturally sensitive inclusion of other points of view. Objectives of a SIA The purpose of carrying out a SIA is multidimensional. SIAs can serve several purposes, some of which include: Help to build public confidence that the design and deployment of the AI system has been done responsibly. Facilitate and strengthen your accountability framework. Bring to light unseen risks that threaten to affect individuals and the public good. Underwrite well-informed decision-making and transparent innovation practices . Demonstrate forethought and due diligence not only within an organisation but also to the wider public. AI projects may require different kinds of impact assessments. For example Data Protection Law requires data protection impact assessments (DPIAs) to be carried out in cases where the processing of personal data is likely to result in a high risk to individuals. DPIAs assess the necessity and proportionality of the processing of personal data, identify risks that may emerge in that processing, and present measures taken to mitigate those risks. Another example are equality impact assessments (EIAs) which aid in fulfiling the requirements of equality legislation. While both DPIAs and EIAs provide relevant insights with respect to the ethical stakes of AI innovation projects, they go only part of the way in identifying and assessing the full range of potential individual and societal impacts of the design, development, and deployment of AI systems. Reaching a comprehensive assessment of these impacts is the purpose of SIAs. Key Concept: Stakeholder Impact Assessment SIAs are tools that create a procedure for, and a means of documenting, the collaborative evaluation and reflective anticipation of the possible harms and benefits of AI innovation projects. SIAs are not intended to replace DPIAs or EIAs, which are obligatory. Rather, SIAs are meant to be integrated into the wider impact assessment regime as a way to demonstrate that sufficient attention has been paid to the ethical permissibility, transparency, accountability, and equity of AI innovation projects. There are three critical points in the AI project lifecycle at which the project team should convene to impact the social impact and sustainability of a project: === \":octicons-log-16: Design\" A SIA should be carried out to determine the ethical permissibility of the project. As a starting point, the team should refer to the SUM Values for the considerations of the possible effects of the project on individual wellbeing and public welfare. This should include a stakeholder engagement and involvement component in the initial SIA through methods established in the initial Project Summary Report (PS Report), so public views can be considered in ways that are proportional to potential project impacts and appropriate to team positionality. This will bolster the inclusion of a diversity of voices and opinions into the design and development process through the participation of a more representative range of stakeholders. The Design Phase SIA includes a revisitation of the Project Summary Report, where engagement objectives and methods for the Development Phase SIA were first established. These, and other relevant project revisions should be reflected in an update to the PS Report. === \":material-developer-board: Develop\" Once a model has been trained, tested, and validated, the project team should revisit the initial SIA to confirm that the AI system to be implemented is still in line with the evaluations and conclusions of the original assessment. This check-in should be logged on the Development Phase section of the SIA with any applicable changes added and discussed. The method of stakeholder engagement that accompanies the SIA process will have been initially established in the PS report and revisited in the Design Phase SIA. This report should be revisited again during the Development Phase SIA and updated where needed. At this point the team must also set a timeframe for re-assessment once the system is in operation. Timeframes for these re-assessments should be decided by the team on a case-by-case basis but should be proportional to the scale of the potential impact of the system on the individuals and communities it will affect. === \":material-network-pos: Deploy\" Once an AI system has gone live, the team should iteratively revisit and re-evaluate the SIA. These checkins should be logged on the Deployment Phase section of the SIA with any applicable changes added and discussed. Deployment-Phase SIAs should focus both on evaluating the existing SIA against real world impacts and on considering how to mitigate the unintended consequences that may have ensued in the wake of the deployment of the system. As with each SIA iteration, the PS report is revisited at this point, when objectives, methods, and timeframes for the next Deployment Phase SIA are established.","title":"Stakeholder Impact Assessment"},{"location":"aeg/chapter3/impact/#skills-for-conducting-sias","text":"","title":"Skills for Conducting SIA's"},{"location":"aeg/chapter3/impact/#weighing-the-values-and-considering-trade-offs","text":"Taking the SUM values as a starting point of conversation and for the SIA, there will come times when these values come into conflict with one another and decisions will have to be made on which value to prioritise. Within a team, discussion should be encouraged on how to weigh the values against one another and how to consider trade-offs should use-case specific circumstances arise when the values come into tension with each other. For instance, there may be circumstances where the use of an AI system could optimally advance the public interest only at the cost of safeguarding the wellbeing or the autonomy of a given individual. In other cases, the use of an AI system could preserve the wellbeing of a particular individual only at the cost of the autonomy of another or of the public welfare more generally. This issue of adjudicating between conflicting values has long been a crucial and thorny dimension of collective life, and the problem of discovering reasonable ways to overcome the disagreements that arise as a result of the plurality of human values has occupied thinkers for just as long. Nonetheless, over the course of the development of modern democratic and plural societies, several useful approaches to managing the tension between conflicting values have emerged.","title":"Weighing the values and considering trade-offs"},{"location":"aeg/chapter3/impact/#consequences-based-and-principles-based-approaches-to-balancing-values","text":"Let's go back to some concepts introduced in chapter 1 that will be useful when trying to balance the tension between values: consequences-based moral thinking or consequentialism and principles-based moral thinking or deontology. These can be seen as procedural tools for thinking through a given dilemma in weighing values. As a quick reminder, a consequence-based approach asks that, in judging the moral correctness of an action, one prioritise considerations of the goodness produced by an outcome. In other words, because what matters most is the consequences of ones actions, the goodness of these consequences should be maximised. Standards of right and wrong (indicators of what one ought to do) are determined, on this view, by whether the action taken maximises overall goodness of the consequences rather than by the principles or standards one applies when acting. A principles-based approach takes the opposite tack. The rightness of an action is determined, from this standpoint, by the intentional application of a universally applicable standard, maxim, or principle. Rather than basing the morality of conduct on the ends served by it, this approach anchors rightness in the duty or obligation of the individual agent to follow a rationally determined (and therefore \u201cuniversalisable\u201d) principle. For deontological or principles-based ethics, the integrity of the principled action and intention matters most, and so justified constraints, which are rooted in the priority to act according to moral standards, must be put on the pursuit of the achievement of one\u2019s goals. Knowing when to prioritise consequences and when to prioritise principles in moral deliberations is a tricky matter, and applying either a consequentialist or deontological approach (or both) may make sense depending upon the context. To take a familiar example, in deontologically following the principle \u2018Thou shall not lie,\u2019 you would be justifiably constrained from stealthily deceiving and misleading others to get ahead in your job. The principle matters here. But, in a different situation, say, where lying to a murderer, who appears at your front door, would save an innocent victim whom you are concealing in your cellar, the prioritisation of consequences makes more sense. Consider another example: In an overburdened council, the introduction of an automated system for making sites available for development would vastly expedite housing delivery. The implementation of this AI system would thus produce a consequence that could be beneficial to the public. Yet, it may, among other things, simultaneously do damage to the value of Connect (which safeguards interpersonal dialogue, meaningful human connection and social cohesion) by eliminating time intensive consultation processes that facilitate interpersonal communication, trust building, and social bonding between council staff and residents. How then could one go about weighing the value of improving public welfare against the value of respecting the integrity of interpersonal relations? One way would be to place each side of this comparison under the rubric of either consequences or principles and then measure them up against each other accordingly. From one perspective, the publicly beneficial consequences of improving service delivery might outweigh the publicly harmful consequences of impairing social cohesion. On a different view, such a trade-off would be unacceptable, because the principle of respecting the integrity of social cohesion trumps any solidarity-harming but publicly beneficial consequences whatsoever. The answers to these questions will always be tricky, but deliberation between team members and stakeholders should always be a part of arriving to any sort of consensus (which we will focus on in the next section). Getting clear on the consequences and the principles involved in a specific case of conflicting values will allow team members to get a better picture of the practical and moral stakes at play in a particular project. It will also help the team get a sharper idea of the proportionality of using an AI technology to achieve a desired outcome given both its potential ethical impacts and the social needs to which it is responding. When drawn upon for guidance, consequentialism and deontology can provide a kind of procedural scale upon which to place, measure, and weigh conflicting values. They are practical tools that can be used within meaningful deliberation.","title":"Consequences-based and principles-based approaches to balancing values"},{"location":"aeg/chapter3/impact/#ensuring-meaningful-and-inclusive-deliberation","text":"The most general approach is to encourage mutually respectful, sincere, and well-informed dialogue, so that reasons from all affected voices can be heard and considered. Deliberations that have been inclusive, open, and impartial tend to generate better and more inferentially sound conclusions, so approaching the adjudication of conflicting values in this manner will likely improve mutual understanding of the rationales and perspectives, which inform those values. Whether or not this ends up being the case in each conversational context, the importance of cultivating a culture of innovation, which encourages reciprocally respectful, open, non-coercive, and mutually accountable communication must be stressed. The success of the modern sciences (which have been built on the dynamic foundations of inclusive, rational, and democratic communication) is perhaps evidence enough to support the validity of this emphasis. Here, procedural ethics is crucial. The importance of meaningful dialogue in balancing values is rooted in central role played in it by the rational exchange and assessment of ideas and beliefs. The validity of the claims we make in conversations about values is bound by practices of giving and asking for reasons. A claim about values that is justified is one that convinces by the unforced strength of the better or more compelling argument. Rational justification and persuasive reason-giving are, in fact, central elements of legitimate and consensus-oriented moral decision making. And, along the same lines, claims made about moral values or properties are subject to critical evaluation vis-\u00e0-vis their inferential strengths and weaknesses. Another way to understand the importance of meaningful dialogue in balancing values has to do with the way that such a procedural view of ethical thinking enables open communication about prioritising values without imposing substantive views about the values themselves. Instead, an emphasis on rational communication in deliberations looks to secure a justified and equitable process of exchanging and evaluating reasons. It starts with the question: what are the enabling conditions in the interpersonal communication of values and beliefs that allow interlocutors to come to defensible and rationally acceptable moral judgments and reason-based consensus? To answer this question, moral thinkers over the past century have endeavoured to reconstruct the practical assumptions behind and presuppositions of rational communication (a summary of the most essential of such assumptions and presuppositions is provided below). Creating a reflective and practicable awareness of these assumptions and presuppositions among members of a team can play a crucial role in creating an innovation environment that is optimally conducive meaningful and inclusive deliberation. Preconditions of meaningful deliberation: Impartiality Interlocutors engaging in meaningful deliberation must consider the interests of all those who are affected by their actions equally. Thinking impartially involves taking on the view of others to try to put oneself in their place. Non-coercion Meaningful deliberation must be free from any sort of implicit or explicit coercion, force, or restriction that would prevent the open and unconstrained exchange of reasons. Sincerity Meaningful deliberation must be free from any sort of deception or duplicity that would prevent the authentic exchange of reasons. Interlocutors must mean what they say. Consistency and coherence Arguments and positions offered in meaningful deliberation must be clear, free from contradictions, and hold together collectively in an understandable way. Mutual respect and egalitarian reciprocity All interlocutors must be treated with respect and given equal opportunity to contribute to the conversation. All voices are worthy of equal consideration in processes of exchanging reasons. Inclusivity and publicity Anyone whose interests are affected by an issue and who could make a contribution to better understanding it must not be excluded from participating in deliberation. All relevant voices must be heard and all relevant information considered.","title":"Ensuring meaningful and inclusive deliberation"},{"location":"aeg/chapter3/impact/#addressing-and-mitigating-power-dynamics-that-may-obstruct-meaningful-and-inclusive-deliberation","text":"The stewardship of meaningful and inclusive dialogue is critical to safeguarding the collective weighing up of values. However, there is an important potential barrier to meaningful deliberation that challenges its feasibility and must be addressed. As guiding assumptions of rational communication, norms like sincerity, impartiality, non-coercion, and inclusiveness may strike some as overly idealistic. In the real world, discussions are rarely fully inclusive, informed, and free of assertive manipulation, coercion, and deception. Rather, deliberation and dialogue are often steered by and crafted to protect the interests of the dominant. Likewise, differential power relationships (for instance, divergent educational backgrounds that derive from differential socioeconomic privileges) create power imbalances that fundamentally challenge the conditions of reciprocity and equal footing that are needed for justified and equitable communication. A team should confront these obstacles to meaningful deliberation head-on through a power-aware approach to facilitating collaborative reflection, dialogue, and engagement. An awareness of and sensitivity to the differential relationships of power that can suppress the full participation of disadvantaged or marginalised voices can better encourage an inclusive, open, and equal opportunity conversation between participants. Clear-headed explorations of power dynamics between civil servants, scientists, citizens, domain experts, and policymakers can assist in avoiding the kind of deficiencies of representation and empowerment that risk reinforcing existing power structures and inequalities. This may involve active mitigation measures like the provision of training, upskilling, and technical resources to those who have lacked access to them. Above all, the norms of meaningful deliberation makes teams aware of the possible distortions of communication (i.e. a lack of egalitarian reciprocity, non-coercion, sincerity, etc.) that must be tackled and rectified for the hurdles of power disparities to be scaled.","title":"Addressing and mitigating power dynamics that may obstruct meaningful and inclusive deliberation"},{"location":"aeg/chapter4/","text":"Fairness & Bias Mitigation, Transparency, Explainability, and Governance \u00b6 Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk)) Chapter Outline \u00b6 Introduction to Fairness AI Fairness Bias Mitigation Accountability Governance Chapter Summary In this chapter we explore various issues that arise within AI systems. We will look at the elements of AI fairness, and the strategies to conduct bias mitigation. Finally, we will touch upon the concepts of accountability and governance, and how they can be integrated into the AI project lifecycle. Learning Objectives In this chapter, you will: Learn about the landscape of meanings the concept of fairness has, as well as how to apply it in the context of AI. Understand the different elements of AI fairness and how they relate to one another. Look at the concept of bias in AI systems, the different kinds of biases which may arise, as well as how to mitigate them. Familiarise yourself with AI accountability, its main components, and how it relates to AI governance.","title":"Introduction"},{"location":"aeg/chapter4/#fairness-bias-mitigation-transparency-explainability-and-governance","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))","title":"Fairness &amp; Bias Mitigation, Transparency, Explainability, and Governance"},{"location":"aeg/chapter4/#chapter-outline","text":"Introduction to Fairness AI Fairness Bias Mitigation Accountability Governance Chapter Summary In this chapter we explore various issues that arise within AI systems. We will look at the elements of AI fairness, and the strategies to conduct bias mitigation. Finally, we will touch upon the concepts of accountability and governance, and how they can be integrated into the AI project lifecycle. Learning Objectives In this chapter, you will: Learn about the landscape of meanings the concept of fairness has, as well as how to apply it in the context of AI. Understand the different elements of AI fairness and how they relate to one another. Look at the concept of bias in AI systems, the different kinds of biases which may arise, as well as how to mitigate them. Familiarise yourself with AI accountability, its main components, and how it relates to AI governance.","title":"Chapter Outline"},{"location":"aeg/chapter4/accountability/","text":"Accountability in AI \u00b6 Introduction to accountability \u00b6 What does accountability mean? It means that humans are answerable for the parts they play across the entire AI design, development, and deployment. It also demands that the results of this work are traceable from start to finish. According to the principle of fairness, designers and implementers are held accountable for being equitable and for not harming anyone through bias or discrimination. According to the principle of sustainability, designers and implementers are held accountable for producing AI innovation that is safe and ethical in its outcomes and wider impacts. And according to the principle of explainability, designers and implementers are held accountable for making sure that any decision the AI system makes, or provides support for humans to make, can be adequately explained to relevant stakeholders. Therefore, the principle of accountability is an end-to-end governing principle. Responsible AI project delivery requires confronting two relevant challenges to accountability. Accountability gap Automated decisions are not self-justifiable. Whereas human agents can be called to account for their judgements and decisions in instances where those judgments and decisions affect the interests of others, the statistical models and underlying hardware that compose AI systems are not responsible in the same morally relevant sense. This creates an accountability gap that must be addressed so that clear and imputable sources of human answerability can be attached to decisions assisted or produced by an AI system. Complexity of AI production processes Establishing human answerability is not a simple matter when it comes to the design, development, and deployment of AI systems. This is due to the complexity and multi-agent character of the production and use of these systems. Typically, AI project delivery workflows include department and delivery leads, technical experts, data procurement and preparation personnel, policy and domain experts, implementers, and others. Due to this production complexity, it may become difficult to answer the question of who among these parties involved in the production of AI systems should bear responsibility if these systems\u2019 uses have negative consequences and impacts. Meeting the special requirements of accountability, which are born out of these two challenges, calls for a sufficiently fine-grained concept of what would make an AI project properly accountable. This concept can be broken down into two subcomponents of accountability, answerability and auditability: Answerability The principle of accountability demands that the onus of justifying algorithmically supported decisions be placed on the shoulders of the human creators and users of those AI systems. This means that it is essential to establish a continuous chain of human responsibility across the whole AI project delivery workflow. Making sure that accountability is effective from end to end necessitates that no gaps be permitted in the answerability of responsible human authorities from first steps of the design of an AI system to its algorithmically steered outcomes. Answerability also demands that explanations and justifications of both the rationale underlying the results of an AI system and the processes behind their production and use be offered by competent human authorities in plain, understandable, and coherent language. These explanations and justifications should be based upon sincere, consistent, sound, and impartial reasons that are accessible to non-technical hearers. Auditability Whereas the notion of answerability responds to the question of who is accountable for an automation supported outcome, the notion of auditability answers the question of how the designers and implementers of AI systems are to be held accountable. This aspect of accountability has to do with demonstrating both the responsibility of design, development, and deployment practices and the justifiability of outcomes. Auditability also has to do with traceability; It refers to the process by which all stages of the AI innovation lifecycle from data collection and model selection to system deployment, updating, and deprovisioning are documented in a way that is accessible and easily understood. The project team must ensure that every step of the process of designing and implementing an AI project is accessible for audit, oversight, and review by appropriate parties. Successful auditability requires builders and implementers of algorithmic systems to: keep records and to make available information that enables monitoring of the soundness and diligence of the innovation processes that produced these systems keep track of the accountable parties within an organisation\u2019s project team and others involved in the supply chain (where system components are procured) keep track of the governance actions taken across the entire AI innovation workflow keep records and make accessible information that enables monitoring of data provenance and analysis from the stages of collection, pre-processing, and modelling to training, testing, and deploying. This is the purpose of the Dataset Factsheet. Moreover, auditability requires the team to enable peers and overseers to probe and to critically review the dynamic operation of the system in order to ensure that the procedures and operations which are producing the model\u2019s behaviour are safe, ethical, and fair. Practically transparent algorithmic models must be built for auditability, reproducible, and equipped for end-to-end recording and monitoring of their data processing.The deliberate incorporation of both of these elements of accountability (answerability and auditability) into the AI project lifecycle may be called accountability-by-design. Key Concept: Accountability-by-design AI systems must be designed to facilitate end-to-end answerability and auditability. This requires both responsible humans-in-the-loop across the entire design and implementation chain as well as activity monitoring protocols that enable end-to-end oversight and review. Types of accountability \u00b6 Accountability deserves consideration across the entire design and implementation workflow. As a best practice, the team should actively consider the different demands that accountability-by-design requires before and after the roll out of the AI project. We will refer to the process of ensuring accountability during the design and development stages of the AI project as \u2018anticipatory accountability\u2019. This is because the team is anticipating the project\u2019s accountability needs prior to it being completed. Following a similar logic, we will refer to the process of addressing accountability after the start of the deployment of your AI project as \u2018remedial accountability\u2019. This is because after the initial implementation of a system, the team is remedying any of the issues that may be raised by its effects and potential externalities. These two subtypes of accountability are sometimes referred to as ex-ante (or before-the-event) accountability and ex-post (after-the-event) accountability respectively. Anticipatory accountability \u00b6 Treating accountability as an anticipatory principle entails that the project team takes as of primary importance the decisions made and actions taken by them prior to the outcome of an algorithmically supported decision process. This kind of ex ante accountability should be prioritised over remedial accountability, which focuses instead on the corrective or justificatory measures that can be taken after that automation supported process had been completed. Ensuring that the AI project delivery processes are accountable prior to the actual application of the system in the world will bolster the soundness of design and implementation processes and thereby more effectively pre-empt possible harms to individual wellbeing and public welfare or other adverse impacts. Likewise, by establishing strong regimes of anticipatory accountability and by making the design and delivery process as open and publicly accessible as possible, the project team will put affected stakeholders in a position to make better informed and more knowledgeable decisions about their involvement with these systems in advance of potentially harmful impacts. Doing so will also strengthen the public narrative and help to safeguard the project from reputational harm. Example During the preprocessing phase of an AI production lifecycle, technical members of a project team are deciding which features to include, and which to leave out. To safeguard sufficient anticipatory accountability, they make sure to log which team members are involved in making these decisions and record the rationale behind the choices made. Remedial Accountability \u00b6 While remedial accountability should be seen, along these lines, as a necessary fallback rather than as a first resort for imputing responsibility in the design, development and deployment of AI systems, strong regimes of remedial accountability are no less important in providing necessary justifications for the bearing these systems have on the lives of affected stakeholders. Putting in place comprehensive auditability regimes as part of your accountability framework and establishing transparent design and use practices, which are methodically logged throughout the AI project delivery lifecycle, are essential components for this sort of remedial accountability. One aspect of remedial accountability that you must pay close attention to is the need to provide explanations to affected stakeholders for algorithmically supported decisions. Offering explanations for the results of algorithmically supported decision-making involves furnishing decision subjects and other interested parties with an understandable account of the rationale behind the specific outcome of interest. It also involves furnishing the decision subject and other interested parties with an explanation of the ethical permissibility, the fairness, and the safety of the use of the AI system. Example After receiving an unfavourable decision in their recruitment process, a job applicant seeks assurance and justification that the resume filtering AI system used in their process was not biased or discriminatory. To safeguard sufficient remedial accountability, the implementers of the system draw on the logged records of bias-mitigation measures and fairness-aware design practices to demonstrate the fair and non-discriminatory practices behind the production of the system. They also provide an explanation of the rationale behind the applicant\u2019s negative result, showing that the determinative factors behind the system\u2019s output were fair and reasonable.","title":"Accountability"},{"location":"aeg/chapter4/accountability/#accountability-in-ai","text":"","title":"Accountability in AI"},{"location":"aeg/chapter4/accountability/#introduction-to-accountability","text":"What does accountability mean? It means that humans are answerable for the parts they play across the entire AI design, development, and deployment. It also demands that the results of this work are traceable from start to finish. According to the principle of fairness, designers and implementers are held accountable for being equitable and for not harming anyone through bias or discrimination. According to the principle of sustainability, designers and implementers are held accountable for producing AI innovation that is safe and ethical in its outcomes and wider impacts. And according to the principle of explainability, designers and implementers are held accountable for making sure that any decision the AI system makes, or provides support for humans to make, can be adequately explained to relevant stakeholders. Therefore, the principle of accountability is an end-to-end governing principle. Responsible AI project delivery requires confronting two relevant challenges to accountability. Accountability gap Automated decisions are not self-justifiable. Whereas human agents can be called to account for their judgements and decisions in instances where those judgments and decisions affect the interests of others, the statistical models and underlying hardware that compose AI systems are not responsible in the same morally relevant sense. This creates an accountability gap that must be addressed so that clear and imputable sources of human answerability can be attached to decisions assisted or produced by an AI system. Complexity of AI production processes Establishing human answerability is not a simple matter when it comes to the design, development, and deployment of AI systems. This is due to the complexity and multi-agent character of the production and use of these systems. Typically, AI project delivery workflows include department and delivery leads, technical experts, data procurement and preparation personnel, policy and domain experts, implementers, and others. Due to this production complexity, it may become difficult to answer the question of who among these parties involved in the production of AI systems should bear responsibility if these systems\u2019 uses have negative consequences and impacts. Meeting the special requirements of accountability, which are born out of these two challenges, calls for a sufficiently fine-grained concept of what would make an AI project properly accountable. This concept can be broken down into two subcomponents of accountability, answerability and auditability: Answerability The principle of accountability demands that the onus of justifying algorithmically supported decisions be placed on the shoulders of the human creators and users of those AI systems. This means that it is essential to establish a continuous chain of human responsibility across the whole AI project delivery workflow. Making sure that accountability is effective from end to end necessitates that no gaps be permitted in the answerability of responsible human authorities from first steps of the design of an AI system to its algorithmically steered outcomes. Answerability also demands that explanations and justifications of both the rationale underlying the results of an AI system and the processes behind their production and use be offered by competent human authorities in plain, understandable, and coherent language. These explanations and justifications should be based upon sincere, consistent, sound, and impartial reasons that are accessible to non-technical hearers. Auditability Whereas the notion of answerability responds to the question of who is accountable for an automation supported outcome, the notion of auditability answers the question of how the designers and implementers of AI systems are to be held accountable. This aspect of accountability has to do with demonstrating both the responsibility of design, development, and deployment practices and the justifiability of outcomes. Auditability also has to do with traceability; It refers to the process by which all stages of the AI innovation lifecycle from data collection and model selection to system deployment, updating, and deprovisioning are documented in a way that is accessible and easily understood. The project team must ensure that every step of the process of designing and implementing an AI project is accessible for audit, oversight, and review by appropriate parties. Successful auditability requires builders and implementers of algorithmic systems to: keep records and to make available information that enables monitoring of the soundness and diligence of the innovation processes that produced these systems keep track of the accountable parties within an organisation\u2019s project team and others involved in the supply chain (where system components are procured) keep track of the governance actions taken across the entire AI innovation workflow keep records and make accessible information that enables monitoring of data provenance and analysis from the stages of collection, pre-processing, and modelling to training, testing, and deploying. This is the purpose of the Dataset Factsheet. Moreover, auditability requires the team to enable peers and overseers to probe and to critically review the dynamic operation of the system in order to ensure that the procedures and operations which are producing the model\u2019s behaviour are safe, ethical, and fair. Practically transparent algorithmic models must be built for auditability, reproducible, and equipped for end-to-end recording and monitoring of their data processing.The deliberate incorporation of both of these elements of accountability (answerability and auditability) into the AI project lifecycle may be called accountability-by-design. Key Concept: Accountability-by-design AI systems must be designed to facilitate end-to-end answerability and auditability. This requires both responsible humans-in-the-loop across the entire design and implementation chain as well as activity monitoring protocols that enable end-to-end oversight and review.","title":"Introduction to accountability"},{"location":"aeg/chapter4/accountability/#types-of-accountability","text":"Accountability deserves consideration across the entire design and implementation workflow. As a best practice, the team should actively consider the different demands that accountability-by-design requires before and after the roll out of the AI project. We will refer to the process of ensuring accountability during the design and development stages of the AI project as \u2018anticipatory accountability\u2019. This is because the team is anticipating the project\u2019s accountability needs prior to it being completed. Following a similar logic, we will refer to the process of addressing accountability after the start of the deployment of your AI project as \u2018remedial accountability\u2019. This is because after the initial implementation of a system, the team is remedying any of the issues that may be raised by its effects and potential externalities. These two subtypes of accountability are sometimes referred to as ex-ante (or before-the-event) accountability and ex-post (after-the-event) accountability respectively.","title":"Types of accountability"},{"location":"aeg/chapter4/accountability/#anticipatory-accountability","text":"Treating accountability as an anticipatory principle entails that the project team takes as of primary importance the decisions made and actions taken by them prior to the outcome of an algorithmically supported decision process. This kind of ex ante accountability should be prioritised over remedial accountability, which focuses instead on the corrective or justificatory measures that can be taken after that automation supported process had been completed. Ensuring that the AI project delivery processes are accountable prior to the actual application of the system in the world will bolster the soundness of design and implementation processes and thereby more effectively pre-empt possible harms to individual wellbeing and public welfare or other adverse impacts. Likewise, by establishing strong regimes of anticipatory accountability and by making the design and delivery process as open and publicly accessible as possible, the project team will put affected stakeholders in a position to make better informed and more knowledgeable decisions about their involvement with these systems in advance of potentially harmful impacts. Doing so will also strengthen the public narrative and help to safeguard the project from reputational harm. Example During the preprocessing phase of an AI production lifecycle, technical members of a project team are deciding which features to include, and which to leave out. To safeguard sufficient anticipatory accountability, they make sure to log which team members are involved in making these decisions and record the rationale behind the choices made.","title":"Anticipatory accountability"},{"location":"aeg/chapter4/accountability/#remedial-accountability","text":"While remedial accountability should be seen, along these lines, as a necessary fallback rather than as a first resort for imputing responsibility in the design, development and deployment of AI systems, strong regimes of remedial accountability are no less important in providing necessary justifications for the bearing these systems have on the lives of affected stakeholders. Putting in place comprehensive auditability regimes as part of your accountability framework and establishing transparent design and use practices, which are methodically logged throughout the AI project delivery lifecycle, are essential components for this sort of remedial accountability. One aspect of remedial accountability that you must pay close attention to is the need to provide explanations to affected stakeholders for algorithmically supported decisions. Offering explanations for the results of algorithmically supported decision-making involves furnishing decision subjects and other interested parties with an understandable account of the rationale behind the specific outcome of interest. It also involves furnishing the decision subject and other interested parties with an explanation of the ethical permissibility, the fairness, and the safety of the use of the AI system. Example After receiving an unfavourable decision in their recruitment process, a job applicant seeks assurance and justification that the resume filtering AI system used in their process was not biased or discriminatory. To safeguard sufficient remedial accountability, the implementers of the system draw on the logged records of bias-mitigation measures and fairness-aware design practices to demonstrate the fair and non-discriminatory practices behind the production of the system. They also provide an explanation of the rationale behind the applicant\u2019s negative result, showing that the determinative factors behind the system\u2019s output were fair and reasonable.","title":"Remedial Accountability"},{"location":"aeg/chapter4/aifairness/","text":"AI Fairness as a contextual and multi-valent concept \u00b6 AI Fairness must be treated as a contextual and multi-valent concept that manifests in a variety of ways and in a variety of social, technical, and sociotechnical environments. It must, accordingly, be understood in and differentiated by the specific settings in which it arises. This means that our operating notion of AI fairness should distinguish between the kinds of fairness concerns that surface in the context of the social world that precedes and informs approaches to fairness in AI/ML innovation activities\u2014i.e., in general normative and ethical notions of fairness, equity, and social justice, in human rights laws related to equality, non-discrimination, and inclusion, and in anti-discrimination laws and equality statutes; the data context \u2014i.e. in criteria of fairness and equity that are applied to responsibly collected and maintained datasets; the design, development, and deployment context \u2014i.e. in criteria of fairness and equity that are applied (a) in setting the research agendas and policy objectives that guide decisions made about where, when, and how to use AI systems and (b) in actual model design and development and system implementation environments as well as in the technical instrumentalization of formal fairness metrics that allocate error rates and the distribution of outcomes through the retooling of model architectures and parameters; and the ecosystem context \u2014i.e. in criteria of fairness and equity that are applied to the wider economic, legal, cultural, and political structures or institutions in which the AI project lifecycle is embedded\u2014and to the policies, norms, and procedures through which these structures and institution influence actions and decisions throughout the AI innovation ecosystem. Each of these contexts will generate different sets of fairness concerns. In applying the principle of discriminatory non-harm to the AI project lifecycle we will accordingly break down the principle of fairness into six subcategories that correspond to their relevant practical contexts: Data Fairness: The AI system is trained and tested on properly representative, fit-for-purpose, relevant, accurately measured, and generalisable datasets. Application Fairness: The policy objectives and agenda-setting priorities that steer the design, development, and deployment of an AI system and the decisions made about where, when, and how to use it do not create or exacerbate inequity, structural discrimination, or systemic injustices and are acceptable to and line up with the aims, expectations, and sense of justice possessed by impacted people. Model Design and Development Fairness: The AI system has a model architecture that does not include target variables, features, processes, or analytical structures (correlations, interactions, and inferences) which are discriminatory, unreasonable, morally objectionable, or unjustifiable or that encode social and historical patterns of discrimination. Metric-Based Fairness: Lawful, clearly defined, and justifiable formal metrics of fairness have been operationalised in the AI system have been made transparently accessible to relevant stakeholders and impacted people. System Implementation Fairness: The AI system is deployed by users sufficiently trained to implement it with an appropriate understanding of its limitations and strengths and in a bias-aware manner that gives due regard to the unique circumstances of affected individuals. Ecosystem Fairness: The wider economic, legal, cultural, and political structures or institutions in which the AI project lifecycle is embedded\u2014and the policies, norms, and procedures through which these structures and institution influence actions and decisions throughout the AI innovation ecosystem \u2014do not steer AI research and innovation agendas in ways that entrench or amplify asymmetrical and discriminatory power dynamics or that generate inequitable outcomes for protected, marginalised, vulnerable, or disadvantaged social groups. Data fairness \u00b6 Responsible data acquisition, handling, and management is a necessary component of algorithmic fairness. If the results of an AI project are generated by a model that has been trained and tested on biased, compromised, or skewed datasets, affected stakeholders will not be adequately protected from discriminatory harm. Data fairness therefore involves the following key elements: Representativeness Depending on the context, either underrepresentation or overrepresentation of demographic, marginalised or legally protected groups in the data sample may lead to the systematic disadvantaging of vulnerable or marginalised stakeholders in the outcomes of the trained model. To avoid such kinds of sampling and selection biases, domain expertise is crucial, for it enables the assessment of the fit between the data collected or procured and the underlying population to be modelled. Fit-for-Purpose and Sufficiency An important question to consider in the data collection and procurement process is: Will the amount of data collected be sufficient for the intended purpose of the project? The quantity of data collected or procured has a significant impact on the accuracy and reasonableness of the outputs of a trained model. A data sample not large enough to represent with sufficient richness the significant or qualifying attributes of the members of a population to be classified may lead to unfair outcomes. Insufficient datasets may not equitably reflect the qualities that should rationally be weighed in producing a justified outcome that is consistent with the desired purpose of the AI system. Members of the project team with technical and policy competences should collaborate to determine if the data quantity is, in this respect, sufficient and fit-for-purpose. Source Integrity and Measurement Accuracy Effective bias mitigation begins at the very commencement of data extraction and collection processes. Both the sources and instruments of measurement may introduce discriminatory factors into a dataset. When incorporated as inputs in the training data, biased prior human decisions and judgments\u2014such as prejudiced scoring, ranking, interview-data or evaluation\u2014will become the \u2018ground truth\u2019 of the model and replicate the bias in the outputs of the system. In order to secure discriminatory non-harm, you must do your best to make sure your data sample has optimal source integrity. This involves securing or confirming that the data gathering processes involved suitable, reliable, and impartial sources of measurement and sound methods of collection. Timeliness and Recency If your datasets include outdated data then changes in the underlying data distribution may adversely affect the generalisability of your trained model. Provided these distributional drifts reflect changing social relationship or group dynamics, this loss of accuracy with regard to the actual characteristics of the underlying population may introduce bias into your AI system. In preventing discriminatory outcomes, you should scrutinise the timeliness and recency of all elements of the data that constitute your datasets. Relevance, Appropriateness and Domain Knowledge The understanding and utilisation of the most appropriate sources and types of data are crucial for building a robust and bias-mitigating AI system. Solid domain knowledge of the underlying population distribution and of the predictive or classificatory goal of the project is instrumental for choosing optimally relevant measurement inputs that contribute to the reasonable determination of the defined solution. You should make sure that domain experts collaborate closely with your technical team to assist in the determination of the optimally appropriate categories and sources of measurement. To ensure the uptake of best practices for responsible data acquisition, handling, and management across your AI project delivery workflow, you should initiate the creation of a Dataset Factsheet at the alpha stage of your project. This factsheet should be maintained diligently throughout the design and implementation lifecycle in order to secure optimal data quality, deliberate bias-mitigation aware practices, and optimal auditability. It should include a comprehensive record of data provenance, procurement, pre-processing, lineage, storage, and security as well as qualitative input from team members about determinations made with regard to data representativeness, data sufficiency, source integrity, data timeliness, data relevance, training/ testing/validating splits, and unforeseen data issues encountered across the workflow. Application Fairness \u00b6 Fairness considerations should enter into your AI project at the earliest possible stage of horizon scanning, problem selection, and project planning. This is because, the overall fairness and equity of an AI application is significantly determined by the objectives, goals, and policy choices that lie behind initial decisions to dedicate time and resources to its design, development, and deployment. For example, the choice made to build a biometric identification system, which uses live facial recognition technology to identify criminal suspects at public events, may be motivated by the objective to increase public safety and security. However, many members of the public may find this use of AI technology unreasonable, disproportionate, and potentially discriminatory. In particular, members of communities historically targeted by disproportionate levels of surveillance in law enforcement contexts may be especially concerned about the potential for abuse and harm. Appropriate fairness and equity considerations should, in this case, occur at the horizon scanning and project planning stage (e.g., as part of a stakeholder impact assessment process that includes engagement with potentially affected individuals and communities). Aligning the policy goals of a project team with the reasonable expectations and potential equity concerns of those affected is a key component of the fairness of the application. Application fairness, therefore, entails that the policy objectives of an AI project are non-discriminatory and are acceptable to and line up with the aims, expectations, and sense of justice possessed by those affected.[@dobbe2018]-[@eckhouse2018]-[@green2018a]-[@green2018b]-[@mitchell2021]-[@passi2019] As such, whether the decision to engage in the production and use of an AI technology can be described as \u201cfairness-aware\u201d depends upon ethical and policy considerations that are external and prior to considerations of the technical feasibility of building an accurate and optimally performing system or the practical feasibility of accessing, collecting, or acquiring enough and the right kind of data. Beyond this priority of assuring the equity and ethical permissibility of policy goals, application fairness requires additional considerations in framing decisions made at the horizon scanning and project scoping or planning stage: 1 Equity considerations surrounding real-world context of the policy issue to be solved: When your project team is assessing the fairness of using an AI solution to address a particular policy issue, it is important to consider how equity considerations extend beyond the statistical and sociotechnical contexts of designing, developing, and deploying the system. Applied concepts of AI fairness and equity should not be treated, in a technology-centred way, as originating exclusively from the design and use of any particular AI system. Nor should they exclusively be treated as abstractions that can be engineered into an AI application through technical or mathematical retooling (e.g., by operationalising formal fairness criteria).[@fazelpour2020]-[@green2020]-[@leslie2020]-[@selbst2019] When designers of algorithmic systems limit their understanding of the scope of \u2018fairness\u2019 or \u2018equity\u2019 to these two dimensions, it can artificially constrain their perspectives in such a way that they erroneously treat only the patterns of bias and discrimination that arise in AI innovation practices or that can be measured, formalised, or statistically digested as actionable indicators of inequity. Rather, equity considerations should be grounded in a human-centred approach, which includes reflection on and critical examination of the wider social and economic patterns of disadvantage, injustice, and discrimination that arise in the real-world contexts surrounding the policy issue in question. Such considerations should include an exploration of how such patterns of inequity may lead, on the one hand, to the disparate distribution of the risks and adverse impacts of the AI system or, on the other, to a lack of equitable access to its potential benefits. For instance, while the development of an AI chatbot to replace a human serviced medical helpline may provide effective healthcare guidance for some, it could have disparate adverse impacts on others, such as vulnerable elderly populations or socioeconomically deprived groups who may face barriers to accessing and using the app. Here, reflection on the real-world contexts surrounding the provision of this type of healthcare support yields a more informed and compassionate awareness of social and economic conditions that could impair the fairness of the application. 2 Equity considerations surrounding the group targeted by the AI innovation intervention: Each AI application that makes predictions about or classifies people, targets a specific subset of the wider population to which they belong. For instance, a r\u00e9sum\u00e9 filtering system that is used to select desirable candidates in a recruitment process will draw from a pool of job applicants that constitute a subgroup within the broader population. Potential equity issues may arise here because the selection of subpopulations sampled by AI applications is non-random. Instead, the sample selection may reflect particular social patterns, structures, and path dependencies that are unfair or discriminatory.[@mitchell2021] In the case of the r\u00e9sum\u00e9 filtering system, the sample may reflect long-term hiring patterns where a disproportionate number of male job candidates from elite universities (or those with similarly privileged educational backgrounds) have been actively recruited. Such practices have historically excluded people from other gender identities and socioeconomic and educational backgrounds. As a result, the pattern of inequity surfaces, in this instance, not within the sampled subset of the population but rather in the way that discriminatory social structures have led to the selection of a certain group of individuals into that subset.[@mehrabi2021]-[@olteanu2019] 3 Equity considerations surrounding the way that the model\u2019s output shapes the range of possible decision-outcomes: AI applications that assist human decision-making shape and delimit the range of possible outcomes for the problem areas they address.[@mitchell2021] For example, a predictive risk model used in children\u2019s social care may generate an output that directly influences the space of choices available to a social worker. Because the model\u2019s target is the identification of at-risk children, it may lead to social care decisions that focus narrowly on whether a child needs to be taken into care. This centring of negative outcomes could restrict the range of viable choices open to the social worker insofar as it de-emphasises the potential availability of other strengths-based approaches (e.g., stewarding positive family functioning through social supports and identifying and promoting protective factors). These alternative decision-making paths could be closed off in the social care environment given how the predictive risk model\u2019s outputs restrictively shape the range of actions that can be taken to address the problem it is being used to inform. Model design and development fairness \u00b6 Because human beings have a hand in all stages of the construction of AI systems, fairness-aware design must take precautions across the AI project workflow to prevent bias from having a discriminatory influence: Design phase \u00b6 Problem formulation At the initial stage of problem formulation and outcome definition, technical and non-technical members of your team should work together to translate project goals into measurable targets.[@fazelpour2021]-[@leslie2019]-[@obermeyer2019]-[@obermeyer2021]-[@passi2019] This will involve the use of both domain knowledge and technical understanding to define what is being optimised in a formalisable way and to translate the project\u2019s objective into a target variable or measurable proxy, which operates as a statistically actionable rendering of the defined outcome. At each of these points, choices must be made about the design of the algorithmic system that may introduce structural biases which ultimately lead to discriminatory harm. Special care must be taken here to identify affected stakeholders and to consider how vulnerable groups might be negatively impacted by the specification of outcome variables and proxies. Attention must also be paid to the question of whether these specifications are reasonable and justifiable given the general purpose of the project and the potential impacts that the outcomes of the system\u2019s use will have on the individuals and communities involved. These challenges of fairness aware design at the problem formulation stage show the need for making diversity and inclusive participation a priority from the start of the AI project lifecycle. This involves both the collaboration of the entire team and the attainment of stakeholder input about the acceptability of the project plan. This also entails collaborative deliberation across the project team and beyond about the ethical impacts of the design choices made. Development phase \u00b6 Data pre-processing Human judgement enters into the process of algorithmic system construction at the stage of labelling, annotating, and organising the training data to be utilised in building the model. Choices made about how to classify and structure raw inputs must be taken in a fairness-aware manner with due consideration given to the sensitive social contexts that may introduce bias into such acts of classification. Similar fairness aware processes should be put in place to review automated or outsourced classifications. Likewise, efforts should be made to attach solid contextual information and ample metadata to the datasets, so that downstream analyses of data processing have access to properties of concern in bias mitigation. The constructive task of selecting the attributes or features that will serve as input variables for a model requires human decisions to be made about the sorts of information that may or may not be relevant or rationally required to yield an accurate and unbiased classification or prediction. Moreover, the feature engineering tasks of aggregating, extracting, or decomposing attributes from datasets may introduce human appraisals that have biasing effects. At this stage, human decisions about how to group or disaggregate input features (e.g., how to carve up categories of gender or ethnic groups) or about which input features to exclude altogether (e.g., leaving out deprivation indicators in a predictive model for clinical diagnostics) can have significant downstream influences on the fairness and equity of an AI system. This applies even when algorithmic techniques are employed to extract and engineer features, or support the selection of features (e.g., to optimise predictive power). For this reason, discrimination awareness should play a large role at this stage of the AI model-building workflow as should domain knowledge and policy expertise. Your team should proceed in the model development stage aware that choices made about grouping or separating and including or excluding features as well as more general judgements about the comprehensiveness or coarseness of the total set of features may have significant consequences for historically marginalised, vulnerable, or protected groups. Model selection The model selection stage determines the model type and structure that will be produced in the next stages. In some projects, this will involve the selection of multiple prospective models for the purpose of comparison based on some performance metric, such as accuracy or sensitivity. The set of relevant models is likely to have been highly constrained by many of the issues dealt with in previous stages (e.g., available resources and skills, problem formulation), for instance, where the problem demands a supervised learning algorithm instead of an unsupervised learning algorithm. Fairness and equity issues can surface in model selection processes in at least two ways: First, they can arise when the choice between algorithms has implications for explainability. For instance, it may be the case that there are better performing models in the pool of available options but which are less interpretable than others. This difference becomes significant when the processing of social or demographic data increases the risk that biases or discriminatory proxies lurk in the algorithmic architecture. Model interpretability can increase the likelihood of detecting and redressing such discriminatory elements. Second, fairness and equity issues can arise when the choice between algorithms has implications for the differential performance of the final model for subgroups of the population. For instance, where several different learning algorithms are simultaneously trained and tested, one of the resulting models could have the highest overall level of accuracy while, at the same time, being less accurate than others in the way it performs for one or more marginalised sub-groups. In cases like this, technical members of your project team should proceed with attentiveness to mitigating any possible discriminatory effects of choosing one model over another and should consult members of the wider team\u2014and impacted stakeholders, where possible\u2014about the acceptability of any trade-offs between overall model accuracy and differential performance. Model training, testing, and validation The process of tuning hyperparameters, setting metrics, and resampling data at the training, validation, and testing stages also involves human choices that may have fairness and equity consequences in the trained model. For instance, the way your project team determines the training-testing split of the dataset can have a considerable impact on the need for external validation to ensure that the model\u2019s performance \u201cin the wild\u201d meets reasonable expectations. Therefore, your technical team should proceed with an attentiveness to bias risks, and continual iterations of peer review and project team consultation should be encouraged to ensure that choices made in adjusting the dials, parameters, and metrics of the model are in line with bias mitigation and discriminatory non-harm goals. Evaluating and validating model structures Design fairness also demands close assessment of the existence in the trained model of lurking or hidden proxies for discriminatory features that may act as significant factors in its output. Including such hidden proxies in the structure of the model may lead to implicit \u2018redlining\u2019 (the unfair treatment of a sensitive group on the basis of an unprotected attribute or interaction of attributes that \u2018stands in\u2019 for a protected or sensitive one). Designers must additionally scrutinise the moral justifiability of the significant correlations and inferences that are determined by the model\u2019s learning mechanisms themselves, for these correlations and inferences could encode social and historical patterns of discrimination where these are baked into the dataset. In cases of the processing of social or demographic data related to human features, where the complexity and high dimensionality of machine learning models preclude the confirmation of the discriminatory non-harm of these inferences (for reason of their opaqueness by human assessors), these models should be avoided.[@icoturing2020]-[@mitchell2021]-[@rudin2019] In cases where this is not possible, a different, more transparent and explainable model or portfolio of models should be chosen. Model structures must also be confirmed to be procedurally fair, in a strict technical sense. This means that any rule or procedure employed in the processing of data by an algorithmic system should be consistently and uniformly applied to every decision subject whose information is being processed by that system. AI project teams should be able to certify that any relevant rule or procedure is applied universally and uniformly to all relevant individuals. Implementers of the system, in this respect, should be able to show that any model output is replicable when the same rules and procedures are applied to the same inputs. Such a uniformity of the application of rules and procedures secures the equal procedural treatment of decision subjects and precludes any rule-changes in the algorithmic processing targeted at a specific person that may disadvantage that individual vis-\u00e0-vis any other. It is important to note that the consistent and uniform application of rules over time apply to deterministic algorithms, whose parameters are static and fixed after model development. Close attention should be paid, in this sense, to the procedural fairness issues that may arise with the use of dynamic learning algorithms. This is because the parameters and inferences of such systems evolve over time and can yield different outputs for the same inputs at different points in system\u2019s lifespan. Metric-based fairness \u00b6 As part of the safeguarding of discriminatory non-harm and diligent fairness and equity considerations, well-informed consideration must be put into how you are going to define and measure the formal metrics of fairness that can be operationalised into the AI system you are developing. Metric-based fairness involves the mathematical mechanisms that can be incorporated into an AI model to allocate the distribution of outcomes and error rates for relevant subpopulations (e.g., groups with protected or sensitive characteristics). In formulating your approach to metric-based fairness, your project team will be confronting challenging issues like the justifiability of differential treatment based on sensitive or protected attributes, where differential treatment can indicate differences in the distribution of model outputs or the distribution of error rates and performance indicators like precision or sensitivity. There is a great diversity of beliefs in this area as to what makes the consequences of an algorithmically supported decision allocatively equitable, fair, and just. Different approaches to metric-based fairness\u2014detailed below\u2014stress different principles: some focus on demographic parity, some on individual fairness, others on error rates equitably distributed across subpopulations. Regardless of this diversity of views, your project team must ensure that the choices made regarding formal fairness metrics are lawful and conform to governing equality, non-discrimination, and human rights laws. Where appropriate, relevant experts should be consulted to confirm the legality of such choices. Your determination of metric-based fairness should heavily depend both on the specific use case being considered and the technical feasibility of incorporating your chosen criteria into the construction of the AI system. (Note that different fairness-aware methods involve different types of technical interventions at the pre-processing, modelling, or post-processing stages of production). Again, this means that determining your fairness definition should be a cooperative and multidisciplinary effort across the project team. You will find below a summary table of several of the main definitions of metric-based fairness that have been integrated by researchers into formal models as well as a list of current articles and technical resources, which should be consulted to orient your team to the relevant knowledge base. (Note that this is a rapidly developing field, so your technical team should keep updated about further advances.) The first four fairness types fall under the category of group fairness and allow for comparative criteria of non-discrimination to be considered in model construction and evaluation. The final two fairness types focus instead on cases of individual fairness, where context-specific issues of effective bias are considered and assessed at the level of the individual agent. Take note, though, that these technical approaches have limited scope in terms of the bigger picture issues of application and design fairness that we have already stressed. Moreover, metric-based approaches face other practical and technical barriers. For instance, to carry out group comparisons, formal approaches to fairness require access to data about sensitive/protected attributes as well as accurate demographic information about the underlying population distribution (both of which may often be unavailable or unreliable, and furthermore, the work of identifying sensitive/protected attributes may pose additional risks of bias). Metric-based approaches also face challenges in the way they handle combinations of protected or sensitive characteristics that may amplify discriminatory treatment. These have been referred to as intersectional attributes (e.g. the combination gender and race characteristics), and they must also be integrated into fairness and equity considerations. Lastly, there are unavoidable trade-offs, inconsistencies, or even incompatibilities, between mathematical definitions of metric-based fairness that must be weighed in determining which of them are best fit for a particular use case. For instance, the desideratum for equalised odds (error rate balance) across subgroups can clash with the desideratum for equalised model calibration (correct predictions of gradated outcomes) or parity of positive predictive values across subgroups.[@chouldechova2017]-[@flores2016]-[@friedler2021]-[@kleinenberg2017]-[@mitchell2021] Selecting fairness metrics \u00b6 Demographic/statistical parity (group fairness) An outcome is fair if each group in the selected set receives benefit in equal or similar proportions, i.e. if there is no correlation between a sensitive or protected attribute and the allocative result. [@calders2009]-[@denis2021]-[@dwork2011]-[@feldman2015]-[@zemel2013] This approach is intended to prevent disparate impact, which occurs when the outcome of an algorithmic process disproportionately harms members of disadvantaged or protected groups. True positive rate parity (group fairness) An outcome is fair if the \u2018true positive\u2019 rates of an algorithmic prediction or classification are equal across groups.[@zafar2017] This approach is intended to align the goals of bias mitigation and accuracy by ensuring that the accuracy of the model is equivalent between relevant population subgroups. This method is also referred to as \u2018equal opportunity\u2019 fairness because it aims to secure equalised odds of an advantageous outcome for qualified individuals in a given population regardless of the protected or disadvantaged groups of which they are members. Equalised odds (group fairness) An outcome is fair if false positive and true positive rates are equal across groups. In other words, both the probability of incorrect positive predictions and the probability of correct positive predictions should be the same across protected and privileged groups.[@hardt2016]-[@verma2018] This approach is motivated by the position that sensitive groups and advantaged groups should have similar error rates in outcomes of algorithmic decisions. Positive predictive value parity (group fairness) An outcome is fair if the rates of positive predictive value (the fraction of correctly predicted positive cases out of all predicted positive cases) are equal across sensitive and advantaged groups.[@chouldechova2017] Outcome fairness is defined here in terms of a parity of precision, where the probability of members from different groups actually having the quality they are predicted to have is the same across groups. Individual fairness (individual fairness) An outcome is fair if it treats individuals with similar relevant qualifications similarly. This approach relies on the establishment of a similarity metric that shows the degree to which pairs of individuals are alike with regard to a specific task.[@dwork2012]-[@kusner2017] Counterfactual fairness (individual fairness) An outcome is fair if an automated decision made about an individual belonging to a sensitive group would have been the same were that individual a member of a different group in a closest possible alternative (or counterfactual) world.[@kusner2017] Like the individual fairness approach, this method of defining fairness focuses on the specific circumstances of an affected decision subject, but, by using the tools of contrastive explanation, it moves beyond individual fairness insofar as it brings out the causal influences behind the algorithmic output. It also presents the possibility of offering the subject of an automated decision knowledge of what factors if changed, could have influenced a different outcome. This could provide them with actionable recourse to change an unfavourable decision. System implementation fairness \u00b6 When your project team is approaching the beta stage, you should begin to build out your plan for implementation training and support. This plan should include adequate preparation for the responsible and unbiased deployment of the AI system by its on-the-ground users. Automated decision-support systems present novel risks of bias and misapplication at the point of delivery, so special attention should be paid to preventing harmful or discriminatory outcomes at this critical juncture of the AI project lifecycle. In order to design an optimal regime of implementer training and support, you should pay special attention to the unique pitfalls of bias-in-use to which the deployment of AI technologies give rise. These can be loosely classified as decision-automation bias (more commonly just \u2018automation bias\u2019) and automation-distrust bias: Decision-Automation Bias I (over-reliance) Users of automated decision-support systems may tend to become hampered in their critical judgement, rational agency, and situational awareness as a result of their faith in the perceived objectivity, neutrality, certainty, or superiority of the AI system (Gaube et al., 2021). This may lead to over-reliance or errors of omission, where implementers lose the capacity to identify and respond to the faults, errors, or deficiencies, which might arise over the course of the use of an automated system, because they become complacent and overly deferent to its directions and cues.[@bussone2015] Decision-Automation Bias II (over-reliance) Decision-automation bias may also lead to over-compliance or errors of commission where implementers defer to the perceived infallibility of the system and thereby become unable to detect problems emerging from its use for reason of a failure to hold the results against available information. Both over-reliance and over-compliance may lead to what is known as out-of-loop syndrome where the degradation of the role of human reason and the de-skilling of critical thinking hampers the user\u2019s ability to complete the tasks that have been automated. This condition may bring about a loss of the ability to respond to system failure and may lead both to safety hazards and to dangers of discriminatory harm. To combat risks of decision-automation bias, you should operationalise strong regimes of accountability at the site of user deployment to steer human decision-agents to act on the basis of good reasons, solid inferences, and critical judgment. Automation-Distrust Bias At the other extreme, users of an automated decision-support system may tend to disregard its salient contributions to evidence-based reasoning either as a result of their distrust or scepticism about AI technologies in general or as a result of their over-prioritisation of the importance of prudence, common sense, and human expertise.[@longoni2019] An aversion to the non-human and amoral character of automated systems may also influence decision subjects\u2019 hesitation to consult these technologies in high impact contexts such as healthcare, transportation, and law.[@dietvorst2015] \u2018Research shows that people often prefer humans\u2019 forecasts to algorithms\u2019 forecasts,[@diab2011]-[@eastwood2012] more strongly weigh human input than algorithmic input,[@onkal2009]-[@promberger2006] and more harshly judge professionals who seek out advice from an algorithm rather than from a human.[@shaffer2013]-[@dietvorst2015] Taking account of the context of impacted individuals in system implementation \u00b6 In cases where you are utilising a decision-support AI system that draws on statistical inferences to determine outcomes which affect individual persons (e.g., a predictive risk model that helps an adult social care work determine an optimal path to caring for an elderly patient), fair implementation processes should include considerations of the specific context of each impacted individual.[@binns2017]-[@binns2018]-[@binns2019]-[@eiselson2013] Any application of this kind of system\u2019s recommendation will be based on statistical generalisations, which pick up relationships between the decision recipient\u2019s input data and patterns or trends that the AI model has extracted from the underlying distribution of that model\u2019s original dataset. Such generalisations will be predicated on inferences about a decision subject\u2019s future behaviours (or outcomes) that are based on populational-level correlations with the historical characteristics and attributes of the members of groups to which that person belongs rather than on the specific qualities of the person themself. Fair and equitable treatment of decision subjects entails that their unique life circumstances and individual contexts be taken into account in decision-making processes that are supported by AI-enabled statistical generalisations. For this reason, you should train your implementers to think contextually and holistically about how these statistical generalisations apply to the specific situation of the decision recipient. This training should involve preparing implementers to work with an active awareness of the socio-technical aspect of implementing AI decision-assistance technologies from an integrative and human- centred point of view. You should train implementers to apply the statistical results to each particular case with appropriate context-sensitivity and \u2018big picture\u2019 sensibility. This means that the dignity they show to decision subjects can be supported by interpretive understanding, reasonableness, and empathy. Ecosystem fairness \u00b6 The AI project lifecycle does not exist in isolation from, or independent of, the wider social system of economic, legal, cultural, and political structures or institutions in which the production and use of AI systems take place. Rather, because it is embedded in these structures and institutions, the policies, norms, and procedures through which such structures and institutions influence human action also influence the AI project lifecycle itself. Inequities and biases at this ecosystem level can steer or shape AI research and innovation agendas in ways that can generate inequitable outcomes for protected, marginalised, vulnerable, or disadvantaged social groups. Such ecosystem-level inequities and biases may originate in and further reinforce asymmetrical power structures, unfair market dynamics, and skewed research funding schemes that favour or bring disproportionate benefit to those in the majority, or those who wield disproportionate power in society, at the cost of those who are disparately impacted by the discriminatory outcomes of the design, development, and use of AI technologies. Ecosystem-level inequities can occur across a wide range of AI research and innovation contexts. For instance, when AI-enabled health interventions such as mobile-phone-based symptom checker apps or remote AI-assisted medical triaging or monitoring are designed without regard for the barriers to access faced by protected, vulnerable, or disadvantaged groups, they will disproportionately benefit users from other, more advantaged groups. Likewise, where funding of the development of AI technologies, which significantly affect the public interest, is concentrated in the hands of firms or vendors who singly pursue commercial or financial gain, this may result in exclusionary research and innovation environments, AI systems that are built without due regard for broad fairness and equity impacts, and limitations on the development and deployment of wide-scale, publicly beneficial technology infrastructure. Moreover, widespread structural and institutional barriers to diversity and inclusion can create homogeneity on AI project teams (and among organisational policy owners) that has consequential fairness impacts on application decisions, resource allocation choices, and system deployment strategies. The concept of ecosystem fairness highlights the importance of mitigating the range of inequity-generating path dependencies that originate at the ecosystem level and that are often neglected by or omitted from analyses of AI project lifecycles. Ecosystem fairness therefore focuses both on rectifying the social structures and institutions that engender indirect discrimination and on addressing the structural and institutional changes needed for the corrective modification of social patterns that engender discriminatory impacts and socio-economic disadvantage. In this way, ecosystem fairness involves the transformation of unjust economic, legal, cultural, and political structures or institutions with the aim of the universal realisation of equitable social arrangements.","title":"AI Fairness"},{"location":"aeg/chapter4/aifairness/#ai-fairness-as-a-contextual-and-multi-valent-concept","text":"AI Fairness must be treated as a contextual and multi-valent concept that manifests in a variety of ways and in a variety of social, technical, and sociotechnical environments. It must, accordingly, be understood in and differentiated by the specific settings in which it arises. This means that our operating notion of AI fairness should distinguish between the kinds of fairness concerns that surface in the context of the social world that precedes and informs approaches to fairness in AI/ML innovation activities\u2014i.e., in general normative and ethical notions of fairness, equity, and social justice, in human rights laws related to equality, non-discrimination, and inclusion, and in anti-discrimination laws and equality statutes; the data context \u2014i.e. in criteria of fairness and equity that are applied to responsibly collected and maintained datasets; the design, development, and deployment context \u2014i.e. in criteria of fairness and equity that are applied (a) in setting the research agendas and policy objectives that guide decisions made about where, when, and how to use AI systems and (b) in actual model design and development and system implementation environments as well as in the technical instrumentalization of formal fairness metrics that allocate error rates and the distribution of outcomes through the retooling of model architectures and parameters; and the ecosystem context \u2014i.e. in criteria of fairness and equity that are applied to the wider economic, legal, cultural, and political structures or institutions in which the AI project lifecycle is embedded\u2014and to the policies, norms, and procedures through which these structures and institution influence actions and decisions throughout the AI innovation ecosystem. Each of these contexts will generate different sets of fairness concerns. In applying the principle of discriminatory non-harm to the AI project lifecycle we will accordingly break down the principle of fairness into six subcategories that correspond to their relevant practical contexts: Data Fairness: The AI system is trained and tested on properly representative, fit-for-purpose, relevant, accurately measured, and generalisable datasets. Application Fairness: The policy objectives and agenda-setting priorities that steer the design, development, and deployment of an AI system and the decisions made about where, when, and how to use it do not create or exacerbate inequity, structural discrimination, or systemic injustices and are acceptable to and line up with the aims, expectations, and sense of justice possessed by impacted people. Model Design and Development Fairness: The AI system has a model architecture that does not include target variables, features, processes, or analytical structures (correlations, interactions, and inferences) which are discriminatory, unreasonable, morally objectionable, or unjustifiable or that encode social and historical patterns of discrimination. Metric-Based Fairness: Lawful, clearly defined, and justifiable formal metrics of fairness have been operationalised in the AI system have been made transparently accessible to relevant stakeholders and impacted people. System Implementation Fairness: The AI system is deployed by users sufficiently trained to implement it with an appropriate understanding of its limitations and strengths and in a bias-aware manner that gives due regard to the unique circumstances of affected individuals. Ecosystem Fairness: The wider economic, legal, cultural, and political structures or institutions in which the AI project lifecycle is embedded\u2014and the policies, norms, and procedures through which these structures and institution influence actions and decisions throughout the AI innovation ecosystem \u2014do not steer AI research and innovation agendas in ways that entrench or amplify asymmetrical and discriminatory power dynamics or that generate inequitable outcomes for protected, marginalised, vulnerable, or disadvantaged social groups.","title":"AI Fairness as a contextual and multi-valent concept"},{"location":"aeg/chapter4/aifairness/#data-fairness","text":"Responsible data acquisition, handling, and management is a necessary component of algorithmic fairness. If the results of an AI project are generated by a model that has been trained and tested on biased, compromised, or skewed datasets, affected stakeholders will not be adequately protected from discriminatory harm. Data fairness therefore involves the following key elements: Representativeness Depending on the context, either underrepresentation or overrepresentation of demographic, marginalised or legally protected groups in the data sample may lead to the systematic disadvantaging of vulnerable or marginalised stakeholders in the outcomes of the trained model. To avoid such kinds of sampling and selection biases, domain expertise is crucial, for it enables the assessment of the fit between the data collected or procured and the underlying population to be modelled. Fit-for-Purpose and Sufficiency An important question to consider in the data collection and procurement process is: Will the amount of data collected be sufficient for the intended purpose of the project? The quantity of data collected or procured has a significant impact on the accuracy and reasonableness of the outputs of a trained model. A data sample not large enough to represent with sufficient richness the significant or qualifying attributes of the members of a population to be classified may lead to unfair outcomes. Insufficient datasets may not equitably reflect the qualities that should rationally be weighed in producing a justified outcome that is consistent with the desired purpose of the AI system. Members of the project team with technical and policy competences should collaborate to determine if the data quantity is, in this respect, sufficient and fit-for-purpose. Source Integrity and Measurement Accuracy Effective bias mitigation begins at the very commencement of data extraction and collection processes. Both the sources and instruments of measurement may introduce discriminatory factors into a dataset. When incorporated as inputs in the training data, biased prior human decisions and judgments\u2014such as prejudiced scoring, ranking, interview-data or evaluation\u2014will become the \u2018ground truth\u2019 of the model and replicate the bias in the outputs of the system. In order to secure discriminatory non-harm, you must do your best to make sure your data sample has optimal source integrity. This involves securing or confirming that the data gathering processes involved suitable, reliable, and impartial sources of measurement and sound methods of collection. Timeliness and Recency If your datasets include outdated data then changes in the underlying data distribution may adversely affect the generalisability of your trained model. Provided these distributional drifts reflect changing social relationship or group dynamics, this loss of accuracy with regard to the actual characteristics of the underlying population may introduce bias into your AI system. In preventing discriminatory outcomes, you should scrutinise the timeliness and recency of all elements of the data that constitute your datasets. Relevance, Appropriateness and Domain Knowledge The understanding and utilisation of the most appropriate sources and types of data are crucial for building a robust and bias-mitigating AI system. Solid domain knowledge of the underlying population distribution and of the predictive or classificatory goal of the project is instrumental for choosing optimally relevant measurement inputs that contribute to the reasonable determination of the defined solution. You should make sure that domain experts collaborate closely with your technical team to assist in the determination of the optimally appropriate categories and sources of measurement. To ensure the uptake of best practices for responsible data acquisition, handling, and management across your AI project delivery workflow, you should initiate the creation of a Dataset Factsheet at the alpha stage of your project. This factsheet should be maintained diligently throughout the design and implementation lifecycle in order to secure optimal data quality, deliberate bias-mitigation aware practices, and optimal auditability. It should include a comprehensive record of data provenance, procurement, pre-processing, lineage, storage, and security as well as qualitative input from team members about determinations made with regard to data representativeness, data sufficiency, source integrity, data timeliness, data relevance, training/ testing/validating splits, and unforeseen data issues encountered across the workflow.","title":"Data fairness"},{"location":"aeg/chapter4/aifairness/#application-fairness","text":"Fairness considerations should enter into your AI project at the earliest possible stage of horizon scanning, problem selection, and project planning. This is because, the overall fairness and equity of an AI application is significantly determined by the objectives, goals, and policy choices that lie behind initial decisions to dedicate time and resources to its design, development, and deployment. For example, the choice made to build a biometric identification system, which uses live facial recognition technology to identify criminal suspects at public events, may be motivated by the objective to increase public safety and security. However, many members of the public may find this use of AI technology unreasonable, disproportionate, and potentially discriminatory. In particular, members of communities historically targeted by disproportionate levels of surveillance in law enforcement contexts may be especially concerned about the potential for abuse and harm. Appropriate fairness and equity considerations should, in this case, occur at the horizon scanning and project planning stage (e.g., as part of a stakeholder impact assessment process that includes engagement with potentially affected individuals and communities). Aligning the policy goals of a project team with the reasonable expectations and potential equity concerns of those affected is a key component of the fairness of the application. Application fairness, therefore, entails that the policy objectives of an AI project are non-discriminatory and are acceptable to and line up with the aims, expectations, and sense of justice possessed by those affected.[@dobbe2018]-[@eckhouse2018]-[@green2018a]-[@green2018b]-[@mitchell2021]-[@passi2019] As such, whether the decision to engage in the production and use of an AI technology can be described as \u201cfairness-aware\u201d depends upon ethical and policy considerations that are external and prior to considerations of the technical feasibility of building an accurate and optimally performing system or the practical feasibility of accessing, collecting, or acquiring enough and the right kind of data. Beyond this priority of assuring the equity and ethical permissibility of policy goals, application fairness requires additional considerations in framing decisions made at the horizon scanning and project scoping or planning stage: 1 Equity considerations surrounding real-world context of the policy issue to be solved: When your project team is assessing the fairness of using an AI solution to address a particular policy issue, it is important to consider how equity considerations extend beyond the statistical and sociotechnical contexts of designing, developing, and deploying the system. Applied concepts of AI fairness and equity should not be treated, in a technology-centred way, as originating exclusively from the design and use of any particular AI system. Nor should they exclusively be treated as abstractions that can be engineered into an AI application through technical or mathematical retooling (e.g., by operationalising formal fairness criteria).[@fazelpour2020]-[@green2020]-[@leslie2020]-[@selbst2019] When designers of algorithmic systems limit their understanding of the scope of \u2018fairness\u2019 or \u2018equity\u2019 to these two dimensions, it can artificially constrain their perspectives in such a way that they erroneously treat only the patterns of bias and discrimination that arise in AI innovation practices or that can be measured, formalised, or statistically digested as actionable indicators of inequity. Rather, equity considerations should be grounded in a human-centred approach, which includes reflection on and critical examination of the wider social and economic patterns of disadvantage, injustice, and discrimination that arise in the real-world contexts surrounding the policy issue in question. Such considerations should include an exploration of how such patterns of inequity may lead, on the one hand, to the disparate distribution of the risks and adverse impacts of the AI system or, on the other, to a lack of equitable access to its potential benefits. For instance, while the development of an AI chatbot to replace a human serviced medical helpline may provide effective healthcare guidance for some, it could have disparate adverse impacts on others, such as vulnerable elderly populations or socioeconomically deprived groups who may face barriers to accessing and using the app. Here, reflection on the real-world contexts surrounding the provision of this type of healthcare support yields a more informed and compassionate awareness of social and economic conditions that could impair the fairness of the application. 2 Equity considerations surrounding the group targeted by the AI innovation intervention: Each AI application that makes predictions about or classifies people, targets a specific subset of the wider population to which they belong. For instance, a r\u00e9sum\u00e9 filtering system that is used to select desirable candidates in a recruitment process will draw from a pool of job applicants that constitute a subgroup within the broader population. Potential equity issues may arise here because the selection of subpopulations sampled by AI applications is non-random. Instead, the sample selection may reflect particular social patterns, structures, and path dependencies that are unfair or discriminatory.[@mitchell2021] In the case of the r\u00e9sum\u00e9 filtering system, the sample may reflect long-term hiring patterns where a disproportionate number of male job candidates from elite universities (or those with similarly privileged educational backgrounds) have been actively recruited. Such practices have historically excluded people from other gender identities and socioeconomic and educational backgrounds. As a result, the pattern of inequity surfaces, in this instance, not within the sampled subset of the population but rather in the way that discriminatory social structures have led to the selection of a certain group of individuals into that subset.[@mehrabi2021]-[@olteanu2019] 3 Equity considerations surrounding the way that the model\u2019s output shapes the range of possible decision-outcomes: AI applications that assist human decision-making shape and delimit the range of possible outcomes for the problem areas they address.[@mitchell2021] For example, a predictive risk model used in children\u2019s social care may generate an output that directly influences the space of choices available to a social worker. Because the model\u2019s target is the identification of at-risk children, it may lead to social care decisions that focus narrowly on whether a child needs to be taken into care. This centring of negative outcomes could restrict the range of viable choices open to the social worker insofar as it de-emphasises the potential availability of other strengths-based approaches (e.g., stewarding positive family functioning through social supports and identifying and promoting protective factors). These alternative decision-making paths could be closed off in the social care environment given how the predictive risk model\u2019s outputs restrictively shape the range of actions that can be taken to address the problem it is being used to inform.","title":"Application Fairness"},{"location":"aeg/chapter4/aifairness/#model-design-and-development-fairness","text":"Because human beings have a hand in all stages of the construction of AI systems, fairness-aware design must take precautions across the AI project workflow to prevent bias from having a discriminatory influence:","title":"Model design and development fairness"},{"location":"aeg/chapter4/aifairness/#design-phase","text":"Problem formulation At the initial stage of problem formulation and outcome definition, technical and non-technical members of your team should work together to translate project goals into measurable targets.[@fazelpour2021]-[@leslie2019]-[@obermeyer2019]-[@obermeyer2021]-[@passi2019] This will involve the use of both domain knowledge and technical understanding to define what is being optimised in a formalisable way and to translate the project\u2019s objective into a target variable or measurable proxy, which operates as a statistically actionable rendering of the defined outcome. At each of these points, choices must be made about the design of the algorithmic system that may introduce structural biases which ultimately lead to discriminatory harm. Special care must be taken here to identify affected stakeholders and to consider how vulnerable groups might be negatively impacted by the specification of outcome variables and proxies. Attention must also be paid to the question of whether these specifications are reasonable and justifiable given the general purpose of the project and the potential impacts that the outcomes of the system\u2019s use will have on the individuals and communities involved. These challenges of fairness aware design at the problem formulation stage show the need for making diversity and inclusive participation a priority from the start of the AI project lifecycle. This involves both the collaboration of the entire team and the attainment of stakeholder input about the acceptability of the project plan. This also entails collaborative deliberation across the project team and beyond about the ethical impacts of the design choices made.","title":"Design phase"},{"location":"aeg/chapter4/aifairness/#development-phase","text":"Data pre-processing Human judgement enters into the process of algorithmic system construction at the stage of labelling, annotating, and organising the training data to be utilised in building the model. Choices made about how to classify and structure raw inputs must be taken in a fairness-aware manner with due consideration given to the sensitive social contexts that may introduce bias into such acts of classification. Similar fairness aware processes should be put in place to review automated or outsourced classifications. Likewise, efforts should be made to attach solid contextual information and ample metadata to the datasets, so that downstream analyses of data processing have access to properties of concern in bias mitigation. The constructive task of selecting the attributes or features that will serve as input variables for a model requires human decisions to be made about the sorts of information that may or may not be relevant or rationally required to yield an accurate and unbiased classification or prediction. Moreover, the feature engineering tasks of aggregating, extracting, or decomposing attributes from datasets may introduce human appraisals that have biasing effects. At this stage, human decisions about how to group or disaggregate input features (e.g., how to carve up categories of gender or ethnic groups) or about which input features to exclude altogether (e.g., leaving out deprivation indicators in a predictive model for clinical diagnostics) can have significant downstream influences on the fairness and equity of an AI system. This applies even when algorithmic techniques are employed to extract and engineer features, or support the selection of features (e.g., to optimise predictive power). For this reason, discrimination awareness should play a large role at this stage of the AI model-building workflow as should domain knowledge and policy expertise. Your team should proceed in the model development stage aware that choices made about grouping or separating and including or excluding features as well as more general judgements about the comprehensiveness or coarseness of the total set of features may have significant consequences for historically marginalised, vulnerable, or protected groups. Model selection The model selection stage determines the model type and structure that will be produced in the next stages. In some projects, this will involve the selection of multiple prospective models for the purpose of comparison based on some performance metric, such as accuracy or sensitivity. The set of relevant models is likely to have been highly constrained by many of the issues dealt with in previous stages (e.g., available resources and skills, problem formulation), for instance, where the problem demands a supervised learning algorithm instead of an unsupervised learning algorithm. Fairness and equity issues can surface in model selection processes in at least two ways: First, they can arise when the choice between algorithms has implications for explainability. For instance, it may be the case that there are better performing models in the pool of available options but which are less interpretable than others. This difference becomes significant when the processing of social or demographic data increases the risk that biases or discriminatory proxies lurk in the algorithmic architecture. Model interpretability can increase the likelihood of detecting and redressing such discriminatory elements. Second, fairness and equity issues can arise when the choice between algorithms has implications for the differential performance of the final model for subgroups of the population. For instance, where several different learning algorithms are simultaneously trained and tested, one of the resulting models could have the highest overall level of accuracy while, at the same time, being less accurate than others in the way it performs for one or more marginalised sub-groups. In cases like this, technical members of your project team should proceed with attentiveness to mitigating any possible discriminatory effects of choosing one model over another and should consult members of the wider team\u2014and impacted stakeholders, where possible\u2014about the acceptability of any trade-offs between overall model accuracy and differential performance. Model training, testing, and validation The process of tuning hyperparameters, setting metrics, and resampling data at the training, validation, and testing stages also involves human choices that may have fairness and equity consequences in the trained model. For instance, the way your project team determines the training-testing split of the dataset can have a considerable impact on the need for external validation to ensure that the model\u2019s performance \u201cin the wild\u201d meets reasonable expectations. Therefore, your technical team should proceed with an attentiveness to bias risks, and continual iterations of peer review and project team consultation should be encouraged to ensure that choices made in adjusting the dials, parameters, and metrics of the model are in line with bias mitigation and discriminatory non-harm goals. Evaluating and validating model structures Design fairness also demands close assessment of the existence in the trained model of lurking or hidden proxies for discriminatory features that may act as significant factors in its output. Including such hidden proxies in the structure of the model may lead to implicit \u2018redlining\u2019 (the unfair treatment of a sensitive group on the basis of an unprotected attribute or interaction of attributes that \u2018stands in\u2019 for a protected or sensitive one). Designers must additionally scrutinise the moral justifiability of the significant correlations and inferences that are determined by the model\u2019s learning mechanisms themselves, for these correlations and inferences could encode social and historical patterns of discrimination where these are baked into the dataset. In cases of the processing of social or demographic data related to human features, where the complexity and high dimensionality of machine learning models preclude the confirmation of the discriminatory non-harm of these inferences (for reason of their opaqueness by human assessors), these models should be avoided.[@icoturing2020]-[@mitchell2021]-[@rudin2019] In cases where this is not possible, a different, more transparent and explainable model or portfolio of models should be chosen. Model structures must also be confirmed to be procedurally fair, in a strict technical sense. This means that any rule or procedure employed in the processing of data by an algorithmic system should be consistently and uniformly applied to every decision subject whose information is being processed by that system. AI project teams should be able to certify that any relevant rule or procedure is applied universally and uniformly to all relevant individuals. Implementers of the system, in this respect, should be able to show that any model output is replicable when the same rules and procedures are applied to the same inputs. Such a uniformity of the application of rules and procedures secures the equal procedural treatment of decision subjects and precludes any rule-changes in the algorithmic processing targeted at a specific person that may disadvantage that individual vis-\u00e0-vis any other. It is important to note that the consistent and uniform application of rules over time apply to deterministic algorithms, whose parameters are static and fixed after model development. Close attention should be paid, in this sense, to the procedural fairness issues that may arise with the use of dynamic learning algorithms. This is because the parameters and inferences of such systems evolve over time and can yield different outputs for the same inputs at different points in system\u2019s lifespan.","title":"Development phase"},{"location":"aeg/chapter4/aifairness/#metric-based-fairness","text":"As part of the safeguarding of discriminatory non-harm and diligent fairness and equity considerations, well-informed consideration must be put into how you are going to define and measure the formal metrics of fairness that can be operationalised into the AI system you are developing. Metric-based fairness involves the mathematical mechanisms that can be incorporated into an AI model to allocate the distribution of outcomes and error rates for relevant subpopulations (e.g., groups with protected or sensitive characteristics). In formulating your approach to metric-based fairness, your project team will be confronting challenging issues like the justifiability of differential treatment based on sensitive or protected attributes, where differential treatment can indicate differences in the distribution of model outputs or the distribution of error rates and performance indicators like precision or sensitivity. There is a great diversity of beliefs in this area as to what makes the consequences of an algorithmically supported decision allocatively equitable, fair, and just. Different approaches to metric-based fairness\u2014detailed below\u2014stress different principles: some focus on demographic parity, some on individual fairness, others on error rates equitably distributed across subpopulations. Regardless of this diversity of views, your project team must ensure that the choices made regarding formal fairness metrics are lawful and conform to governing equality, non-discrimination, and human rights laws. Where appropriate, relevant experts should be consulted to confirm the legality of such choices. Your determination of metric-based fairness should heavily depend both on the specific use case being considered and the technical feasibility of incorporating your chosen criteria into the construction of the AI system. (Note that different fairness-aware methods involve different types of technical interventions at the pre-processing, modelling, or post-processing stages of production). Again, this means that determining your fairness definition should be a cooperative and multidisciplinary effort across the project team. You will find below a summary table of several of the main definitions of metric-based fairness that have been integrated by researchers into formal models as well as a list of current articles and technical resources, which should be consulted to orient your team to the relevant knowledge base. (Note that this is a rapidly developing field, so your technical team should keep updated about further advances.) The first four fairness types fall under the category of group fairness and allow for comparative criteria of non-discrimination to be considered in model construction and evaluation. The final two fairness types focus instead on cases of individual fairness, where context-specific issues of effective bias are considered and assessed at the level of the individual agent. Take note, though, that these technical approaches have limited scope in terms of the bigger picture issues of application and design fairness that we have already stressed. Moreover, metric-based approaches face other practical and technical barriers. For instance, to carry out group comparisons, formal approaches to fairness require access to data about sensitive/protected attributes as well as accurate demographic information about the underlying population distribution (both of which may often be unavailable or unreliable, and furthermore, the work of identifying sensitive/protected attributes may pose additional risks of bias). Metric-based approaches also face challenges in the way they handle combinations of protected or sensitive characteristics that may amplify discriminatory treatment. These have been referred to as intersectional attributes (e.g. the combination gender and race characteristics), and they must also be integrated into fairness and equity considerations. Lastly, there are unavoidable trade-offs, inconsistencies, or even incompatibilities, between mathematical definitions of metric-based fairness that must be weighed in determining which of them are best fit for a particular use case. For instance, the desideratum for equalised odds (error rate balance) across subgroups can clash with the desideratum for equalised model calibration (correct predictions of gradated outcomes) or parity of positive predictive values across subgroups.[@chouldechova2017]-[@flores2016]-[@friedler2021]-[@kleinenberg2017]-[@mitchell2021]","title":"Metric-based fairness"},{"location":"aeg/chapter4/aifairness/#selecting-fairness-metrics","text":"Demographic/statistical parity (group fairness) An outcome is fair if each group in the selected set receives benefit in equal or similar proportions, i.e. if there is no correlation between a sensitive or protected attribute and the allocative result. [@calders2009]-[@denis2021]-[@dwork2011]-[@feldman2015]-[@zemel2013] This approach is intended to prevent disparate impact, which occurs when the outcome of an algorithmic process disproportionately harms members of disadvantaged or protected groups. True positive rate parity (group fairness) An outcome is fair if the \u2018true positive\u2019 rates of an algorithmic prediction or classification are equal across groups.[@zafar2017] This approach is intended to align the goals of bias mitigation and accuracy by ensuring that the accuracy of the model is equivalent between relevant population subgroups. This method is also referred to as \u2018equal opportunity\u2019 fairness because it aims to secure equalised odds of an advantageous outcome for qualified individuals in a given population regardless of the protected or disadvantaged groups of which they are members. Equalised odds (group fairness) An outcome is fair if false positive and true positive rates are equal across groups. In other words, both the probability of incorrect positive predictions and the probability of correct positive predictions should be the same across protected and privileged groups.[@hardt2016]-[@verma2018] This approach is motivated by the position that sensitive groups and advantaged groups should have similar error rates in outcomes of algorithmic decisions. Positive predictive value parity (group fairness) An outcome is fair if the rates of positive predictive value (the fraction of correctly predicted positive cases out of all predicted positive cases) are equal across sensitive and advantaged groups.[@chouldechova2017] Outcome fairness is defined here in terms of a parity of precision, where the probability of members from different groups actually having the quality they are predicted to have is the same across groups. Individual fairness (individual fairness) An outcome is fair if it treats individuals with similar relevant qualifications similarly. This approach relies on the establishment of a similarity metric that shows the degree to which pairs of individuals are alike with regard to a specific task.[@dwork2012]-[@kusner2017] Counterfactual fairness (individual fairness) An outcome is fair if an automated decision made about an individual belonging to a sensitive group would have been the same were that individual a member of a different group in a closest possible alternative (or counterfactual) world.[@kusner2017] Like the individual fairness approach, this method of defining fairness focuses on the specific circumstances of an affected decision subject, but, by using the tools of contrastive explanation, it moves beyond individual fairness insofar as it brings out the causal influences behind the algorithmic output. It also presents the possibility of offering the subject of an automated decision knowledge of what factors if changed, could have influenced a different outcome. This could provide them with actionable recourse to change an unfavourable decision.","title":"Selecting fairness metrics"},{"location":"aeg/chapter4/aifairness/#system-implementation-fairness","text":"When your project team is approaching the beta stage, you should begin to build out your plan for implementation training and support. This plan should include adequate preparation for the responsible and unbiased deployment of the AI system by its on-the-ground users. Automated decision-support systems present novel risks of bias and misapplication at the point of delivery, so special attention should be paid to preventing harmful or discriminatory outcomes at this critical juncture of the AI project lifecycle. In order to design an optimal regime of implementer training and support, you should pay special attention to the unique pitfalls of bias-in-use to which the deployment of AI technologies give rise. These can be loosely classified as decision-automation bias (more commonly just \u2018automation bias\u2019) and automation-distrust bias: Decision-Automation Bias I (over-reliance) Users of automated decision-support systems may tend to become hampered in their critical judgement, rational agency, and situational awareness as a result of their faith in the perceived objectivity, neutrality, certainty, or superiority of the AI system (Gaube et al., 2021). This may lead to over-reliance or errors of omission, where implementers lose the capacity to identify and respond to the faults, errors, or deficiencies, which might arise over the course of the use of an automated system, because they become complacent and overly deferent to its directions and cues.[@bussone2015] Decision-Automation Bias II (over-reliance) Decision-automation bias may also lead to over-compliance or errors of commission where implementers defer to the perceived infallibility of the system and thereby become unable to detect problems emerging from its use for reason of a failure to hold the results against available information. Both over-reliance and over-compliance may lead to what is known as out-of-loop syndrome where the degradation of the role of human reason and the de-skilling of critical thinking hampers the user\u2019s ability to complete the tasks that have been automated. This condition may bring about a loss of the ability to respond to system failure and may lead both to safety hazards and to dangers of discriminatory harm. To combat risks of decision-automation bias, you should operationalise strong regimes of accountability at the site of user deployment to steer human decision-agents to act on the basis of good reasons, solid inferences, and critical judgment. Automation-Distrust Bias At the other extreme, users of an automated decision-support system may tend to disregard its salient contributions to evidence-based reasoning either as a result of their distrust or scepticism about AI technologies in general or as a result of their over-prioritisation of the importance of prudence, common sense, and human expertise.[@longoni2019] An aversion to the non-human and amoral character of automated systems may also influence decision subjects\u2019 hesitation to consult these technologies in high impact contexts such as healthcare, transportation, and law.[@dietvorst2015] \u2018Research shows that people often prefer humans\u2019 forecasts to algorithms\u2019 forecasts,[@diab2011]-[@eastwood2012] more strongly weigh human input than algorithmic input,[@onkal2009]-[@promberger2006] and more harshly judge professionals who seek out advice from an algorithm rather than from a human.[@shaffer2013]-[@dietvorst2015]","title":"System implementation fairness"},{"location":"aeg/chapter4/aifairness/#taking-account-of-the-context-of-impacted-individuals-in-system-implementation","text":"In cases where you are utilising a decision-support AI system that draws on statistical inferences to determine outcomes which affect individual persons (e.g., a predictive risk model that helps an adult social care work determine an optimal path to caring for an elderly patient), fair implementation processes should include considerations of the specific context of each impacted individual.[@binns2017]-[@binns2018]-[@binns2019]-[@eiselson2013] Any application of this kind of system\u2019s recommendation will be based on statistical generalisations, which pick up relationships between the decision recipient\u2019s input data and patterns or trends that the AI model has extracted from the underlying distribution of that model\u2019s original dataset. Such generalisations will be predicated on inferences about a decision subject\u2019s future behaviours (or outcomes) that are based on populational-level correlations with the historical characteristics and attributes of the members of groups to which that person belongs rather than on the specific qualities of the person themself. Fair and equitable treatment of decision subjects entails that their unique life circumstances and individual contexts be taken into account in decision-making processes that are supported by AI-enabled statistical generalisations. For this reason, you should train your implementers to think contextually and holistically about how these statistical generalisations apply to the specific situation of the decision recipient. This training should involve preparing implementers to work with an active awareness of the socio-technical aspect of implementing AI decision-assistance technologies from an integrative and human- centred point of view. You should train implementers to apply the statistical results to each particular case with appropriate context-sensitivity and \u2018big picture\u2019 sensibility. This means that the dignity they show to decision subjects can be supported by interpretive understanding, reasonableness, and empathy.","title":"Taking account of the context of impacted individuals in system implementation"},{"location":"aeg/chapter4/aifairness/#ecosystem-fairness","text":"The AI project lifecycle does not exist in isolation from, or independent of, the wider social system of economic, legal, cultural, and political structures or institutions in which the production and use of AI systems take place. Rather, because it is embedded in these structures and institutions, the policies, norms, and procedures through which such structures and institutions influence human action also influence the AI project lifecycle itself. Inequities and biases at this ecosystem level can steer or shape AI research and innovation agendas in ways that can generate inequitable outcomes for protected, marginalised, vulnerable, or disadvantaged social groups. Such ecosystem-level inequities and biases may originate in and further reinforce asymmetrical power structures, unfair market dynamics, and skewed research funding schemes that favour or bring disproportionate benefit to those in the majority, or those who wield disproportionate power in society, at the cost of those who are disparately impacted by the discriminatory outcomes of the design, development, and use of AI technologies. Ecosystem-level inequities can occur across a wide range of AI research and innovation contexts. For instance, when AI-enabled health interventions such as mobile-phone-based symptom checker apps or remote AI-assisted medical triaging or monitoring are designed without regard for the barriers to access faced by protected, vulnerable, or disadvantaged groups, they will disproportionately benefit users from other, more advantaged groups. Likewise, where funding of the development of AI technologies, which significantly affect the public interest, is concentrated in the hands of firms or vendors who singly pursue commercial or financial gain, this may result in exclusionary research and innovation environments, AI systems that are built without due regard for broad fairness and equity impacts, and limitations on the development and deployment of wide-scale, publicly beneficial technology infrastructure. Moreover, widespread structural and institutional barriers to diversity and inclusion can create homogeneity on AI project teams (and among organisational policy owners) that has consequential fairness impacts on application decisions, resource allocation choices, and system deployment strategies. The concept of ecosystem fairness highlights the importance of mitigating the range of inequity-generating path dependencies that originate at the ecosystem level and that are often neglected by or omitted from analyses of AI project lifecycles. Ecosystem fairness therefore focuses both on rectifying the social structures and institutions that engender indirect discrimination and on addressing the structural and institutional changes needed for the corrective modification of social patterns that engender discriminatory impacts and socio-economic disadvantage. In this way, ecosystem fairness involves the transformation of unjust economic, legal, cultural, and political structures or institutions with the aim of the universal realisation of equitable social arrangements.","title":"Ecosystem fairness"},{"location":"aeg/chapter4/bias/","text":"Bias Mitigation \u00b6 Bias Self-assessment and Risk Management \u00b6 By pinpointing risks of bias or downstream discriminations, project teams can streamline possible solutions in a proactive, pre-emptive, and anticipatory way. This is what fairness-aware design and implementation will enable the team to do. At each stage of the AI project lifecycle, a collaborative Bias Self-assessment should be carried out with regard to the applicable dimension of fairness. This self-assessment consists of three steps: Step 1: Familiarising with biases and fairness types that are relevant to each project stage. Step 2: Reflecting and identifying how a particular AI project might be vulnerable to biases that may arise at each stage and pose risks to each relevant fairness type. Step 3: Determine and document bias risk mitigation actions that will be implemented to correct any existing problems that have been identified, strengthen areas of weakness that have possible discriminatory consequences, and take proactive bias-prevention measures in areas that have been identified as potential sources of risk. The Fairness Self-Assessment and Risk Mitigation template will help you go through this process. It locates a set of social, statistical, and cognitive biases within specific steps of an AI project lifecycle. These biases require ongoing reflection and deliberation to minimise the possible negative impact upon downstream activities or the risk of discriminatory outcomes. List of Biases 1 \u00b6 World biases \u00b6 Historical bias: Historical bias concerns pre-existing societal patterns of discrimination and social injustice\u2014and the prejudices and discriminatory attitudes that correspond to such patterns. These patterns, prejudices, and attitudes can be drawn into every stage of the AI innovation lifecycle and be perpetuated, reinforced, or exacerbated through inequitable innovation ecosystem dynamics and the pursuit of biased application choices and research agendas. They can also arise in AI innovation contexts when historical patterns of inequity or discrimination are inadvertently or unintentionally reproduced, or even augmented, in the development and use of an AI system\u2014even when the system is functioning to a high standard of accuracy and reliability.[@mehrabi2019]-[@suresh2019] For instance, even with scientifically sound sampling and feature selection, a project will exhibit historical bias where it perpetuates (or exacerbates) socioeconomic inequalities through the outcomes it generates. Structural racism: Structural racism (also sometimes called systemic racism) is a form of racial discrimination that is \u2018not simply the result of private prejudices held by individuals, but is also produced and reproduced by laws, rules, and practices, sanctioned and even implemented by various levels of government, and embedded in the economic system as well as in cultural and societal norms\u2019.[@bailey2021] Other forms of discrimination such as sexism, classism, ableism, ageism, antisemitism, and transphobia can also similarly have structural or systemic aspects. Institutional bias: Institutional bias is \u2018a tendency for the procedures and practices of particular institutions to operate in ways which result in certain social groups being advantaged or favoured and others being disadvantaged or devalued. This need not be the result of any conscious prejudice or discrimination but rather of the majority simply following existing rules or norms\u2019.[@chandler2011] Data biases \u00b6 Representation bias: When a population is either inappropriately represented (e.g. not allowing sufficient self-representation in demographic variables) or a sub-group is underrepresented in the dataset, the model may subsequently fail to generalise and under-perform for a sub-group (or sub-groups).[@feng2022]-[mehrabi2019]-[@suresh2019] For example, representation bias could arise in a symptom-checking application that has been trained on a data collected exclusively through smartphone use or online interaction as this dataset would likely underrepresent groups within the general population like elderly people who may lack access to smartphones or connectivity. Selection bias: Selection bias is a term used for a range of biases that affect the selection or inclusion of data points within a dataset. In general, this bias arises when an association is present between the variables being studied and additional factors that make it more likely that some data will be present in a dataset when compared to other possible data points in the space.[@mehrabi2019] For instance, where individuals differ in their geographic or socioeconomic access to an activity or service that is the site of data collection, this variation may result in their exclusion from the corresponding dataset. Likewise, where certain socioeconomically deprived or marginalised social groups are disproportionately dependent on a social service to fulfil basic needs, it may be oversampled if data is collected from the provision of that service. Admission bias: Admission rate bias is a subtype of selection bias. It occurs when the exposure to a risk and the occurrence of a disease increases the likelihood of admission or referral (hence, this is also known as \u2018referral bias\u2019). As a result, the cases included in any subsequent study or dataset will systematically differ from the population at large.[@spencer2017] Diagnostic decision bias: Diagnostic decision bias occurs in medical datasets that are based on implicit biases in clinical judgement about patient diagnosis (for instance, where biased medical training and unrepresentative or discriminatory clinical knowledge lead to the under-diagnosis of minority or under-served groups). This leads to \u2018differential diagnosis of patients with the same pathology, on the basis of demographic features\u2019 (Straw & Callison-Burch, 2020, p. 2) and, ultimately, datasets that reflect biased diagnoses rather than the true rate of disease. Chronological bias: Chronological bias arises when individuals in the dataset are added at different times, and where this chronological difference results in individuals being subjected to different methods or criteria of data extraction based on the time their data were recorded. For instance, if the dataset used to build a predictive risk model in children\u2019s social care spans over several years, large-scale care reforms, policy changes, adjustments in relevant statutes (such as changes to legal thresholds or definitions), and changes in data recording methods may create major inconsistencies in the data points extracted from person to person. Diagnostic access bias: Where barriers to patient access exist for diagnostic tests, there can be systematic exclusion of sub-groups from a dataset, resulting in the under- or over-estimation of the true prevalence or incidence of the disease.[@banerjee2017] These barriers can exist for myriad reasons, including cultural (e.g. distrust of formal healthcare institutions), geographic (e.g. remote living), socioeconomic (e.g. unable to get time off from childcare or work), among other reasons. Prevalence-incidence bias: Prevalence-incidence bias occurs as a result of the timing in which cases are included in the dataset for a study.[@spencer2017] For example, excluding patients who have died could result in the appearance of decreased severity for a disease. A longer timeframe between exposure and investigation could increase the likelihood of additional patients dying or recovering and being excluded from an analysis. Data coding bias: Data coding bias occurs when the misrepresentation or erasure of demographic characteristics such as gender or ethnicity by biased coding systems obscures patient needs, adversely impacts patient\u2019s access to appropriate screenings, diagnosis, and treatments, and, subsequently, prejudices the datasets in which biased coding is embedded. Missing data bias: Missing data can cause a wide variety of issues within an AI project, and these data may be missing for a variety of reasons related to broader social factors. Missing data bias can lead to inaccurate inferences and affect the validity of the model where it is the result of non-random but statistically informative events.[@chen2021]-[@feng2022] For instance, missing data bias may arise in predictive risk models used in social care to detect potentially harmful behaviour in adolescents where interview responses and longitudinal data collected over extended periods of time are used as part of the dataset. This can be seen in cases where interview questions about socially stigmatised behaviours or traits like drug use or sexual orientation trigger fears of punishment, humiliation, or reproach and thus prompt non-responses, and in cases where data collection over time leads to the inconsistent involvement and drop-out of study participants. Wrong sample-size bias: Using the wrong sample size for the study can lead to chance findings that fail to represent adequately the variability of the underlying data distribution, in the case of small samples, or findings that are statistically significant but not relevant or actionable, in the case of larger samples. Wrong sample size bias may occur in cases where model designers have included too many features in a machine learning algorithm. This is often referred to as the \u2018curse of dimensionality\u2019, a mathematical phenomenon wherein increases in the number of features or \u2018data dimensions\u2019 included in an algorithm means that exponentially more data points need to be sampled to enable good predictive or classificatory performance. Measurement quality bias: Measurement quality bias arises when under-resourced clinical environments lack the personnel, expertise, training capacity, and digital maturity to consistently collect high quality & complete data. They can also occur during deployment when measurement tools & devices are designed for dominant groups and consequently mismeasure minoritised, disadvantaged, or underserved groups. Design, development, and deployment biases \u00b6 Hardware bias: Hardware bias arises where physically instantiated algorithmic systems or measurement devices are not designed to consider the diverse physiological needs of minoritised, marginalised, disadvantaged, or other non-majority stakeholders. When deployed, such systems will therefore perform less effectively for members of these groups due to their design. Annotation bias: Annotation bias occurs when annotators incorporate their subjective perceptions into their annotations.[@chen2021]-[kuwatly2020] Data is often annotated by trained expert annotators or crowdsourced annotators. In its simplest form, annotators can choose an inaccurate label due to fatigue or lack of focus[@hovy2021] but annotation bias can also result from positionality limitations that derive from demographic features, such as age, education, or first language,[kuwatly2020] and other systemic cultural or societal biases that influence annotators.[@chen2021] For instance, annotators may label differently facial expressions of different ethnic, age, or gender groups,[@chen2021] have different levels of familiarity with communication norms,[@hovy2021] or different understandings of what should be annotated as harmful content.[@rottger2022] When the role of annotator subjectivity is unacknowledged or annotators are not specifically trained to mitigate biases, there are greater chances that the model will incorporate annotation biases and be unfair.[@chen2021]-[@rottger2022] Label or label choice bias: A label (or target variable) used within an algorithmic model may not have the same meaning for all data subjects. There may be a discrepancy between what sense the designers are seeking to capture in a label, or what they are trying to measure in it, and the way that affected individuals understand its meaning.[@corbett-davies2018]-[@feng2022]-[@obermeyer2019]-[@rajkomar2018] Where there is this kind of variation in meaning for different groups within a population, adverse consequences and discriminatory impact could follow. For example, designers of a predictive model in public health may choose \u2018patient wellbeing\u2019 as their label, defining it in terms of disease prevalence and hospitalisation. However, subpopulations who suffer from health disparities and socioeconomic deprivation may understanding wellbeing more in terms of basic functionings, the food security needed for health promotion, and the absence of the social environmental stressors that contribute to the development of chronic medical conditions. Were this predictive model to be used to develop public health policy, members of this latter group could suffer from a further entrenchment of poor health outcomes. Measurement bias: Measurement bias addresses the choice of how to measure the labels or features being used. It arises when the measurement scale or criteria being applied fails to capture data pertaining to the concepts or constructs that are being measured in a fair and equitable manner.[@jacobs2021]-[@mehrabi2019]-[@mitchell2021]-[olteanu2019]-[@suresh2019] For example, a recidivism risk model that uses prior arrests or arrested relatives as proxies to measure criminality may surface measurement bias insofar as patterns of arrest can reflect discriminatory tendencies to over-police certain protected social groups or biased assessments on the part of arresting officers rather than true criminality. Cohort bias: Cohort bias is a subtype of measurement bias where model designers use categories that default \u2018to traditional or easily measured groups without considering other potentially protected groups or levels of granularity (e.g. whether sex is recorded as male, female, or other or more granular categories)\u2019.[@rajkomar2018] Model selection bias: Model selection bias occurs when AI designers and developers choose a model that does not sufficiently respond to the needs and requirements of the research question or problem and the domain context or use-case. This may result not only in a lack of appropriate transparency and explainability (where a complex model is chosen and the context demands interpretable results) but also in outcomes based upon model inferences that do not reflect the level of nuance needed to confront the question or problem itself. Evaluation bias: Evaluation bias occurs during model iteration and evaluation as a result (1) of the application of evaluative metrics that mask the differential performance of an AI model for subgroups or (2) of the use of benchmarking datasets that do not accurately represent the composition of the target population.[@suresh2019] For example, an evaluation bias may occur where performance metrics that measure only overall accuracy are applied to a trained computer vision system that performs differentially for subgroups that have different skin tones. Likewise, evaluation biases arise where the external benchmark datasets that are used to evaluate the performance of trained models are insufficiently representative of the populations to which they will be applied. In the case of computer vision, this may occur where established benchmarks overly represent a segment of the populations (such as adult light-skinned males) and thus reinforce the biased criteria for optimal performance. Semantic bias: Semantic bias occurs when discriminatory inferences are allowed to arise in the architecture of a trained AI model and to remain an element of the productionalised system. When historical biases are baked into datasets in the form of discriminatory proxies or embedded prejudices (e.g. word embeddings that pick up on racial or gender biases), these biases can be semantically encoded in the model\u2019s covariates and parameters.[@hovy2022] Semantic biases occur when model design and evaluation processes fail to detect and mitigate such discriminatory aspects. Confounding: Confounding is a well-known causal concept in statistics, and commonly arises in observational studies. It refers to a distortion that arises when a (confounding) variable independently influences both the dependant and independent variables (e.g. exposure and outcome), leading to a spurious association and a skewed output. Clear examples of confounding can be found in the use of electronic health records (EHRs) that arise in clinical environments and healthcare processes. EHRs are observational data and often reflect not only the health status of patients, but also patients\u2019 interactions with the healthcare system. This can introduce confounders such as the frequency of inpatient medical testing reflecting the busyness or labour shortages of medical staff rather than the progression of a disease during hospitalisation, differences between onset of a disease and the date of diagnosis, and health conditions that are missing from the EHRs of a patient due to a non-random lack of testing. Contextual awareness and domain knowledge are crucial elements for identifying and redressing confounders. Social determinant blindness: Social determinant blindness occurs where clinical researchers and/or AI model designers act with a lack of awareness of the environmental, occupational, and life-course exposures of patients who face precarity, historical marginalisation, and discrimination. This leads to \u2018a lack of data on the complete etiologic context and exposures of a patient\u2019s health, including lack of opportunities and resources, such as broadband connectivity and social factors that can trigger adverse health outcomes\u2019.[@dankwa-mullan2022] Aggregation bias: Aggregation bias arises when a \u2018one-size-fits-all\u2019 approach is taken to the outputs of a trained algorithmic model (i.e. one where model results apply evenly to all members of the impacted population) even where variations in subgroup characteristics mean that mapping functions from inputs to outputs are not consistent across these subgroups.[@mehrabi2019]-[@suresh2019] In other words, in a model where aggregation bias is present, even when combinations of features affect members of different subgroups differently, the output of the system disregards the relevant variations in conditional distributions for the subgroups. This results in the loss of information, lowered performance, and, in cases where data from one subgroup is more prevalent than those of others, the development of a model that is more effective for that sub-group. Good examples of aggregation bias come up in clinical decision-support systems in medicine, where clinically significant variations between sexes and ethnicities\u2014in terms of disease aetiology, expression, complications, and treatment\u2014mean that systems which aggregate results by treating all data points similarly will not perform optimally for any subgroup. Reporting bias: Reporting bias arises when systems are produced without transparently reported evidence of effectiveness across demographic categories or testing for differential performance across sensitive, protected, or intersectional subgroups. This deficit in equity-aware testing and reporting can lead system developers and users to disregard or to attempt to bypass public accountability for equity, fairness, and bias mitigation. This means that transparency and accountability measures that should be in place to ensure that there are no discriminatory harms resulting from the use of the system are obscured or deprioritised. Population bias and training-serving skew: Population bias occurs when the demographics or characteristics of the cohort that comprises the training dataset differ from those of the original target population to whom the model is applied.[@mehrabi2019]-[@olteanu2019] For instance, a polygenic risk model that was trained primarily on data from a cohort of people of European ancestry in a country that has significant ethnic diversity is applied to people of non-European descent in that country and consequently performs worse for them. Another example arises in the social media research context where the demographic composition of internet platform users is skewed towards certain population subgroups and hence can differ from the studied target population.[@olteanu2019] Training-serving skew occurs, in a similar manner, when a model is deployed for individuals whose data is dissimilar to the dataset used to train, test, and validate the model.[@feng2022] This can occur, for instance, where a trained model is applied to a population in a geographical area different from where the original data was collected, or to the same population but at a time much later than when the training data was collected. In both cases, the trained model may fail to generalise because the new, out-of-sample inputs are being drawn from populations with different underlying distributions. Race correction bias: Race correction bias arises when algorithmic systems, which are adjusted or corrected for race or ethnicity, guide clinical decisions in ways that may direct more attention, care, or resources to Caucasian patients or to patients from other advantaged ethnic groups than to members of racialised and minoritised groups.[@vyas2020] They can also lead to informed mistrust, reluctance, and care avoidance on the part of racialised and minoritised groups and thus to harmful health outcomes for them. It has been argued that race correction is not only an unreliable proxy for genetic and biological difference but also fails to surface the underlying social causes of health disparities. Cause-effect bias: Cause-effect bias arises when users or implementers of decision-support systems, which generate inferences based upon statistical correlations, mistakenly assume that correlation implies causation without examining the validity of such an attribution.[@fazelpour2021]-[@mehrabi2019] For instance, a model for predicting pneumonia risk and hospital re-admission shows that patients with asthma have a lower risk of death than non-asthmatics, leading users to erroneously conclusion that having asthma is a protective factor in the risk scenario. However, a closer look at the model\u2019s inferences demonstrates that the correlation of asthma to lower risk was attributable to the fact that patients in the cohort known to have asthma were, as a common practice, automatically triaged to the Intensive Care Unit as a precautionary measure, and this meant that they had a 50% reduction in mortality risk.[@caruana2015] Implementation bias: Implementation bias refers to any bias that arises when a system is implemented or used in ways that were not intended by the designers or developers but, nevertheless, made more likely due to affordances of the system or its deployment. For example, a biometric identification system that is used by a public authority to assist in the detection of potential terrorist activity could be repurposed to target and monitor activists or political opponents. Decision-automation bias: Decision-automation bias arises when users of automated decision-support systems become hampered in their critical judgment, rational agency, and situational awareness as a result of their faith in the efficacy of the system. This may lead to over-reliance or errors of omission, where implementers lose the capacity to identify and respond to the faults, errors, or deficiencies, which might arise over the course of the use of an automated system, because they become complacent and overly deferent to its directions and cues. Decision-automation bias may also lead to over-compliance or errors of commission where implementers defer to the perceived infallibility of the system and thereby become unable to detect problems emerging from its use for reason of a failure to hold the results against available information. Automation-distrust bias: Automation-distrust bias arises when users of an automated decision-support system disregard its salient contributions to evidence-based reasoning either as a result of their distrust or scepticism about AI technologies in general or as a result of their over-prioritisation of the importance of prudence, common sense, and human expertise. An aversion to the non-human and amoral character of automated systems may also influence decision subjects\u2019 hesitation to consult these technologies in high impact contexts such as healthcare, transportation, and law. Dismissal bias: Dismissal bias is a \u2018conscious or unconscious desensitisation to alerts that are systematically incorrect for a protected group (e.g. an early warning score for patients with sepsis). Alert fatigue is a form of this\u2019.[@rajkomar2018] Status quo bias: An affectively motivated preference for \u201cthe way things are currently\u201d, which can prevent more innovative or effective processes or services being implemented. This bias is most acutely felt during the transition between projects, such as the choice to deprovision a system and begin a new project, in spite of deteriorating performance from the existing solution. Although this bias is often treated as a cognitive bias, we highlight it here as a social bias to draw attention to the broader social or institutional factors that in part determine the status quo. Ecosystem biases \u00b6 Ecosystem bias: Ecosystem bias occurs when economic, legal, cultural, and political structures or institutions\u2014and the policies, norms, and procedures through which these structures and institution influence human action\u2014steer AI research and innovation agendas in ways that generate inequitable outcomes for minoritised, marginalised, vulnerable, historically discriminated against, or disadvantaged social groups.[@schwartz] Ecosystem biases, which exist in the wider social system wherein AI technologies are designed and used, may originate in and further entrench asymmetrical power structures, unfair market dynamics, and skewed research funding schemes that favour or bring disproportionate benefit to those in the majority, or those who wield disproportionate power in society, at the cost of those who are disparately impacted by the discriminatory outcomes of the design, development, and use of AI technologies. Privilege bias: Privilege bias occurs when health policies, institutions, and infrastructures skew the benefits of healthcare technologies and medical devices disproportionately towards privileged social groups. \u2018Models may be unavailable in settings where protected groups receive care or require technology/sensors disproportionately available to the nonprotected class\u2019.[@rajkomar2018] Research bias: Research bias occurs where there is a deficit in social and health equity standards to guide how AI research and innovation is funded, conducted, reviewed, published, and disseminated. It can also manifest in a lack of inclusion and diversity on research teams and in clinical trials, and limited studies incorporating representative real-world data for health insights.[@dankwa-mullan2022] Research bias additionally includes inequitable manifestations of funding structures or in incentives set by investors or funding institutions. Allocation discrepancy: Allocation discrepancy occurs when resources (such as extra clinical attention or social services) are withheld from a protected group because it is associated with fewer positive predictions.[@rajkomar2018] Optimism bias: Also known as the planning fallacy, optimism bias can lead project teams to under-estimate the amount of time required to implement a new system or plan adequately. In the context of the project lifecycle, this bias may arise during project planning, but can create downstream issues when implementing a model during the model productionalisation stage, due to a failure to recognise possible system engineering barriers. Law of the instrument (Maslow\u2019s hammer): This bias is best captured by the popular phrase \u2018If all you have is a hammer, everything looks like a nail\u2019. The phrase cautions against over-reliance on a particular tool or method, often one that is familiar to members of the project team. For example, a project team that is composed of experts in a specific ML technique may over-use that technique and mis-apply it in a context where a different technique would be better suited, or where it would be better not to use ML/AI technology at all. McNamara fallacy: McNamara fallacy describes the belief that quantitative information is more valuable than other information.[@schwartz2022] This can lead to scientistic reductivism,[@leslie2022] technochauvinism,[@broussard2018] or technological solutionism.[@morozov2013] The McNamara fallacy plays a significant role in biasing AI innovation processes when AI researchers, designers, and developers view algorithmic techniques and statistical insights as the only inputs capable of solving societal problems, thereby actively disregarding interdisciplinary understandings of the subtle historical and sociocultural contexts of inequity and discrimination. Informed mistrust: Informed mistrust occurs where, \u2018given historical exploitation and unethical practices, protected groups may believe that a model is biased against them. These patients may avoid seeking care from clinicians or systems that use the model or deliberately omit information. The protected group may be harmed by not receiving appropriate care\u2019.[@rajkomar2018] De-agentification bias: De-agentification bias occurs when social structures and innovation practices systemically exclude minoritised, marginalised, vulnerable, historically discriminated against, or disadvantaged social groups from participating or providing input in AI innovation ecosystems. \u2018Protected groups may not have input into the development, use, and evaluation of models. They may not have the resources, education, or political influence to detect biases, protest, and force correction\u2019.[@rajkomar2018] Feedback loops: Feedback loops occur where \u2018the clinician accepts the recommendation of a model even when it is incorrect to do so, [and therefore] the model's recommended versus administered treatments will always match. The next time the model is trained, it will learn to continue these mistakes\u2019.[@rajkomar2018] Positive results bias: Positive results bias, also known as publication bias, refers to the system-wide or social phenomenon of observing a skewed level of positive results published in journals, because negative or null results tend to go unpublished.[@pluddeman2017] The consequence of this can be the overestimation of efficacy for specific techniques or methods, as well as research duplication since other research teams might attempt to repeat studies that have already been performed but not published. An example of this was observed in the well-known 'reproducibility crisis' that affected the social psychology literature. Biases of rhetoric or spin: Biases of rhetoric or spin occur during the communication of research or development (e.g. model performance) and refer to the use of unjustified or illegitimate forms of persuasive language that lacks meaningful content or substantive evidence.[@heneghan2017] These biases relate to overemphasis of the performance or efficacy of a technique or intervention (e.g. showing comparative preference for the favoured technique to the detriment of alternatives). Cognition biases \u00b6 Status quo bias: Status quo bias is an affectively motivated preference for \u201cthe way things are currently\u201d, which can prevent more innovative or effective processes or services being implemented. This bias is most acutely felt during the transition period between projects, such as the choice not to deprovision a system and begin a new project, despite deteriorating performance from the existing solution. Although this bias is often treated as a cognitive bias, we also highlight it here as a social bias to draw attention to the broader social or institutional factors that in part determine the status quo. Confirmation bias: Confirmation bias arises from the tendency to search for, gather, or use information that confirms pre-existing ideas and beliefs, and to dismiss or downplay the significance of information that disconfirms one\u2019s favoured hypothesis. This can be the result of motivated reasoning or sub-conscious attitudes, which in turn may lead to prejudicial judgements that are not based on reasoned evidence. For example, confirmation biases could surface in the judgment of the user of an AI decision-support application, who believes in following common sense intuitions acquired through professional experience rather than the outputs of an algorithmic model and, for this reason, dismisses its recommendations regardless of their rational persuasiveness or veracity. Self-assessment bias: A tendency to evaluate one\u2019s abilities in more favourable terms than others, or to be more critical of others than oneself. In the context of a project team, this could include the overly positive assessment the group\u2019s abilities (e.g. through reinforcing groupthink). For instance, during project planning, a project team may believe that their resources and capabilities are sufficient for the objective of the project, but in fact be forced to either cut corners or deliver a sub-par product. Availability bias: The tendency to make judgements or decisions based on the information that is most readily available (e.g. more easily recalled). When this information is recalled on multiple occasions, the bias can be reinforced through repetition\u2014known as a 'cascade'. This bias can cause issues for project teams throughout the project lifecycle where decisions are influenced by available or oft-repeated information (e.g. hypothesis testing during data analysis). Na\u00efve realism: A disposition to perceive the world in objective terms that can inhibit recognition of socially constructed categories. For instance, treating \u2018employability\u2019 something that is objectively measurable and, therefore, able to be predicted by a machine learning algorithm based on objective factors (e.g. exam grades, educational attainment). Some of the biases in this list are specific to health care systems, bust most are widely applicable. They are taken from CITE. \u21a9","title":"Bias Mitigation"},{"location":"aeg/chapter4/bias/#bias-mitigation","text":"","title":"Bias Mitigation"},{"location":"aeg/chapter4/bias/#bias-self-assessment-and-risk-management","text":"By pinpointing risks of bias or downstream discriminations, project teams can streamline possible solutions in a proactive, pre-emptive, and anticipatory way. This is what fairness-aware design and implementation will enable the team to do. At each stage of the AI project lifecycle, a collaborative Bias Self-assessment should be carried out with regard to the applicable dimension of fairness. This self-assessment consists of three steps: Step 1: Familiarising with biases and fairness types that are relevant to each project stage. Step 2: Reflecting and identifying how a particular AI project might be vulnerable to biases that may arise at each stage and pose risks to each relevant fairness type. Step 3: Determine and document bias risk mitigation actions that will be implemented to correct any existing problems that have been identified, strengthen areas of weakness that have possible discriminatory consequences, and take proactive bias-prevention measures in areas that have been identified as potential sources of risk. The Fairness Self-Assessment and Risk Mitigation template will help you go through this process. It locates a set of social, statistical, and cognitive biases within specific steps of an AI project lifecycle. These biases require ongoing reflection and deliberation to minimise the possible negative impact upon downstream activities or the risk of discriminatory outcomes.","title":"Bias Self-assessment and Risk Management"},{"location":"aeg/chapter4/bias/#list-of-biases1","text":"","title":"List of Biases1"},{"location":"aeg/chapter4/bias/#world-biases","text":"Historical bias: Historical bias concerns pre-existing societal patterns of discrimination and social injustice\u2014and the prejudices and discriminatory attitudes that correspond to such patterns. These patterns, prejudices, and attitudes can be drawn into every stage of the AI innovation lifecycle and be perpetuated, reinforced, or exacerbated through inequitable innovation ecosystem dynamics and the pursuit of biased application choices and research agendas. They can also arise in AI innovation contexts when historical patterns of inequity or discrimination are inadvertently or unintentionally reproduced, or even augmented, in the development and use of an AI system\u2014even when the system is functioning to a high standard of accuracy and reliability.[@mehrabi2019]-[@suresh2019] For instance, even with scientifically sound sampling and feature selection, a project will exhibit historical bias where it perpetuates (or exacerbates) socioeconomic inequalities through the outcomes it generates. Structural racism: Structural racism (also sometimes called systemic racism) is a form of racial discrimination that is \u2018not simply the result of private prejudices held by individuals, but is also produced and reproduced by laws, rules, and practices, sanctioned and even implemented by various levels of government, and embedded in the economic system as well as in cultural and societal norms\u2019.[@bailey2021] Other forms of discrimination such as sexism, classism, ableism, ageism, antisemitism, and transphobia can also similarly have structural or systemic aspects. Institutional bias: Institutional bias is \u2018a tendency for the procedures and practices of particular institutions to operate in ways which result in certain social groups being advantaged or favoured and others being disadvantaged or devalued. This need not be the result of any conscious prejudice or discrimination but rather of the majority simply following existing rules or norms\u2019.[@chandler2011]","title":"World biases"},{"location":"aeg/chapter4/bias/#data-biases","text":"Representation bias: When a population is either inappropriately represented (e.g. not allowing sufficient self-representation in demographic variables) or a sub-group is underrepresented in the dataset, the model may subsequently fail to generalise and under-perform for a sub-group (or sub-groups).[@feng2022]-[mehrabi2019]-[@suresh2019] For example, representation bias could arise in a symptom-checking application that has been trained on a data collected exclusively through smartphone use or online interaction as this dataset would likely underrepresent groups within the general population like elderly people who may lack access to smartphones or connectivity. Selection bias: Selection bias is a term used for a range of biases that affect the selection or inclusion of data points within a dataset. In general, this bias arises when an association is present between the variables being studied and additional factors that make it more likely that some data will be present in a dataset when compared to other possible data points in the space.[@mehrabi2019] For instance, where individuals differ in their geographic or socioeconomic access to an activity or service that is the site of data collection, this variation may result in their exclusion from the corresponding dataset. Likewise, where certain socioeconomically deprived or marginalised social groups are disproportionately dependent on a social service to fulfil basic needs, it may be oversampled if data is collected from the provision of that service. Admission bias: Admission rate bias is a subtype of selection bias. It occurs when the exposure to a risk and the occurrence of a disease increases the likelihood of admission or referral (hence, this is also known as \u2018referral bias\u2019). As a result, the cases included in any subsequent study or dataset will systematically differ from the population at large.[@spencer2017] Diagnostic decision bias: Diagnostic decision bias occurs in medical datasets that are based on implicit biases in clinical judgement about patient diagnosis (for instance, where biased medical training and unrepresentative or discriminatory clinical knowledge lead to the under-diagnosis of minority or under-served groups). This leads to \u2018differential diagnosis of patients with the same pathology, on the basis of demographic features\u2019 (Straw & Callison-Burch, 2020, p. 2) and, ultimately, datasets that reflect biased diagnoses rather than the true rate of disease. Chronological bias: Chronological bias arises when individuals in the dataset are added at different times, and where this chronological difference results in individuals being subjected to different methods or criteria of data extraction based on the time their data were recorded. For instance, if the dataset used to build a predictive risk model in children\u2019s social care spans over several years, large-scale care reforms, policy changes, adjustments in relevant statutes (such as changes to legal thresholds or definitions), and changes in data recording methods may create major inconsistencies in the data points extracted from person to person. Diagnostic access bias: Where barriers to patient access exist for diagnostic tests, there can be systematic exclusion of sub-groups from a dataset, resulting in the under- or over-estimation of the true prevalence or incidence of the disease.[@banerjee2017] These barriers can exist for myriad reasons, including cultural (e.g. distrust of formal healthcare institutions), geographic (e.g. remote living), socioeconomic (e.g. unable to get time off from childcare or work), among other reasons. Prevalence-incidence bias: Prevalence-incidence bias occurs as a result of the timing in which cases are included in the dataset for a study.[@spencer2017] For example, excluding patients who have died could result in the appearance of decreased severity for a disease. A longer timeframe between exposure and investigation could increase the likelihood of additional patients dying or recovering and being excluded from an analysis. Data coding bias: Data coding bias occurs when the misrepresentation or erasure of demographic characteristics such as gender or ethnicity by biased coding systems obscures patient needs, adversely impacts patient\u2019s access to appropriate screenings, diagnosis, and treatments, and, subsequently, prejudices the datasets in which biased coding is embedded. Missing data bias: Missing data can cause a wide variety of issues within an AI project, and these data may be missing for a variety of reasons related to broader social factors. Missing data bias can lead to inaccurate inferences and affect the validity of the model where it is the result of non-random but statistically informative events.[@chen2021]-[@feng2022] For instance, missing data bias may arise in predictive risk models used in social care to detect potentially harmful behaviour in adolescents where interview responses and longitudinal data collected over extended periods of time are used as part of the dataset. This can be seen in cases where interview questions about socially stigmatised behaviours or traits like drug use or sexual orientation trigger fears of punishment, humiliation, or reproach and thus prompt non-responses, and in cases where data collection over time leads to the inconsistent involvement and drop-out of study participants. Wrong sample-size bias: Using the wrong sample size for the study can lead to chance findings that fail to represent adequately the variability of the underlying data distribution, in the case of small samples, or findings that are statistically significant but not relevant or actionable, in the case of larger samples. Wrong sample size bias may occur in cases where model designers have included too many features in a machine learning algorithm. This is often referred to as the \u2018curse of dimensionality\u2019, a mathematical phenomenon wherein increases in the number of features or \u2018data dimensions\u2019 included in an algorithm means that exponentially more data points need to be sampled to enable good predictive or classificatory performance. Measurement quality bias: Measurement quality bias arises when under-resourced clinical environments lack the personnel, expertise, training capacity, and digital maturity to consistently collect high quality & complete data. They can also occur during deployment when measurement tools & devices are designed for dominant groups and consequently mismeasure minoritised, disadvantaged, or underserved groups.","title":"Data biases"},{"location":"aeg/chapter4/bias/#design-development-and-deployment-biases","text":"Hardware bias: Hardware bias arises where physically instantiated algorithmic systems or measurement devices are not designed to consider the diverse physiological needs of minoritised, marginalised, disadvantaged, or other non-majority stakeholders. When deployed, such systems will therefore perform less effectively for members of these groups due to their design. Annotation bias: Annotation bias occurs when annotators incorporate their subjective perceptions into their annotations.[@chen2021]-[kuwatly2020] Data is often annotated by trained expert annotators or crowdsourced annotators. In its simplest form, annotators can choose an inaccurate label due to fatigue or lack of focus[@hovy2021] but annotation bias can also result from positionality limitations that derive from demographic features, such as age, education, or first language,[kuwatly2020] and other systemic cultural or societal biases that influence annotators.[@chen2021] For instance, annotators may label differently facial expressions of different ethnic, age, or gender groups,[@chen2021] have different levels of familiarity with communication norms,[@hovy2021] or different understandings of what should be annotated as harmful content.[@rottger2022] When the role of annotator subjectivity is unacknowledged or annotators are not specifically trained to mitigate biases, there are greater chances that the model will incorporate annotation biases and be unfair.[@chen2021]-[@rottger2022] Label or label choice bias: A label (or target variable) used within an algorithmic model may not have the same meaning for all data subjects. There may be a discrepancy between what sense the designers are seeking to capture in a label, or what they are trying to measure in it, and the way that affected individuals understand its meaning.[@corbett-davies2018]-[@feng2022]-[@obermeyer2019]-[@rajkomar2018] Where there is this kind of variation in meaning for different groups within a population, adverse consequences and discriminatory impact could follow. For example, designers of a predictive model in public health may choose \u2018patient wellbeing\u2019 as their label, defining it in terms of disease prevalence and hospitalisation. However, subpopulations who suffer from health disparities and socioeconomic deprivation may understanding wellbeing more in terms of basic functionings, the food security needed for health promotion, and the absence of the social environmental stressors that contribute to the development of chronic medical conditions. Were this predictive model to be used to develop public health policy, members of this latter group could suffer from a further entrenchment of poor health outcomes. Measurement bias: Measurement bias addresses the choice of how to measure the labels or features being used. It arises when the measurement scale or criteria being applied fails to capture data pertaining to the concepts or constructs that are being measured in a fair and equitable manner.[@jacobs2021]-[@mehrabi2019]-[@mitchell2021]-[olteanu2019]-[@suresh2019] For example, a recidivism risk model that uses prior arrests or arrested relatives as proxies to measure criminality may surface measurement bias insofar as patterns of arrest can reflect discriminatory tendencies to over-police certain protected social groups or biased assessments on the part of arresting officers rather than true criminality. Cohort bias: Cohort bias is a subtype of measurement bias where model designers use categories that default \u2018to traditional or easily measured groups without considering other potentially protected groups or levels of granularity (e.g. whether sex is recorded as male, female, or other or more granular categories)\u2019.[@rajkomar2018] Model selection bias: Model selection bias occurs when AI designers and developers choose a model that does not sufficiently respond to the needs and requirements of the research question or problem and the domain context or use-case. This may result not only in a lack of appropriate transparency and explainability (where a complex model is chosen and the context demands interpretable results) but also in outcomes based upon model inferences that do not reflect the level of nuance needed to confront the question or problem itself. Evaluation bias: Evaluation bias occurs during model iteration and evaluation as a result (1) of the application of evaluative metrics that mask the differential performance of an AI model for subgroups or (2) of the use of benchmarking datasets that do not accurately represent the composition of the target population.[@suresh2019] For example, an evaluation bias may occur where performance metrics that measure only overall accuracy are applied to a trained computer vision system that performs differentially for subgroups that have different skin tones. Likewise, evaluation biases arise where the external benchmark datasets that are used to evaluate the performance of trained models are insufficiently representative of the populations to which they will be applied. In the case of computer vision, this may occur where established benchmarks overly represent a segment of the populations (such as adult light-skinned males) and thus reinforce the biased criteria for optimal performance. Semantic bias: Semantic bias occurs when discriminatory inferences are allowed to arise in the architecture of a trained AI model and to remain an element of the productionalised system. When historical biases are baked into datasets in the form of discriminatory proxies or embedded prejudices (e.g. word embeddings that pick up on racial or gender biases), these biases can be semantically encoded in the model\u2019s covariates and parameters.[@hovy2022] Semantic biases occur when model design and evaluation processes fail to detect and mitigate such discriminatory aspects. Confounding: Confounding is a well-known causal concept in statistics, and commonly arises in observational studies. It refers to a distortion that arises when a (confounding) variable independently influences both the dependant and independent variables (e.g. exposure and outcome), leading to a spurious association and a skewed output. Clear examples of confounding can be found in the use of electronic health records (EHRs) that arise in clinical environments and healthcare processes. EHRs are observational data and often reflect not only the health status of patients, but also patients\u2019 interactions with the healthcare system. This can introduce confounders such as the frequency of inpatient medical testing reflecting the busyness or labour shortages of medical staff rather than the progression of a disease during hospitalisation, differences between onset of a disease and the date of diagnosis, and health conditions that are missing from the EHRs of a patient due to a non-random lack of testing. Contextual awareness and domain knowledge are crucial elements for identifying and redressing confounders. Social determinant blindness: Social determinant blindness occurs where clinical researchers and/or AI model designers act with a lack of awareness of the environmental, occupational, and life-course exposures of patients who face precarity, historical marginalisation, and discrimination. This leads to \u2018a lack of data on the complete etiologic context and exposures of a patient\u2019s health, including lack of opportunities and resources, such as broadband connectivity and social factors that can trigger adverse health outcomes\u2019.[@dankwa-mullan2022] Aggregation bias: Aggregation bias arises when a \u2018one-size-fits-all\u2019 approach is taken to the outputs of a trained algorithmic model (i.e. one where model results apply evenly to all members of the impacted population) even where variations in subgroup characteristics mean that mapping functions from inputs to outputs are not consistent across these subgroups.[@mehrabi2019]-[@suresh2019] In other words, in a model where aggregation bias is present, even when combinations of features affect members of different subgroups differently, the output of the system disregards the relevant variations in conditional distributions for the subgroups. This results in the loss of information, lowered performance, and, in cases where data from one subgroup is more prevalent than those of others, the development of a model that is more effective for that sub-group. Good examples of aggregation bias come up in clinical decision-support systems in medicine, where clinically significant variations between sexes and ethnicities\u2014in terms of disease aetiology, expression, complications, and treatment\u2014mean that systems which aggregate results by treating all data points similarly will not perform optimally for any subgroup. Reporting bias: Reporting bias arises when systems are produced without transparently reported evidence of effectiveness across demographic categories or testing for differential performance across sensitive, protected, or intersectional subgroups. This deficit in equity-aware testing and reporting can lead system developers and users to disregard or to attempt to bypass public accountability for equity, fairness, and bias mitigation. This means that transparency and accountability measures that should be in place to ensure that there are no discriminatory harms resulting from the use of the system are obscured or deprioritised. Population bias and training-serving skew: Population bias occurs when the demographics or characteristics of the cohort that comprises the training dataset differ from those of the original target population to whom the model is applied.[@mehrabi2019]-[@olteanu2019] For instance, a polygenic risk model that was trained primarily on data from a cohort of people of European ancestry in a country that has significant ethnic diversity is applied to people of non-European descent in that country and consequently performs worse for them. Another example arises in the social media research context where the demographic composition of internet platform users is skewed towards certain population subgroups and hence can differ from the studied target population.[@olteanu2019] Training-serving skew occurs, in a similar manner, when a model is deployed for individuals whose data is dissimilar to the dataset used to train, test, and validate the model.[@feng2022] This can occur, for instance, where a trained model is applied to a population in a geographical area different from where the original data was collected, or to the same population but at a time much later than when the training data was collected. In both cases, the trained model may fail to generalise because the new, out-of-sample inputs are being drawn from populations with different underlying distributions. Race correction bias: Race correction bias arises when algorithmic systems, which are adjusted or corrected for race or ethnicity, guide clinical decisions in ways that may direct more attention, care, or resources to Caucasian patients or to patients from other advantaged ethnic groups than to members of racialised and minoritised groups.[@vyas2020] They can also lead to informed mistrust, reluctance, and care avoidance on the part of racialised and minoritised groups and thus to harmful health outcomes for them. It has been argued that race correction is not only an unreliable proxy for genetic and biological difference but also fails to surface the underlying social causes of health disparities. Cause-effect bias: Cause-effect bias arises when users or implementers of decision-support systems, which generate inferences based upon statistical correlations, mistakenly assume that correlation implies causation without examining the validity of such an attribution.[@fazelpour2021]-[@mehrabi2019] For instance, a model for predicting pneumonia risk and hospital re-admission shows that patients with asthma have a lower risk of death than non-asthmatics, leading users to erroneously conclusion that having asthma is a protective factor in the risk scenario. However, a closer look at the model\u2019s inferences demonstrates that the correlation of asthma to lower risk was attributable to the fact that patients in the cohort known to have asthma were, as a common practice, automatically triaged to the Intensive Care Unit as a precautionary measure, and this meant that they had a 50% reduction in mortality risk.[@caruana2015] Implementation bias: Implementation bias refers to any bias that arises when a system is implemented or used in ways that were not intended by the designers or developers but, nevertheless, made more likely due to affordances of the system or its deployment. For example, a biometric identification system that is used by a public authority to assist in the detection of potential terrorist activity could be repurposed to target and monitor activists or political opponents. Decision-automation bias: Decision-automation bias arises when users of automated decision-support systems become hampered in their critical judgment, rational agency, and situational awareness as a result of their faith in the efficacy of the system. This may lead to over-reliance or errors of omission, where implementers lose the capacity to identify and respond to the faults, errors, or deficiencies, which might arise over the course of the use of an automated system, because they become complacent and overly deferent to its directions and cues. Decision-automation bias may also lead to over-compliance or errors of commission where implementers defer to the perceived infallibility of the system and thereby become unable to detect problems emerging from its use for reason of a failure to hold the results against available information. Automation-distrust bias: Automation-distrust bias arises when users of an automated decision-support system disregard its salient contributions to evidence-based reasoning either as a result of their distrust or scepticism about AI technologies in general or as a result of their over-prioritisation of the importance of prudence, common sense, and human expertise. An aversion to the non-human and amoral character of automated systems may also influence decision subjects\u2019 hesitation to consult these technologies in high impact contexts such as healthcare, transportation, and law. Dismissal bias: Dismissal bias is a \u2018conscious or unconscious desensitisation to alerts that are systematically incorrect for a protected group (e.g. an early warning score for patients with sepsis). Alert fatigue is a form of this\u2019.[@rajkomar2018] Status quo bias: An affectively motivated preference for \u201cthe way things are currently\u201d, which can prevent more innovative or effective processes or services being implemented. This bias is most acutely felt during the transition between projects, such as the choice to deprovision a system and begin a new project, in spite of deteriorating performance from the existing solution. Although this bias is often treated as a cognitive bias, we highlight it here as a social bias to draw attention to the broader social or institutional factors that in part determine the status quo.","title":"Design, development, and deployment biases"},{"location":"aeg/chapter4/bias/#ecosystem-biases","text":"Ecosystem bias: Ecosystem bias occurs when economic, legal, cultural, and political structures or institutions\u2014and the policies, norms, and procedures through which these structures and institution influence human action\u2014steer AI research and innovation agendas in ways that generate inequitable outcomes for minoritised, marginalised, vulnerable, historically discriminated against, or disadvantaged social groups.[@schwartz] Ecosystem biases, which exist in the wider social system wherein AI technologies are designed and used, may originate in and further entrench asymmetrical power structures, unfair market dynamics, and skewed research funding schemes that favour or bring disproportionate benefit to those in the majority, or those who wield disproportionate power in society, at the cost of those who are disparately impacted by the discriminatory outcomes of the design, development, and use of AI technologies. Privilege bias: Privilege bias occurs when health policies, institutions, and infrastructures skew the benefits of healthcare technologies and medical devices disproportionately towards privileged social groups. \u2018Models may be unavailable in settings where protected groups receive care or require technology/sensors disproportionately available to the nonprotected class\u2019.[@rajkomar2018] Research bias: Research bias occurs where there is a deficit in social and health equity standards to guide how AI research and innovation is funded, conducted, reviewed, published, and disseminated. It can also manifest in a lack of inclusion and diversity on research teams and in clinical trials, and limited studies incorporating representative real-world data for health insights.[@dankwa-mullan2022] Research bias additionally includes inequitable manifestations of funding structures or in incentives set by investors or funding institutions. Allocation discrepancy: Allocation discrepancy occurs when resources (such as extra clinical attention or social services) are withheld from a protected group because it is associated with fewer positive predictions.[@rajkomar2018] Optimism bias: Also known as the planning fallacy, optimism bias can lead project teams to under-estimate the amount of time required to implement a new system or plan adequately. In the context of the project lifecycle, this bias may arise during project planning, but can create downstream issues when implementing a model during the model productionalisation stage, due to a failure to recognise possible system engineering barriers. Law of the instrument (Maslow\u2019s hammer): This bias is best captured by the popular phrase \u2018If all you have is a hammer, everything looks like a nail\u2019. The phrase cautions against over-reliance on a particular tool or method, often one that is familiar to members of the project team. For example, a project team that is composed of experts in a specific ML technique may over-use that technique and mis-apply it in a context where a different technique would be better suited, or where it would be better not to use ML/AI technology at all. McNamara fallacy: McNamara fallacy describes the belief that quantitative information is more valuable than other information.[@schwartz2022] This can lead to scientistic reductivism,[@leslie2022] technochauvinism,[@broussard2018] or technological solutionism.[@morozov2013] The McNamara fallacy plays a significant role in biasing AI innovation processes when AI researchers, designers, and developers view algorithmic techniques and statistical insights as the only inputs capable of solving societal problems, thereby actively disregarding interdisciplinary understandings of the subtle historical and sociocultural contexts of inequity and discrimination. Informed mistrust: Informed mistrust occurs where, \u2018given historical exploitation and unethical practices, protected groups may believe that a model is biased against them. These patients may avoid seeking care from clinicians or systems that use the model or deliberately omit information. The protected group may be harmed by not receiving appropriate care\u2019.[@rajkomar2018] De-agentification bias: De-agentification bias occurs when social structures and innovation practices systemically exclude minoritised, marginalised, vulnerable, historically discriminated against, or disadvantaged social groups from participating or providing input in AI innovation ecosystems. \u2018Protected groups may not have input into the development, use, and evaluation of models. They may not have the resources, education, or political influence to detect biases, protest, and force correction\u2019.[@rajkomar2018] Feedback loops: Feedback loops occur where \u2018the clinician accepts the recommendation of a model even when it is incorrect to do so, [and therefore] the model's recommended versus administered treatments will always match. The next time the model is trained, it will learn to continue these mistakes\u2019.[@rajkomar2018] Positive results bias: Positive results bias, also known as publication bias, refers to the system-wide or social phenomenon of observing a skewed level of positive results published in journals, because negative or null results tend to go unpublished.[@pluddeman2017] The consequence of this can be the overestimation of efficacy for specific techniques or methods, as well as research duplication since other research teams might attempt to repeat studies that have already been performed but not published. An example of this was observed in the well-known 'reproducibility crisis' that affected the social psychology literature. Biases of rhetoric or spin: Biases of rhetoric or spin occur during the communication of research or development (e.g. model performance) and refer to the use of unjustified or illegitimate forms of persuasive language that lacks meaningful content or substantive evidence.[@heneghan2017] These biases relate to overemphasis of the performance or efficacy of a technique or intervention (e.g. showing comparative preference for the favoured technique to the detriment of alternatives).","title":"Ecosystem biases"},{"location":"aeg/chapter4/bias/#cognition-biases","text":"Status quo bias: Status quo bias is an affectively motivated preference for \u201cthe way things are currently\u201d, which can prevent more innovative or effective processes or services being implemented. This bias is most acutely felt during the transition period between projects, such as the choice not to deprovision a system and begin a new project, despite deteriorating performance from the existing solution. Although this bias is often treated as a cognitive bias, we also highlight it here as a social bias to draw attention to the broader social or institutional factors that in part determine the status quo. Confirmation bias: Confirmation bias arises from the tendency to search for, gather, or use information that confirms pre-existing ideas and beliefs, and to dismiss or downplay the significance of information that disconfirms one\u2019s favoured hypothesis. This can be the result of motivated reasoning or sub-conscious attitudes, which in turn may lead to prejudicial judgements that are not based on reasoned evidence. For example, confirmation biases could surface in the judgment of the user of an AI decision-support application, who believes in following common sense intuitions acquired through professional experience rather than the outputs of an algorithmic model and, for this reason, dismisses its recommendations regardless of their rational persuasiveness or veracity. Self-assessment bias: A tendency to evaluate one\u2019s abilities in more favourable terms than others, or to be more critical of others than oneself. In the context of a project team, this could include the overly positive assessment the group\u2019s abilities (e.g. through reinforcing groupthink). For instance, during project planning, a project team may believe that their resources and capabilities are sufficient for the objective of the project, but in fact be forced to either cut corners or deliver a sub-par product. Availability bias: The tendency to make judgements or decisions based on the information that is most readily available (e.g. more easily recalled). When this information is recalled on multiple occasions, the bias can be reinforced through repetition\u2014known as a 'cascade'. This bias can cause issues for project teams throughout the project lifecycle where decisions are influenced by available or oft-repeated information (e.g. hypothesis testing during data analysis). Na\u00efve realism: A disposition to perceive the world in objective terms that can inhibit recognition of socially constructed categories. For instance, treating \u2018employability\u2019 something that is objectively measurable and, therefore, able to be predicted by a machine learning algorithm based on objective factors (e.g. exam grades, educational attainment). Some of the biases in this list are specific to health care systems, bust most are widely applicable. They are taken from CITE. \u21a9","title":"Cognition biases"},{"location":"aeg/chapter4/fairness/","text":"Introduction to the principle of fairness \u00b6 In this section, we will explore the complicated landscape of AI fairness definition as a preliminary step towards understanding the ways in which existing biases manifest in the design, development, and deployment of AI systems. Understanding how concepts of fairness are used in the field of AI ethics and governance is a crucial prerequisite to understanding where and how unfair biases arise across AI project lifecycles, because relevant notions of fairness operate both as ethical and legal criteria based upon which biases can be identified across the AI project workflow and as normative yardsticks against which they can be measured and then appropriately mitigated. When thinking about AI fairness, it is important to keep in mind that these technologies, no matter how neutral they may seem, are designed and produced by human beings, who are bound by the limitations of their own given contexts and by biases that can arise both in their cognitive processes and in the social environments that influence their actions and interactions. Pre-existing or historical configurations of discrimination and social injustice\u2014as well as the prejudices and biased attitudes that are shaped by such configurations\u2014can be drawn into the AI innovation lifecycle and create unfair biases at any point in the project workflow. This is the case from the earliest stages of agenda setting, problem selection, project planning, problem formulation, and data extraction to later phases of model development and system deployment. Additionally, the datasets used to train, test, and validate AI/ML models can encode socially and historically crystallised forms of inequity and discrimination, thereby embedding biases in an algorithmic model\u2019s variables, inferences, and architecture. This wide range of entry points for bias and discrimination across the AI/ML innovation lifecycle has complicated the notion of fairness, since the inception of fairness-centred approaches like \u2018discrimination-aware data mining\u2019 and \u2018fair machine learning\u2019 more than a decade ago.[@barocas2016a]-[@binns2017]-[@friedman1996]-[@hajian2013]. To be sure, possibilities for reaching consensus on a commonly accepted definition for AI/ML fairness and on how to put such a definition into practice have been hampered by the myriad technical and sociotechnical contexts in which fairness issues arise. Such prospects for consensus have also been challenged by the broad spectrum of views in society on what the concept of fairness means and how it should best be operationalised. For this reason, in this practical guidance, we take a context-based and society-centred approach to understanding AI/ML fairness that is anchored in two pillars. First, to understand how concepts of fairness are defined and applied in AI innovation contexts, we must begin by acknowledging that there is a plurality of views in the social world on the meaning of fairness\u2014myriad interpretations of its sense and significance within and across cultures, societies, and legal systems. For instance, the meaning of the term \u2018fairness\u2019, in its contemporary English language usage, has dozens of interpretations which include a range of related but distinctive ideas such as equity, consistency, non-discrimination, impartiality, justice, equality, honesty, and reasonableness.[@audard2014]-[@carr2017] Likewise, the translation of the word \u2018fairness\u2019 into other languages has proven to be notoriously difficult, with some researchers claiming that it cannot be consistently understood across different linguistic groups.[@audard2014]-[@vandenberghe2022] It is clear, from this vantage point, that fairness (and adjacent notions like equity, impartiality, justice, equality, and non-discrimination) must be approached with an appropriately nuanced responsiveness both to the many ways in which these concepts can be interpreted and to the many contexts in which they can be applied. Second, despite this pluralism in the understanding and application of the concept of fairness, there has been a considerable convergence around how the interrelated priorities of non-discrimination and equality constitute the justificatory nucleus of fairness concerns. Though some claim that fairness is ultimately a subjective value that varies according to individual preferences and cultural outlooks, general ethical and legal concepts of fairness are predicated on core beliefs in the equal moral status of all human beings and in the corollary right of all human beings to equal respect, concern, protection, and regard before the law. On this view, it is because each person possesses an intrinsic and equal moral worth that everyone deserves equal respect and concern\u2014respect and concern that is grounded in the common dignity and humanity of every person.[@dworkin2000]-[@carr2000]-[@giovanola2022] The normative core of fairness, in this respect, has to do with the moral duty to treat others as moral equals and to secure the membership of all in a \u2018moral community\u2019 where every person can regard themself as having equal value.[@vlastos1984] Wrongful discrimination, along these lines, occurs when decisions, actions, institutional dynamics, or social structures do not respect the equal moral standing of individual persons.[@eidelson2015]-[@giovanola2022]-[@sangiovanni2017] Convergence around this centrality of equality and non-discrimination as an indispensable cornerstone of fairness concerns has also led to their widespread acceptance as normative anchors of both international human rights law and anti-discrimination and equality statutes. In human rights law, interlocking principles of equality and non-discrimination are taken to be essential preconditions for the realisation of all human rights insofar as equality and non-discrimination are implied in the guarantee of the equal enjoyment and protection of fundamental rights and freedoms to all human beings per se.[@clifford2013] For this reason, principles of equality and non-discrimination are treated as jus cogens in human rights law\u2014i.e., they are treated as peremptory or foundational norms that permeate all human rights provisions and from which no derogation is permitted in any case.[@carozza2013]-[@clifford2013] In anti-discrimination and equality statutes in the UK and beyond, dovetailing priorities of equality and non-discrimination likewise form principal aims and essential underpinnings of fairness concerns. In this case, equality before the law manifests as equal protection from discriminatory harassment and from both direct and indirect kinds of discrimination (UK Equality Act, 2010). In discriminatory harassment, unwanted or abusive behaviour linked to a protected characteristic violates someone\u2019s dignity, degrades their identity, or creates an offensive environment for them. For example, an employer who makes a racist remark about a protected group in the presence of an employee from that racial background would be considered to have harassed that employee based on the protected characteristic of race. What are protected characteristics? In the 2010 UK Equality Act, protected classes include age, gender reassignment, being married or in a civil partnership, being pregnant or on maternity leave, disability, race including colour, nationality, ethnic or national origin, religion or belief, sex, and sexual orientation. The European Convention on Human Rights, which forms the basis of the UK\u2019s 1998 Human Rights Act, includes as protected characteristics \u2018sex, race, colour, language, religion, political or other opinion, national or social origin, association with a national minority, property, birth or other status'. Direct discrimination occurs when individuals are treated adversely based on their membership in some protected class. This type of discrimination is also known as \u2018disparate treatment\u2019 because it involves instances where otherwise similarly positioned individuals receive different and more-or-less favourable treatment on the basis of differences between their respective protected characteristics. For instance, direct discrimination would occur if an otherwise well-qualified job applicant were intentionally denied an opportunity for employment because of their age, disability, or sexual orientation. By contrast, indirect discrimination occurs when existing provisions, criteria, policies, arrangements, or practices\u2014which could appear on their face to be neutral\u2014disparately harm or unfairly disadvantage members of some protected class in comparison with others who are not members of that group. This type of discrimination is also known as \u2018disparate impact\u2019 because what matters here is not directly unfavourable treatment in individual cases but rather the broader disproportionate adverse effects of provisions, criteria, policies, arrangements, or practices that may subtly or implicitly disfavour members of some protected group while appearing to treat everyone equally. Indirect discrimination can therefore involve the impacts of tacitly unjust or unfair social structures, underlying inequalities, or systemic patterns of implicit historical bias that manifest unintentionally through prevailing norms, rules, policies, and behaviours. For instance, indirect discrimination would occur if a job advertisement specified that applicants needed to be native English speakers, for this would automatically disadvantage candidates of different nationalities regardless of their levels of fluency or language training. These three facets of anti-discrimination and equality law (harassment, direct discrimination, and indirect discrimination) have significantly shaped contemporary approaches to AI fairness.[@adams-prassl2022]-[@liu2018]-[@ntoutsi2019]-[@pessach2020]-[@watcher2021] Indeed, attempts to put the principle of AI fairness into practice have largely converged around the priority to do no discriminatory harm to affected people along each of these three vectors of potential injury. It is thus helpful to think about basic AI fairness considerations as tracking three corresponding questions (An example of an associated discriminatory harm is provided alongside each.): How could the use of the AI system we are planning to build or acquire\u2014or the policies, decisions, and processes behind its design, development, and deployment\u2014lead to the discriminatory harassment of impacted individuals (i.e., unwanted or abusive treatment of them which is linked to a protected characteristic and which violates their dignity, degrades their identity, or creates a humiliating or offensive environment for them)? ??? example \"Example\" An AI-enabled customer support chat bot is built from a large language model that has been pretrained on billions of data points scraped from the internet and then customised to provide tailored responses to customer questions about the provision of a public service. After the system goes live, it is soon discovered that, when certain customer names (which are indicative of protected classes) are entered, the chatbot generates racist and sexist text responses to customer inquiries. How could the use of the AI system we are planning to build or acquire\u2014or the policies, decisions, and processes behind its design, development, and deployment\u2014lead to the disproportionate adverse treatment of impacted individuals from protected groups on the basis of their protected characteristics? ??? example \"Example\" An AI system used to filter job applications in a recruitment process is trained on historic data that contains details about the characteristics of successful candidates over the past several years. Because white male applicants were predominantly hired over this time, the system learns to infer the likelihood of success based on proxy features connected to the protected characteristics of race and sex. It consequently filters out non-white and non-male job candidates from the applicant pool. How could the use of the AI system we are planning to build or acquire\u2014or the policies, choices, and processes behind its design, development, and deployment\u2014lead to indirect discrimination against impacted individuals from protected groups? ??? example \"Example\" An AI-enabled medical diagnosis tool is built as a smartphone application and made available to all participants in a national health system without sufficient considerations of the barriers to access faced by some citizens. It becomes clear after the app is launched that the device disproportionately favours younger, more digitally literate, and more affluent community members, while disadvantaging both the elderly, less digitally literate population and digitally deprived people who do not have access to smartphone technologies and internet connections. It is important to note, regarding this final question on indirect discrimination, that the Public Sector Equality Duty mandates considerations both of how to \u2018reduce the inequalities of outcome which result from socio-economic disadvantage\u2019 and of how to advance equality of opportunity and other substantive forms of equality. This means that our approach to putting the principle of AI fairness into practice must include social justice considerations that concentrate on how the production and use of AI technologies can address and rectify structural inequalities and institutionalised patterns of inequity and discrimination rather than reinforce or exacerbate them. We must consequently take a multi-pronged approach to AI fairness that integrates formal approaches to non-discrimination and equality (which focus primarily on consistent and impartial application of rules and equal treatment before the law) with more demanding substantive and transformative approaches (which focus on equalizing the distribution of opportunities and outcomes and on the fundamental importance of addressing the material pre-conditions and structural changes needed for the universal realisation of equitable social arrangements). What is social justice? Social justice is a commitment to the achievement of a society that is equitable, fair, and capable of confronting the root causes of injustice. In an equitable and fair society, all individuals are recognised as worthy of equal moral standing and are able to realise the full assemblage of fundamental rights, opportunities, and positions. In a socially just world, every person has access to the material means needed to participate fully in work life, social life, and creative life through the provision of proper education, adequate living and working conditions, general safety, social security, and other means of realising maximal health and well-being. Social justice also entails the advancement of diversity and participatory parity and a pluralistically informed recognition of identity and cultural difference. Struggles for social justice typically include accounting for historical and structural injustice coupled to demands for reparations and other means of restoring rights, opportunities, and resources to those who have been denied them or otherwise harmed.] Discriminatory non-harm \u00b6 While there are different ways to characterise or define fairness in the design and use of AI systems, you should consider the principle of discriminatory non-harm as a minimum required threshold of fairness. This principle directs us to do no harm to others through direct or indirect discrimination or through discriminatory harassment linked to a protected characteristic that violates the dignity of impacted individuals, degrades their identity, or creates a humiliating or offensive environment for them: Key Concept: Principle of Discriminatory Non-Harm (Do No Discriminatory Harm) The producers and users of AI systems should prioritise the identification and mitigation of biases and discriminatory influences, which could lead to direct or indirect discrimination or discriminatory harassment. This entails an end-to-end focus on how unfair biases and discriminatory influences could arise (1) in the processes behind the design, development, and deployment of these systems, (2) in the outcomes produced by their implementation, and (3) in the wider economic, legal, cultural, and political structures or institutions in which the AI project lifecycle is embedded\u2014and in the policies, norms, and procedures through which these structures and institution influence actions and decisions throughout the broader AI innovation ecosystem. Developers and users of AI systems should, in this respect, acknowledge and address discriminatory patterns that may originate in the data used to train, test, and validate the system and in the model architectures (i.e., the variables, parameters, inferences, etc.) that generate system outputs. Beyond this, the principle of discriminatory non-harm implies that producers and users of AI systems should ensure that their research, innovation, and implementation practices are undertaken in an optimally responsible and ethical manner, more broadly, in keeping with the historical tendency that deficiencies in the deployment and operation of faulty systems often disparately impact protected, underrepresented, or disadvantaged groups. The principle of discriminatory non-harm applies to any AI system that processes social or demographic data (i.e., data pertaining to features of human subjects, population- and group-level traits and characteristics, or patterns of human activity and behaviour). However, the principle applies equally to AI systems that process bio-physical or biomedical data. In this case, imbalanced datasets, selection biases, or measurement errors could have discriminatory effects on impacted individuals and communities\u2014for instance, where a demographic group\u2019s lack of representation in a biomedical dataset (e.g., one used to train a diagnostic prediction model) means that the trained system performs poorly for that group relative to others that are better represented in the data. Prioritising discriminatory non-harm implies that the producers and users of AI systems ensure that the decisions and behaviours of their models neither treat impacted individuals adversely based on their membership in some protected class or socioeconomic group nor, intentionally or unintentionally, generate discriminatory or inequitable impacts on affected individuals and communities that unfairly disadvantage members of some protected class or socioeconomic group in comparison with others who are not members of that class or group. It can also be seen as a proportional approach to bias mitigation because it sets a baseline for fair AI systems, while, nevertheless, creating conditions for developers and users to strive towards an ideal for fair and equitable outcomes for all people as moral equals and as members of a just community where every person can regard themself as having intrinsic dignity and equal moral standing. Finally, the scope of the principle means that, beyond designers and users, any individuals, organisations, or departments who are procuring AI systems must ensure that the vendors of such systems can demonstrate the mitigation of potential biases and discriminatory influences in the processes behind their production and in their outputs.","title":"Introduction to Fairness"},{"location":"aeg/chapter4/fairness/#introduction-to-the-principle-of-fairness","text":"In this section, we will explore the complicated landscape of AI fairness definition as a preliminary step towards understanding the ways in which existing biases manifest in the design, development, and deployment of AI systems. Understanding how concepts of fairness are used in the field of AI ethics and governance is a crucial prerequisite to understanding where and how unfair biases arise across AI project lifecycles, because relevant notions of fairness operate both as ethical and legal criteria based upon which biases can be identified across the AI project workflow and as normative yardsticks against which they can be measured and then appropriately mitigated. When thinking about AI fairness, it is important to keep in mind that these technologies, no matter how neutral they may seem, are designed and produced by human beings, who are bound by the limitations of their own given contexts and by biases that can arise both in their cognitive processes and in the social environments that influence their actions and interactions. Pre-existing or historical configurations of discrimination and social injustice\u2014as well as the prejudices and biased attitudes that are shaped by such configurations\u2014can be drawn into the AI innovation lifecycle and create unfair biases at any point in the project workflow. This is the case from the earliest stages of agenda setting, problem selection, project planning, problem formulation, and data extraction to later phases of model development and system deployment. Additionally, the datasets used to train, test, and validate AI/ML models can encode socially and historically crystallised forms of inequity and discrimination, thereby embedding biases in an algorithmic model\u2019s variables, inferences, and architecture. This wide range of entry points for bias and discrimination across the AI/ML innovation lifecycle has complicated the notion of fairness, since the inception of fairness-centred approaches like \u2018discrimination-aware data mining\u2019 and \u2018fair machine learning\u2019 more than a decade ago.[@barocas2016a]-[@binns2017]-[@friedman1996]-[@hajian2013]. To be sure, possibilities for reaching consensus on a commonly accepted definition for AI/ML fairness and on how to put such a definition into practice have been hampered by the myriad technical and sociotechnical contexts in which fairness issues arise. Such prospects for consensus have also been challenged by the broad spectrum of views in society on what the concept of fairness means and how it should best be operationalised. For this reason, in this practical guidance, we take a context-based and society-centred approach to understanding AI/ML fairness that is anchored in two pillars. First, to understand how concepts of fairness are defined and applied in AI innovation contexts, we must begin by acknowledging that there is a plurality of views in the social world on the meaning of fairness\u2014myriad interpretations of its sense and significance within and across cultures, societies, and legal systems. For instance, the meaning of the term \u2018fairness\u2019, in its contemporary English language usage, has dozens of interpretations which include a range of related but distinctive ideas such as equity, consistency, non-discrimination, impartiality, justice, equality, honesty, and reasonableness.[@audard2014]-[@carr2017] Likewise, the translation of the word \u2018fairness\u2019 into other languages has proven to be notoriously difficult, with some researchers claiming that it cannot be consistently understood across different linguistic groups.[@audard2014]-[@vandenberghe2022] It is clear, from this vantage point, that fairness (and adjacent notions like equity, impartiality, justice, equality, and non-discrimination) must be approached with an appropriately nuanced responsiveness both to the many ways in which these concepts can be interpreted and to the many contexts in which they can be applied. Second, despite this pluralism in the understanding and application of the concept of fairness, there has been a considerable convergence around how the interrelated priorities of non-discrimination and equality constitute the justificatory nucleus of fairness concerns. Though some claim that fairness is ultimately a subjective value that varies according to individual preferences and cultural outlooks, general ethical and legal concepts of fairness are predicated on core beliefs in the equal moral status of all human beings and in the corollary right of all human beings to equal respect, concern, protection, and regard before the law. On this view, it is because each person possesses an intrinsic and equal moral worth that everyone deserves equal respect and concern\u2014respect and concern that is grounded in the common dignity and humanity of every person.[@dworkin2000]-[@carr2000]-[@giovanola2022] The normative core of fairness, in this respect, has to do with the moral duty to treat others as moral equals and to secure the membership of all in a \u2018moral community\u2019 where every person can regard themself as having equal value.[@vlastos1984] Wrongful discrimination, along these lines, occurs when decisions, actions, institutional dynamics, or social structures do not respect the equal moral standing of individual persons.[@eidelson2015]-[@giovanola2022]-[@sangiovanni2017] Convergence around this centrality of equality and non-discrimination as an indispensable cornerstone of fairness concerns has also led to their widespread acceptance as normative anchors of both international human rights law and anti-discrimination and equality statutes. In human rights law, interlocking principles of equality and non-discrimination are taken to be essential preconditions for the realisation of all human rights insofar as equality and non-discrimination are implied in the guarantee of the equal enjoyment and protection of fundamental rights and freedoms to all human beings per se.[@clifford2013] For this reason, principles of equality and non-discrimination are treated as jus cogens in human rights law\u2014i.e., they are treated as peremptory or foundational norms that permeate all human rights provisions and from which no derogation is permitted in any case.[@carozza2013]-[@clifford2013] In anti-discrimination and equality statutes in the UK and beyond, dovetailing priorities of equality and non-discrimination likewise form principal aims and essential underpinnings of fairness concerns. In this case, equality before the law manifests as equal protection from discriminatory harassment and from both direct and indirect kinds of discrimination (UK Equality Act, 2010). In discriminatory harassment, unwanted or abusive behaviour linked to a protected characteristic violates someone\u2019s dignity, degrades their identity, or creates an offensive environment for them. For example, an employer who makes a racist remark about a protected group in the presence of an employee from that racial background would be considered to have harassed that employee based on the protected characteristic of race. What are protected characteristics? In the 2010 UK Equality Act, protected classes include age, gender reassignment, being married or in a civil partnership, being pregnant or on maternity leave, disability, race including colour, nationality, ethnic or national origin, religion or belief, sex, and sexual orientation. The European Convention on Human Rights, which forms the basis of the UK\u2019s 1998 Human Rights Act, includes as protected characteristics \u2018sex, race, colour, language, religion, political or other opinion, national or social origin, association with a national minority, property, birth or other status'. Direct discrimination occurs when individuals are treated adversely based on their membership in some protected class. This type of discrimination is also known as \u2018disparate treatment\u2019 because it involves instances where otherwise similarly positioned individuals receive different and more-or-less favourable treatment on the basis of differences between their respective protected characteristics. For instance, direct discrimination would occur if an otherwise well-qualified job applicant were intentionally denied an opportunity for employment because of their age, disability, or sexual orientation. By contrast, indirect discrimination occurs when existing provisions, criteria, policies, arrangements, or practices\u2014which could appear on their face to be neutral\u2014disparately harm or unfairly disadvantage members of some protected class in comparison with others who are not members of that group. This type of discrimination is also known as \u2018disparate impact\u2019 because what matters here is not directly unfavourable treatment in individual cases but rather the broader disproportionate adverse effects of provisions, criteria, policies, arrangements, or practices that may subtly or implicitly disfavour members of some protected group while appearing to treat everyone equally. Indirect discrimination can therefore involve the impacts of tacitly unjust or unfair social structures, underlying inequalities, or systemic patterns of implicit historical bias that manifest unintentionally through prevailing norms, rules, policies, and behaviours. For instance, indirect discrimination would occur if a job advertisement specified that applicants needed to be native English speakers, for this would automatically disadvantage candidates of different nationalities regardless of their levels of fluency or language training. These three facets of anti-discrimination and equality law (harassment, direct discrimination, and indirect discrimination) have significantly shaped contemporary approaches to AI fairness.[@adams-prassl2022]-[@liu2018]-[@ntoutsi2019]-[@pessach2020]-[@watcher2021] Indeed, attempts to put the principle of AI fairness into practice have largely converged around the priority to do no discriminatory harm to affected people along each of these three vectors of potential injury. It is thus helpful to think about basic AI fairness considerations as tracking three corresponding questions (An example of an associated discriminatory harm is provided alongside each.): How could the use of the AI system we are planning to build or acquire\u2014or the policies, decisions, and processes behind its design, development, and deployment\u2014lead to the discriminatory harassment of impacted individuals (i.e., unwanted or abusive treatment of them which is linked to a protected characteristic and which violates their dignity, degrades their identity, or creates a humiliating or offensive environment for them)? ??? example \"Example\" An AI-enabled customer support chat bot is built from a large language model that has been pretrained on billions of data points scraped from the internet and then customised to provide tailored responses to customer questions about the provision of a public service. After the system goes live, it is soon discovered that, when certain customer names (which are indicative of protected classes) are entered, the chatbot generates racist and sexist text responses to customer inquiries. How could the use of the AI system we are planning to build or acquire\u2014or the policies, decisions, and processes behind its design, development, and deployment\u2014lead to the disproportionate adverse treatment of impacted individuals from protected groups on the basis of their protected characteristics? ??? example \"Example\" An AI system used to filter job applications in a recruitment process is trained on historic data that contains details about the characteristics of successful candidates over the past several years. Because white male applicants were predominantly hired over this time, the system learns to infer the likelihood of success based on proxy features connected to the protected characteristics of race and sex. It consequently filters out non-white and non-male job candidates from the applicant pool. How could the use of the AI system we are planning to build or acquire\u2014or the policies, choices, and processes behind its design, development, and deployment\u2014lead to indirect discrimination against impacted individuals from protected groups? ??? example \"Example\" An AI-enabled medical diagnosis tool is built as a smartphone application and made available to all participants in a national health system without sufficient considerations of the barriers to access faced by some citizens. It becomes clear after the app is launched that the device disproportionately favours younger, more digitally literate, and more affluent community members, while disadvantaging both the elderly, less digitally literate population and digitally deprived people who do not have access to smartphone technologies and internet connections. It is important to note, regarding this final question on indirect discrimination, that the Public Sector Equality Duty mandates considerations both of how to \u2018reduce the inequalities of outcome which result from socio-economic disadvantage\u2019 and of how to advance equality of opportunity and other substantive forms of equality. This means that our approach to putting the principle of AI fairness into practice must include social justice considerations that concentrate on how the production and use of AI technologies can address and rectify structural inequalities and institutionalised patterns of inequity and discrimination rather than reinforce or exacerbate them. We must consequently take a multi-pronged approach to AI fairness that integrates formal approaches to non-discrimination and equality (which focus primarily on consistent and impartial application of rules and equal treatment before the law) with more demanding substantive and transformative approaches (which focus on equalizing the distribution of opportunities and outcomes and on the fundamental importance of addressing the material pre-conditions and structural changes needed for the universal realisation of equitable social arrangements). What is social justice? Social justice is a commitment to the achievement of a society that is equitable, fair, and capable of confronting the root causes of injustice. In an equitable and fair society, all individuals are recognised as worthy of equal moral standing and are able to realise the full assemblage of fundamental rights, opportunities, and positions. In a socially just world, every person has access to the material means needed to participate fully in work life, social life, and creative life through the provision of proper education, adequate living and working conditions, general safety, social security, and other means of realising maximal health and well-being. Social justice also entails the advancement of diversity and participatory parity and a pluralistically informed recognition of identity and cultural difference. Struggles for social justice typically include accounting for historical and structural injustice coupled to demands for reparations and other means of restoring rights, opportunities, and resources to those who have been denied them or otherwise harmed.]","title":"Introduction to the principle of fairness"},{"location":"aeg/chapter4/fairness/#discriminatory-non-harm","text":"While there are different ways to characterise or define fairness in the design and use of AI systems, you should consider the principle of discriminatory non-harm as a minimum required threshold of fairness. This principle directs us to do no harm to others through direct or indirect discrimination or through discriminatory harassment linked to a protected characteristic that violates the dignity of impacted individuals, degrades their identity, or creates a humiliating or offensive environment for them: Key Concept: Principle of Discriminatory Non-Harm (Do No Discriminatory Harm) The producers and users of AI systems should prioritise the identification and mitigation of biases and discriminatory influences, which could lead to direct or indirect discrimination or discriminatory harassment. This entails an end-to-end focus on how unfair biases and discriminatory influences could arise (1) in the processes behind the design, development, and deployment of these systems, (2) in the outcomes produced by their implementation, and (3) in the wider economic, legal, cultural, and political structures or institutions in which the AI project lifecycle is embedded\u2014and in the policies, norms, and procedures through which these structures and institution influence actions and decisions throughout the broader AI innovation ecosystem. Developers and users of AI systems should, in this respect, acknowledge and address discriminatory patterns that may originate in the data used to train, test, and validate the system and in the model architectures (i.e., the variables, parameters, inferences, etc.) that generate system outputs. Beyond this, the principle of discriminatory non-harm implies that producers and users of AI systems should ensure that their research, innovation, and implementation practices are undertaken in an optimally responsible and ethical manner, more broadly, in keeping with the historical tendency that deficiencies in the deployment and operation of faulty systems often disparately impact protected, underrepresented, or disadvantaged groups. The principle of discriminatory non-harm applies to any AI system that processes social or demographic data (i.e., data pertaining to features of human subjects, population- and group-level traits and characteristics, or patterns of human activity and behaviour). However, the principle applies equally to AI systems that process bio-physical or biomedical data. In this case, imbalanced datasets, selection biases, or measurement errors could have discriminatory effects on impacted individuals and communities\u2014for instance, where a demographic group\u2019s lack of representation in a biomedical dataset (e.g., one used to train a diagnostic prediction model) means that the trained system performs poorly for that group relative to others that are better represented in the data. Prioritising discriminatory non-harm implies that the producers and users of AI systems ensure that the decisions and behaviours of their models neither treat impacted individuals adversely based on their membership in some protected class or socioeconomic group nor, intentionally or unintentionally, generate discriminatory or inequitable impacts on affected individuals and communities that unfairly disadvantage members of some protected class or socioeconomic group in comparison with others who are not members of that class or group. It can also be seen as a proportional approach to bias mitigation because it sets a baseline for fair AI systems, while, nevertheless, creating conditions for developers and users to strive towards an ideal for fair and equitable outcomes for all people as moral equals and as members of a just community where every person can regard themself as having intrinsic dignity and equal moral standing. Finally, the scope of the principle means that, beyond designers and users, any individuals, organisations, or departments who are procuring AI systems must ensure that the vendors of such systems can demonstrate the mitigation of potential biases and discriminatory influences in the processes behind their production and in their outputs.","title":"Discriminatory non-harm"},{"location":"aeg/chapter4/governance/","text":"Governance \u00b6 Putting accountability into practice \u00b6 Now that we have explored some of the main aspects of the concept of accountability, we are ready to examine\u2013in greater detail\u2013how elements of answerability and auditability can be put into practice. The central importance of the end-to-end operability of good governance practices should guide your strategy to embed accountability across the project workflow. Three components are essential to creating a such a workflow: Maintaining strong regimes of professional and institutional transparency. Establishing and maintaining a clear and accessible Process-Based Governance Framework (PBG Framework). Establishing a well-defined auditability trail for your PBG Framework through robust activity logging protocols that are consolidated digitally in a Process Log. Maintaining professional and institutional transparency \u00b6 At every stage of the design and implementation of your AI project, team members should be held to rigorous standards of conduct that secure and maintain professionalism and institutional transparency. These standards should include the core values of selflessness, integrity, honesty, accountability, openness, sincerity, neutrality, objectivity, impartiality, and leadership. Furthermore, from start to finish of the AI project lifecycle, the design, development, and deployment process should be as transparent and as open to public scrutiny as possible with restrictions on accessibility to relevant information limited to the reasonable protection of justified confidentiality and of analytics that may tip off bad actors to methods of gaming the system of service provision or otherwise taking advantage of their insight to the detriment of the system's performance or the rest of the users. Process-based governance framework \u00b6 We have looked at some of the most important values and princciples necessary for establishing responsible innovation practices in the AI project lifecycle. Perhaps the most vital of these measures is the effective operationalisation of these practices.The recently-adopted standard, ISO 37000, defines governance as \u2018the system by which the whole organisation is directed, controlled, and held accountable to achieve its core purpose in the long run\u2019. Establishing a diligent and well-conceived governance framework that covers the entire design, development, and deployment process will provide the foundation for effectively establishing needed practical actions and controls, exhaustively distributing roles and responsibilities, and operationalising answerability and auditability throughout the AI lifecycle. Organising all of the governance actions into a PBG Framework is a way to better accomplish this task. The purpose of a PBG Framework is to provide a template for the integrations of the norms, values, and principles, which motivate and steer responsible innovation, with the actual processes that characterise the AI design and development pipeline. Establishing a PBG framework creates the baseline conditions for ensuring that the goal of instituting an AI innovation process that is accountable-by-design is achieved. A PBG Framework should give the team a landscape view of the governance actions that are organising the control structures of the project workflow. Constructing a good PBG Framework will provide the team with a big picture of: \u2022 The relevant stages of the workflow in which actions are necessary to meet governance goals \u2022 The relevant team members and roles involved in each governance action \u2022 Explicit timeframes for any necessary follow-up actions, re-assessments, and continual monitoring \u2022 Clear and well-defined protocols for logging activity and for instituting mechanisms to assure end-to-end auditability and appropriate documentation The PBG framework asks that teams not only outline the governance actions established for individual projects, but also roles involved in each action, timeframes for follow-up actions, and logging protocols. Establishing proportional governance actions \u00b6 Just as with the determination of proportionate stakeholder involvement, the establishment of proportionate governance protocols should involve a preliminary assessment of the potential risks and hazards of the model or tool under consideration. Low-stakes AI applications that are not safety critical, do not directly impact the lives of people, and do not process potentially sensitive social and demographic data may need less proactive governance controls and processes than high-stakes projects. By completing the Project Summary Report and Stakeholder Impact Assessments, the project team will need to carry out evaluations of the scope of the possible risks that could arise from the project and of the potential hazards it poses to affected individuals and groups. These assessments of the dangers posed to individual wellbeing and public welfare will help formulate proportionate governance actions to be outlined in the PBG framework. Notwithstanding the importance of the need for this reasonable application of proportionate governance actions, a strong regime of accountability-by-design across the project lifecycle should nonetheless be established. It may be the case that the assessment of potential risks and adverse impacts does not sufficiently anticipate the full range of possible harms. In instances where such unforeseen harms do arise, proper mechanisms of anticipatory accountability and corresponding documentation protocols should already be in place, so that the best practices of the project team are demonstrable. Here is a summary picture of where all possible governance actions fit across the project workflow: Each principle can be operationalised through specific processes: Sustainability Stakeholder engagement process (SEP) Process facilitating a contextually informed understanding of the social environment and human factors that may be impacted by, or may impact, individual AI projects, and the uptake of proportionate stakeholder engagement and input throughout the AI lifecycle. Stakeholder impact assessment (SIA) Process facilitating the iterative evaluation of the social impact and sustainability of individual AI projects, as well as the corroboration of these potential impacts in dialogue with stakeholders, when appropriate. Safety SSA & RA (Safety Self-Assessment and Risk Management) Process facilitating the evaluation of how AI projects align with safety objectives through the iterative identification and documentation of risks of potential safety risks across the lifecycle, and assurance actions implemented to address these. Responsible data management Data factsheet Live document facilitating the uptake of best practices for Responsible Data Management and Stewardship across the AI project workflow by facilitating the documentation of a comprehensive record of the data lineage iterative assessments of data integrity, quality, protection, and privacy. Fairness BSA & RM (Bias self-assessment & risk management) Process facilitating the evaluation of how AI projects align with the principle of fairness through the iterative identification and documentation of risks of bias across the lifecycle, and assurance actions implemented to address these. Fairness position statement Document establishing the metric-based fairness criteria for individual AI projects, providing an explanation in plain and nontechnical language. Accountability PBG Framework Live document outlining governance actions, relevant team members and roles involved in each action, timeframes for follow-up actions, and logging protocols, for individual AI projects. Explainability Transparency & EAM (explainability assurance management) Iterative process aimed to facilitate the implementation and evaluation of transparency and explainability assurance activities across the project lifecycle and assist in providing clarification of AI system outputs to a range of impacted stakeholders. Accountability across the workflow \u00b6 The task of establishing a PBG framework for the project should be initially undertaken in the project planning step of the project alongside the Project Summary Report The results of the Stakeholder Analysis (particularly, the scoping of potential stakeholder impacts) should inform a proportional selection of governance actions within the PBG framework. At this stage, the PBG framework will provide a prospective and provisional plotting of governance actions, roles, and responsibilities for the project. This preliminary outline of governance structures will provide the necessary information for answering the Governance Framework Reflection questions within the PS Report. In the PS Report, the task of reflecting on your governance framework on your involves answering the following questions: Do established governance actions proportionally mitigate possible harms to stakeholders posed by this project? If not, how can your PBG framework be rectified to address these potential harms? Does this distribution of responsibilities outlined in the PBG Framework establish a continuous chain of human accountability throughout the design, development, and deployment of this project? If not, how can any identified gaps or breaks in the chain be rectified in the PBG Framework? How will you ensure that all team members, who are assigned roles/responsibilities understand the roles/responsibilities that have been assigned to them? If you are procuring parts or elements of the system from third-party vendors, suppliers, sub-contractors, or external developers, how are you instituting appropriate governance controls that will establish end-to-end accountability, traceability, and auditability for these procured parts or elements? If any data being used in the production of the AI system will be acquired from a vendor, supplier, or third party, how are you instituting appropriate governance controls that will establish end-to-end accountability, answerability, and auditability across the data lifecycle? These questions (alongside the rest of the PS Report) are to be revisited and updated as part of completing each iteration of the Stakeholder Impact Assessment, at each point informing any necessary updates to the project\u2019s governance structure (and PBG framework). The PBG Framework is therefore a live document reflecting a governance structure that responds to the emerging needs across the design, development, and deployment lifecycle. It is to be updated after each revisitation of the PS report to reflect the project\u2019s current governance structure. The process by which these questions are answered should be as collaborative and inclusive as possible. The aim is to involve all relevant members of the project team (and any other relevant managers, operators, or vendors), so that all people involved in the workflow can share input and come to understand expectations about their roles and responsibilities. Any future revisions or updates of this part of the PS Report should likewise include all affected parties.","title":"AI Governance"},{"location":"aeg/chapter4/governance/#governance","text":"","title":"Governance"},{"location":"aeg/chapter4/governance/#putting-accountability-into-practice","text":"Now that we have explored some of the main aspects of the concept of accountability, we are ready to examine\u2013in greater detail\u2013how elements of answerability and auditability can be put into practice. The central importance of the end-to-end operability of good governance practices should guide your strategy to embed accountability across the project workflow. Three components are essential to creating a such a workflow: Maintaining strong regimes of professional and institutional transparency. Establishing and maintaining a clear and accessible Process-Based Governance Framework (PBG Framework). Establishing a well-defined auditability trail for your PBG Framework through robust activity logging protocols that are consolidated digitally in a Process Log.","title":"Putting accountability into practice"},{"location":"aeg/chapter4/governance/#maintaining-professional-and-institutional-transparency","text":"At every stage of the design and implementation of your AI project, team members should be held to rigorous standards of conduct that secure and maintain professionalism and institutional transparency. These standards should include the core values of selflessness, integrity, honesty, accountability, openness, sincerity, neutrality, objectivity, impartiality, and leadership. Furthermore, from start to finish of the AI project lifecycle, the design, development, and deployment process should be as transparent and as open to public scrutiny as possible with restrictions on accessibility to relevant information limited to the reasonable protection of justified confidentiality and of analytics that may tip off bad actors to methods of gaming the system of service provision or otherwise taking advantage of their insight to the detriment of the system's performance or the rest of the users.","title":"Maintaining professional and institutional transparency"},{"location":"aeg/chapter4/governance/#process-based-governance-framework","text":"We have looked at some of the most important values and princciples necessary for establishing responsible innovation practices in the AI project lifecycle. Perhaps the most vital of these measures is the effective operationalisation of these practices.The recently-adopted standard, ISO 37000, defines governance as \u2018the system by which the whole organisation is directed, controlled, and held accountable to achieve its core purpose in the long run\u2019. Establishing a diligent and well-conceived governance framework that covers the entire design, development, and deployment process will provide the foundation for effectively establishing needed practical actions and controls, exhaustively distributing roles and responsibilities, and operationalising answerability and auditability throughout the AI lifecycle. Organising all of the governance actions into a PBG Framework is a way to better accomplish this task. The purpose of a PBG Framework is to provide a template for the integrations of the norms, values, and principles, which motivate and steer responsible innovation, with the actual processes that characterise the AI design and development pipeline. Establishing a PBG framework creates the baseline conditions for ensuring that the goal of instituting an AI innovation process that is accountable-by-design is achieved. A PBG Framework should give the team a landscape view of the governance actions that are organising the control structures of the project workflow. Constructing a good PBG Framework will provide the team with a big picture of: \u2022 The relevant stages of the workflow in which actions are necessary to meet governance goals \u2022 The relevant team members and roles involved in each governance action \u2022 Explicit timeframes for any necessary follow-up actions, re-assessments, and continual monitoring \u2022 Clear and well-defined protocols for logging activity and for instituting mechanisms to assure end-to-end auditability and appropriate documentation The PBG framework asks that teams not only outline the governance actions established for individual projects, but also roles involved in each action, timeframes for follow-up actions, and logging protocols.","title":"Process-based governance framework"},{"location":"aeg/chapter4/governance/#establishing-proportional-governance-actions","text":"Just as with the determination of proportionate stakeholder involvement, the establishment of proportionate governance protocols should involve a preliminary assessment of the potential risks and hazards of the model or tool under consideration. Low-stakes AI applications that are not safety critical, do not directly impact the lives of people, and do not process potentially sensitive social and demographic data may need less proactive governance controls and processes than high-stakes projects. By completing the Project Summary Report and Stakeholder Impact Assessments, the project team will need to carry out evaluations of the scope of the possible risks that could arise from the project and of the potential hazards it poses to affected individuals and groups. These assessments of the dangers posed to individual wellbeing and public welfare will help formulate proportionate governance actions to be outlined in the PBG framework. Notwithstanding the importance of the need for this reasonable application of proportionate governance actions, a strong regime of accountability-by-design across the project lifecycle should nonetheless be established. It may be the case that the assessment of potential risks and adverse impacts does not sufficiently anticipate the full range of possible harms. In instances where such unforeseen harms do arise, proper mechanisms of anticipatory accountability and corresponding documentation protocols should already be in place, so that the best practices of the project team are demonstrable. Here is a summary picture of where all possible governance actions fit across the project workflow: Each principle can be operationalised through specific processes: Sustainability Stakeholder engagement process (SEP) Process facilitating a contextually informed understanding of the social environment and human factors that may be impacted by, or may impact, individual AI projects, and the uptake of proportionate stakeholder engagement and input throughout the AI lifecycle. Stakeholder impact assessment (SIA) Process facilitating the iterative evaluation of the social impact and sustainability of individual AI projects, as well as the corroboration of these potential impacts in dialogue with stakeholders, when appropriate. Safety SSA & RA (Safety Self-Assessment and Risk Management) Process facilitating the evaluation of how AI projects align with safety objectives through the iterative identification and documentation of risks of potential safety risks across the lifecycle, and assurance actions implemented to address these. Responsible data management Data factsheet Live document facilitating the uptake of best practices for Responsible Data Management and Stewardship across the AI project workflow by facilitating the documentation of a comprehensive record of the data lineage iterative assessments of data integrity, quality, protection, and privacy. Fairness BSA & RM (Bias self-assessment & risk management) Process facilitating the evaluation of how AI projects align with the principle of fairness through the iterative identification and documentation of risks of bias across the lifecycle, and assurance actions implemented to address these. Fairness position statement Document establishing the metric-based fairness criteria for individual AI projects, providing an explanation in plain and nontechnical language. Accountability PBG Framework Live document outlining governance actions, relevant team members and roles involved in each action, timeframes for follow-up actions, and logging protocols, for individual AI projects. Explainability Transparency & EAM (explainability assurance management) Iterative process aimed to facilitate the implementation and evaluation of transparency and explainability assurance activities across the project lifecycle and assist in providing clarification of AI system outputs to a range of impacted stakeholders.","title":"Establishing proportional governance actions"},{"location":"aeg/chapter4/governance/#accountability-across-the-workflow","text":"The task of establishing a PBG framework for the project should be initially undertaken in the project planning step of the project alongside the Project Summary Report The results of the Stakeholder Analysis (particularly, the scoping of potential stakeholder impacts) should inform a proportional selection of governance actions within the PBG framework. At this stage, the PBG framework will provide a prospective and provisional plotting of governance actions, roles, and responsibilities for the project. This preliminary outline of governance structures will provide the necessary information for answering the Governance Framework Reflection questions within the PS Report. In the PS Report, the task of reflecting on your governance framework on your involves answering the following questions: Do established governance actions proportionally mitigate possible harms to stakeholders posed by this project? If not, how can your PBG framework be rectified to address these potential harms? Does this distribution of responsibilities outlined in the PBG Framework establish a continuous chain of human accountability throughout the design, development, and deployment of this project? If not, how can any identified gaps or breaks in the chain be rectified in the PBG Framework? How will you ensure that all team members, who are assigned roles/responsibilities understand the roles/responsibilities that have been assigned to them? If you are procuring parts or elements of the system from third-party vendors, suppliers, sub-contractors, or external developers, how are you instituting appropriate governance controls that will establish end-to-end accountability, traceability, and auditability for these procured parts or elements? If any data being used in the production of the AI system will be acquired from a vendor, supplier, or third party, how are you instituting appropriate governance controls that will establish end-to-end accountability, answerability, and auditability across the data lifecycle? These questions (alongside the rest of the PS Report) are to be revisited and updated as part of completing each iteration of the Stakeholder Impact Assessment, at each point informing any necessary updates to the project\u2019s governance structure (and PBG framework). The PBG Framework is therefore a live document reflecting a governance structure that responds to the emerging needs across the design, development, and deployment lifecycle. It is to be updated after each revisitation of the PS report to reflect the project\u2019s current governance structure. The process by which these questions are answered should be as collaborative and inclusive as possible. The aim is to involve all relevant members of the project team (and any other relevant managers, operators, or vendors), so that all people involved in the workflow can share input and come to understand expectations about their roles and responsibilities. Any future revisions or updates of this part of the PS Report should likewise include all affected parties.","title":"Accountability across the workflow"},{"location":"aeg/chapter5/","text":"Transparency, Explainability, and CARE & ACT Principles \u00b6 Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk)) Chapter Outline \u00b6 Transparency and Explainability Consider context Anticipate impacts Reflect on Purpose, Positionality, and Power Engage inclusively Act Responsibly Chapter Summary The last chapter of the course is divided into two sections. First, we will delve into the concepts of transparency and explainability in AI, looking at the difference between the two, and the various types of AI explanations. In the second half of the chapter we will bring everything back together with a review of the CARE & ACT principles. These principles serve as a practical tool to ensure AI systems are developed in an ethical and responsible manner. They are the following: Consider context Anticipate impacts Reflect on purpose, positionality, and power Engage inclusively Act transparently and responsibly Learning Objectives Familiarise yourself with the concepts of transparency and explainability in the context of AI systems and what the difference between them is. Understand the difference between process-based and outcome-based explanations. Learn about the different types of explanations that may be required in terms of AI-assisted decisions, and what is required of each type. Familiarise yourself with the CARE & ACT principles, and how they can be used as a practical tool for thinking about ethical and responsible design and development of AI systems.","title":"Introduction"},{"location":"aeg/chapter5/#transparency-explainability-and-care-act-principles","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))","title":"Transparency, Explainability, and CARE &amp; ACT Principles"},{"location":"aeg/chapter5/#chapter-outline","text":"Transparency and Explainability Consider context Anticipate impacts Reflect on Purpose, Positionality, and Power Engage inclusively Act Responsibly Chapter Summary The last chapter of the course is divided into two sections. First, we will delve into the concepts of transparency and explainability in AI, looking at the difference between the two, and the various types of AI explanations. In the second half of the chapter we will bring everything back together with a review of the CARE & ACT principles. These principles serve as a practical tool to ensure AI systems are developed in an ethical and responsible manner. They are the following: Consider context Anticipate impacts Reflect on purpose, positionality, and power Engage inclusively Act transparently and responsibly Learning Objectives Familiarise yourself with the concepts of transparency and explainability in the context of AI systems and what the difference between them is. Understand the difference between process-based and outcome-based explanations. Learn about the different types of explanations that may be required in terms of AI-assisted decisions, and what is required of each type. Familiarise yourself with the CARE & ACT principles, and how they can be used as a practical tool for thinking about ethical and responsible design and development of AI systems.","title":"Chapter Outline"},{"location":"aeg/chapter5/act/","text":"Act transparently and responsibly \u00b6 The imperative of acting transparently and responsibly is a call to all AI and data science researchers, developers, deployers and users to marshal the habits of responsible research and innovation cultivated in the CARE processes to produce systems that prioritise data stewardship and that are robust, accountable, fair, non-discriminatory, explainable, reproducible, and replicable. While the mechanisms and procedures which are put in place to ensure that these normative goals are achieved will differ from project to project we can summarise the following priorities that should be incorporated into a team's governance, self-assessment, and reporting practices: Full documentation of data provenance, lineage, linkage, and sourcing: This involves keeping track of and documenting both responsible data management practices across the entire project lifecycle, from data extraction or procurement and data analysis, cleaning, and pre-processing to data use, retention, deletion, and updating.[@bender2018]-[@gebru2021]-[@holland2018] It also involves demonstrating that the data is ethically sourced, responsibly linked, and legally available for the project's purposes[@weinhardt2020] and making explicit measures taken to ensure data quality (source integrity and measurement accuracy, timeliness and recency, relevance, sufficiency of quantity, dataset representativeness), data integrity (attributability, consistency, completeness, contemporareousness, traceability, and auditability) and FAIR data (findable, accessible, interoperable, and reusable). Full documentation of privacy, confidentiality, consent, and data protection due diligence. This involves demonstrating that data has been handled securely and responsibly from beginning to end of the project's lifecycle so that any potential breaches of confidentiality, privacy, and anonymity have been prevented and any risks of re-identification through triangulation and data linkage mitigated. Regardless of the jurisdictions of data collection and use, the rights and interests of data subjects should always aimed to be optimally protected by adhering to the highest standards of privacy preservation, data protection, and responsible data handling and storage. Transparent and accountable reporting of processes and results and appropriate publicity of datasets. This involves that results of model design, development, and deployment should be carried out in a way that enables the interpretability, reproducibility, and replicability of the results. Research design, analysis, and reporting should be pursued in an interpretability-aware manner that prioritises process transparency, the understandability of models, and the accessibility and explainability of the rationale behind their results. An end-to-end process for bias self-assessment. This should cover all research stages as well as all sources of biases that could arise in the data, in the data collection, in the data pre-processing, in the organising, categorising, describing, annotating, structuring of data (text-as-data, in particular), and in research design and execution choices.","title":"Act Responsibly"},{"location":"aeg/chapter5/act/#act-transparently-and-responsibly","text":"The imperative of acting transparently and responsibly is a call to all AI and data science researchers, developers, deployers and users to marshal the habits of responsible research and innovation cultivated in the CARE processes to produce systems that prioritise data stewardship and that are robust, accountable, fair, non-discriminatory, explainable, reproducible, and replicable. While the mechanisms and procedures which are put in place to ensure that these normative goals are achieved will differ from project to project we can summarise the following priorities that should be incorporated into a team's governance, self-assessment, and reporting practices: Full documentation of data provenance, lineage, linkage, and sourcing: This involves keeping track of and documenting both responsible data management practices across the entire project lifecycle, from data extraction or procurement and data analysis, cleaning, and pre-processing to data use, retention, deletion, and updating.[@bender2018]-[@gebru2021]-[@holland2018] It also involves demonstrating that the data is ethically sourced, responsibly linked, and legally available for the project's purposes[@weinhardt2020] and making explicit measures taken to ensure data quality (source integrity and measurement accuracy, timeliness and recency, relevance, sufficiency of quantity, dataset representativeness), data integrity (attributability, consistency, completeness, contemporareousness, traceability, and auditability) and FAIR data (findable, accessible, interoperable, and reusable). Full documentation of privacy, confidentiality, consent, and data protection due diligence. This involves demonstrating that data has been handled securely and responsibly from beginning to end of the project's lifecycle so that any potential breaches of confidentiality, privacy, and anonymity have been prevented and any risks of re-identification through triangulation and data linkage mitigated. Regardless of the jurisdictions of data collection and use, the rights and interests of data subjects should always aimed to be optimally protected by adhering to the highest standards of privacy preservation, data protection, and responsible data handling and storage. Transparent and accountable reporting of processes and results and appropriate publicity of datasets. This involves that results of model design, development, and deployment should be carried out in a way that enables the interpretability, reproducibility, and replicability of the results. Research design, analysis, and reporting should be pursued in an interpretability-aware manner that prioritises process transparency, the understandability of models, and the accessibility and explainability of the rationale behind their results. An end-to-end process for bias self-assessment. This should cover all research stages as well as all sources of biases that could arise in the data, in the data collection, in the data pre-processing, in the organising, categorising, describing, annotating, structuring of data (text-as-data, in particular), and in research design and execution choices.","title":"Act transparently and responsibly"},{"location":"aeg/chapter5/anticipate/","text":"Anticipate impacts \u00b6 Anticipating impacts of an AI system involves reflecting on and assessing the potential short-term and long-term effects the system may have on impacted individuals and on affected communities and social groups, more broadly. Why is this kind of anticipatory reflection important? Its purpose is to safeguard the sustainability of AI projects across the entire project lifecycle instead of taking an approach of dealing with issues as they appear. There is no guarantee that a team will be able to anticipate all potential impacts, but dealing with the most relevant ones before they become a problem ensures more sustainable systems overall (it is also a much more efficient use of resources over time). How does one ensure that the activities and outputs of the AI system are socially and environmentally sustainable? Project team members must proceed with a continuous responsiveness to the real-world impacts that their system could have. The way to translate into practice as we have seen, is through concerted and stakeholder-involving exploration of the possible adverse and beneficial effects that could otherwise remain hidden from view if deliberate and structured processes for anticipating downstream impacts were not in place. Attending to sustainability, along these lines, also entails the iterative re-visitation and re-evaluation of impact assessments. To be sure, in its general usage, the word \u201csustainability\u201d refers to the maintenance of and care for an object or endeavour over time. In the context of AI, this implies that building sustainability into a project is not a \u201cone-off\u201d affair. Rather, carrying out an initial impact assessment at the inception of a project is only a first, albeit critical, step in a much longer, end-to-end process of responsive re-evaluation and re-assessment. Such an iterative approach ensures that continuous attention is payed both to the dynamic and changing character of the project lifecycle and to the shifting conditions of the real-world environments in which studies are embedded. Methodical impact evaluation should involve an initial adoption of normative criteria that function as metrics for scoping and assessing the possible harms and benefits of the research and its outputs. Taking GPAI\u2019s \u201c12 Principles and Priorities of Responsible Data Innovation\u201d as an example, relevant impact assessment questions could include: How, if at all, could our research and its outputs impact each of the following twelve principles and priorities as they relate to all affected stakeholders, especially those who are vulnerable, marginalised, or historically discriminated against? (Affected stakeholders include research subjects and participants, subjects of data collected for or used in the study, researchers, and all other impacted people and social groups.) 12 Principles and Priorities of Responsible Data Innovation Respect for and protection of human dignity Interconnectivity, solidarity, and intergenerational reciprocity Environmental flourishing, sustainability, and the rights of the biosphere Protection of human freedom and autonomy Prevention of harm and protection of the right to life and physical, psychological, and moral integrity Non-discrimination, fairness, and equality Rights of Indigenous peoples and Indigenous data sovereignty Data protection and the right to respect of private and family life Economic and social rights Accountability and effective remedy Democracy Rule of law How could our research and its outputs advance each of these twelve principles and priorities or hinder their realisation? Are there particular stakeholder groups who could disproportionately enjoy the benefits of the research and its outputs, or suffer from the potential harms they generate, as these harms and benefits relate to each of the twelve principles and priorities? If things go wrong in our research or if its outputs (especially tools produced or capacities enabled) are used out-of-the-scope of their intended purpose and function, what harms could be done to stakeholders in relation to each of the twelve principles and priorities? It is important to note here that stakeholder involvement in the mpact assessment process can be a critical safeguard against evaluative blind spots and omissions. Methodical impact evaluation should also involve an assessment of the severity of potential adverse impacts. This brings clarity to the prioritisation of impact mitigation actions by allowing the severity levels of potential negative effects to be differentiated, elucidated, and refined. As explained in the United Nations Guiding Principles on Business and Human Rights (UNGP), assessing the severity of potential negative impacts on fundamental rights and freedoms involves consideration of their scale, scope, and remediability, where scale is defined as \u201cthe gravity or seriousness of the impact,\u201d scope as \u201chow widespread the impact is, or the numbers of people impacted,\u201d and remediability, as the \u201cability to restore those affected to a situation at least the same as, or equivalent to, their situation before the impact\u201d (UNGP, 2011, Principle 14). One notable challenge faced by researchers who are assessing the severity of potential adverse impacts is identifying cumulative or aggregate downstream impacts, which can be much more difficult than identifying harms directly or proximately caused by a project. Discerning these impacts may require additional research and consultation with domain experts and other relevant stakeholders. This difficulty results from the fact that cumulative impacts are often incremental and more difficult to perceive, and they frequently involve complex contexts of multiple actors or projects operating in the same area or sector or affecting the same populations.[@gotzmann2020] Some \u201cbig picture\u201d questions to reflect on when assessing cumulative or aggregate impacts include: Could the project contribute to wider scale adverse impacts when its deployment is coordinated with (or occurs in tandem with) other projects or innovation activity that serve similar functions or purposes? For example, if the impacts of a project that aims to discover an effective method of behavioural nudging at scale are considered in combination with the proliferation of many other similar projects or computational systems in a given sector, concerns about wider cumulative effects like mass manipulation, objectification, and infringement on autonomy and human dignity become relevant. Could the project replicate, reinforce, or augment socio-historically entrenched legacy harms that create knock-on effects in impacted individuals and groups? For example, if a project analyses sensitive personal information contained in databases scraped from social media websites without gaining the proper consent of research subject in accordance with their reasonable expectations, it could add to the legacy harms of companies that have used data recklessly and eroded public trust regarding the respect of privacy and data protection rights in the digital sphere. This can create wider chilling effects on elements of open communication, information sharing, and interpersonal connection that are essential components for the sustainability of democratic forms of life. Could the production and use of the system be understood to contribute to wider aggregate adverse impacts on the biosphere and on planetary health when its deployment is considered in combination with other systems that may have similar environmental impacts? For example, a project that involves moderate levels of energy consumption in model training or data storage may be seen to contribute to significant environmental impact when considered alongside the energy consumption of similar projects across research ecosystems. Once impacts have been evaluated and the severity of any potential harms assessed, impact prevention and mitigation planning should commence. Diligent impact mitigation planning begins with a scoping and prioritization stage. Team members (and engaged stakeholders, where appropriate) should go through all the identified potential adverse impacts and map out the interrelations and interdependencies between them as well as surrounding social factors (such as contextually specific stakeholder vulnerabilities and precariousness) that could make impact mitigation more challenging. Where prioritization of prevention and mitigation actions is necessary (for instance, where delays in addressing a potential harm could reduce its remediability), decision-making should be steered by the relative severity of the impacts under consideration. As a general rule, while impact prevention and mitigation planning may involve prioritization of actions, all potential adverse impacts must be addressed. When potential adverse impacts have been mapped out and organised, and mitigation actions have been considered, the research team (and engaged stakeholders, where appropriate) should begin co-designing an impact mitigation plan (IMP). The IMP will become the part of your transparent reporting methodology that specifies the actions and processes needed to address the adverse impacts which have been identified and that assigns responsibility for the completions of these tasks and processes. As such, the IMP will serve a crucial documenting function. Establishment of protocols for re-visitation and re-evaluation of the research impact assessment: Impact assessments must pay continuous attention both to the dynamic and changing character of the project lifecycle and to the shifting conditions of the real-world environments in which research practices, results, and outputs are embedded. There are two sets of factors that should inform when and how often initial impact assessments are re-visited to ensure that they remain adequately responsive to factors that could present new potential harms or significantly influence impacts that have been previously identified: Lifecycle and production factors: Choices made at any point along the workflow may affect the veracity of prior impact assessments\u2014leading to a need for re-assessment, reconsideration, and amendment. For instance, design choices could be made that were not anticipated in the initial impact assessment (such choices might include adjusting the variables that are included in the model, choosing more complex algorithms, or grouping variables in ways that may impact specific groups). These changes may influence how a computational model performs, how it is explained, or how it impacts affected individuals and groups. Processes are also iterative and frequently bi-directional, and this often results in the need for revision and update. For these reasons, impact assessments must remain agile, attentive to change, and at-the-ready to evaluatively move back and forth across the decision-making pipeline as downstream actions affect upstream choices and evaluations. Environmental factors: Changes in project-relevant social, regulatory, policy or legal environments (occurring during the time in which the research is taking place) may have a bearing on how well the resulting computational model works and on how the research outputs impact affected individuals and groups. Likewise, domain-level reforms, policy changes, or changes in data recording methods may take place in the population of concern in ways that affect whether the data used to train the model accurately portrays phenomena, populations, or related factors in an accurate manner. In the same vein, cultural or behavioral shifts may occur within affected populations that alter the underlying data distribution and hamper the predictive and explanatory efficacy of a model, which has been trained on data collected prior to such shifts. All of these alterations of environmental conditions can have a significant effect on how research practices, outputs, and results impact affected individual and communities.","title":"Anticipate Impacts"},{"location":"aeg/chapter5/anticipate/#anticipate-impacts","text":"Anticipating impacts of an AI system involves reflecting on and assessing the potential short-term and long-term effects the system may have on impacted individuals and on affected communities and social groups, more broadly. Why is this kind of anticipatory reflection important? Its purpose is to safeguard the sustainability of AI projects across the entire project lifecycle instead of taking an approach of dealing with issues as they appear. There is no guarantee that a team will be able to anticipate all potential impacts, but dealing with the most relevant ones before they become a problem ensures more sustainable systems overall (it is also a much more efficient use of resources over time). How does one ensure that the activities and outputs of the AI system are socially and environmentally sustainable? Project team members must proceed with a continuous responsiveness to the real-world impacts that their system could have. The way to translate into practice as we have seen, is through concerted and stakeholder-involving exploration of the possible adverse and beneficial effects that could otherwise remain hidden from view if deliberate and structured processes for anticipating downstream impacts were not in place. Attending to sustainability, along these lines, also entails the iterative re-visitation and re-evaluation of impact assessments. To be sure, in its general usage, the word \u201csustainability\u201d refers to the maintenance of and care for an object or endeavour over time. In the context of AI, this implies that building sustainability into a project is not a \u201cone-off\u201d affair. Rather, carrying out an initial impact assessment at the inception of a project is only a first, albeit critical, step in a much longer, end-to-end process of responsive re-evaluation and re-assessment. Such an iterative approach ensures that continuous attention is payed both to the dynamic and changing character of the project lifecycle and to the shifting conditions of the real-world environments in which studies are embedded. Methodical impact evaluation should involve an initial adoption of normative criteria that function as metrics for scoping and assessing the possible harms and benefits of the research and its outputs. Taking GPAI\u2019s \u201c12 Principles and Priorities of Responsible Data Innovation\u201d as an example, relevant impact assessment questions could include: How, if at all, could our research and its outputs impact each of the following twelve principles and priorities as they relate to all affected stakeholders, especially those who are vulnerable, marginalised, or historically discriminated against? (Affected stakeholders include research subjects and participants, subjects of data collected for or used in the study, researchers, and all other impacted people and social groups.) 12 Principles and Priorities of Responsible Data Innovation Respect for and protection of human dignity Interconnectivity, solidarity, and intergenerational reciprocity Environmental flourishing, sustainability, and the rights of the biosphere Protection of human freedom and autonomy Prevention of harm and protection of the right to life and physical, psychological, and moral integrity Non-discrimination, fairness, and equality Rights of Indigenous peoples and Indigenous data sovereignty Data protection and the right to respect of private and family life Economic and social rights Accountability and effective remedy Democracy Rule of law How could our research and its outputs advance each of these twelve principles and priorities or hinder their realisation? Are there particular stakeholder groups who could disproportionately enjoy the benefits of the research and its outputs, or suffer from the potential harms they generate, as these harms and benefits relate to each of the twelve principles and priorities? If things go wrong in our research or if its outputs (especially tools produced or capacities enabled) are used out-of-the-scope of their intended purpose and function, what harms could be done to stakeholders in relation to each of the twelve principles and priorities? It is important to note here that stakeholder involvement in the mpact assessment process can be a critical safeguard against evaluative blind spots and omissions. Methodical impact evaluation should also involve an assessment of the severity of potential adverse impacts. This brings clarity to the prioritisation of impact mitigation actions by allowing the severity levels of potential negative effects to be differentiated, elucidated, and refined. As explained in the United Nations Guiding Principles on Business and Human Rights (UNGP), assessing the severity of potential negative impacts on fundamental rights and freedoms involves consideration of their scale, scope, and remediability, where scale is defined as \u201cthe gravity or seriousness of the impact,\u201d scope as \u201chow widespread the impact is, or the numbers of people impacted,\u201d and remediability, as the \u201cability to restore those affected to a situation at least the same as, or equivalent to, their situation before the impact\u201d (UNGP, 2011, Principle 14). One notable challenge faced by researchers who are assessing the severity of potential adverse impacts is identifying cumulative or aggregate downstream impacts, which can be much more difficult than identifying harms directly or proximately caused by a project. Discerning these impacts may require additional research and consultation with domain experts and other relevant stakeholders. This difficulty results from the fact that cumulative impacts are often incremental and more difficult to perceive, and they frequently involve complex contexts of multiple actors or projects operating in the same area or sector or affecting the same populations.[@gotzmann2020] Some \u201cbig picture\u201d questions to reflect on when assessing cumulative or aggregate impacts include: Could the project contribute to wider scale adverse impacts when its deployment is coordinated with (or occurs in tandem with) other projects or innovation activity that serve similar functions or purposes? For example, if the impacts of a project that aims to discover an effective method of behavioural nudging at scale are considered in combination with the proliferation of many other similar projects or computational systems in a given sector, concerns about wider cumulative effects like mass manipulation, objectification, and infringement on autonomy and human dignity become relevant. Could the project replicate, reinforce, or augment socio-historically entrenched legacy harms that create knock-on effects in impacted individuals and groups? For example, if a project analyses sensitive personal information contained in databases scraped from social media websites without gaining the proper consent of research subject in accordance with their reasonable expectations, it could add to the legacy harms of companies that have used data recklessly and eroded public trust regarding the respect of privacy and data protection rights in the digital sphere. This can create wider chilling effects on elements of open communication, information sharing, and interpersonal connection that are essential components for the sustainability of democratic forms of life. Could the production and use of the system be understood to contribute to wider aggregate adverse impacts on the biosphere and on planetary health when its deployment is considered in combination with other systems that may have similar environmental impacts? For example, a project that involves moderate levels of energy consumption in model training or data storage may be seen to contribute to significant environmental impact when considered alongside the energy consumption of similar projects across research ecosystems. Once impacts have been evaluated and the severity of any potential harms assessed, impact prevention and mitigation planning should commence. Diligent impact mitigation planning begins with a scoping and prioritization stage. Team members (and engaged stakeholders, where appropriate) should go through all the identified potential adverse impacts and map out the interrelations and interdependencies between them as well as surrounding social factors (such as contextually specific stakeholder vulnerabilities and precariousness) that could make impact mitigation more challenging. Where prioritization of prevention and mitigation actions is necessary (for instance, where delays in addressing a potential harm could reduce its remediability), decision-making should be steered by the relative severity of the impacts under consideration. As a general rule, while impact prevention and mitigation planning may involve prioritization of actions, all potential adverse impacts must be addressed. When potential adverse impacts have been mapped out and organised, and mitigation actions have been considered, the research team (and engaged stakeholders, where appropriate) should begin co-designing an impact mitigation plan (IMP). The IMP will become the part of your transparent reporting methodology that specifies the actions and processes needed to address the adverse impacts which have been identified and that assigns responsibility for the completions of these tasks and processes. As such, the IMP will serve a crucial documenting function. Establishment of protocols for re-visitation and re-evaluation of the research impact assessment: Impact assessments must pay continuous attention both to the dynamic and changing character of the project lifecycle and to the shifting conditions of the real-world environments in which research practices, results, and outputs are embedded. There are two sets of factors that should inform when and how often initial impact assessments are re-visited to ensure that they remain adequately responsive to factors that could present new potential harms or significantly influence impacts that have been previously identified: Lifecycle and production factors: Choices made at any point along the workflow may affect the veracity of prior impact assessments\u2014leading to a need for re-assessment, reconsideration, and amendment. For instance, design choices could be made that were not anticipated in the initial impact assessment (such choices might include adjusting the variables that are included in the model, choosing more complex algorithms, or grouping variables in ways that may impact specific groups). These changes may influence how a computational model performs, how it is explained, or how it impacts affected individuals and groups. Processes are also iterative and frequently bi-directional, and this often results in the need for revision and update. For these reasons, impact assessments must remain agile, attentive to change, and at-the-ready to evaluatively move back and forth across the decision-making pipeline as downstream actions affect upstream choices and evaluations. Environmental factors: Changes in project-relevant social, regulatory, policy or legal environments (occurring during the time in which the research is taking place) may have a bearing on how well the resulting computational model works and on how the research outputs impact affected individuals and groups. Likewise, domain-level reforms, policy changes, or changes in data recording methods may take place in the population of concern in ways that affect whether the data used to train the model accurately portrays phenomena, populations, or related factors in an accurate manner. In the same vein, cultural or behavioral shifts may occur within affected populations that alter the underlying data distribution and hamper the predictive and explanatory efficacy of a model, which has been trained on data collected prior to such shifts. All of these alterations of environmental conditions can have a significant effect on how research practices, outputs, and results impact affected individual and communities.","title":"Anticipate impacts"},{"location":"aeg/chapter5/consider/","text":"Consider context \u00b6 No AI system exists in a vaccuum. They are all embedded in a wider socio-technical environment which will affect the way the system's deployment functions. Therefore, considering the wider context the system operates in is imperative for responsible research and innovation in AI. This translates into think diligently about the conditions and circumstances surrounding the system, its operation and its outputs, including the norms, values, and interests that inform the people undertaking the development of the project and that shape and motivate the reasonable expectations of the project's stakeholders. Some of the questions to bear in mind when considering context are: How are these norms, values and interests influencing or steering the project and its outputs? How could they influence the the users\u2019 meaningful consent and expectations of privacy, confidentiality, and anonymity? How could they shape a project\u2019s reception and impacts across impacted communities? Considering these questions will ensure reflection within the project team, and will help to anticipate the potential negative impacts the use of an AI system may have. Considering context also involves taking into account the specific domain(s), geographical location(s), and jurisdiction(s) in which the project is situated and reflecting on the expectations of affected stakeholders in these specific contexts. Some relevant questions are: How are do the existing institutional norms and rules in a given domain or jurisdiction shape expectations regarding project goals, practices, and outputs? How do the unique social, cultural, legal, economic, and political environments in which different projects are embedded influence the conditions of data generation, the intentions and behaviours of the research subjects that are captured by extracted data, and the space of possible inferences that data analytics, modelling, and simulation can yield? In summary All in all, contextual considerations should, at minimum, track three vectors: The first involves considering the contextual determinants of the condition of the production of the project (e.g., thinking about the positionality of the team, the expectations of the revelant community of practice, and the external influences on the aims and means of research by funders, collaborators, and providers of data and research infrastructure). The second involves considering the context of the users of the system (e.g., thinking about subjects\u2019 reasonable expectations of gainful obscurity and \u2018privacy in public\u2019 and considering the changing contexts of their communications such as with whom they are interacting, where, how, and what kinds of data are being shared). The third involves considering the contexts of the social, cultural, legal, economic, and political environments in which different projects are embedded as well as the historical, geographic, sectoral, and jurisdictional specificities that configure such environments (e.g., thinking about the ways different social groups\u2014both within and between cultures\u2014understand and define key values, research variables, and studied concepts differently as well as the ways that these divergent understandings place limitations on what computational approaches to prediction, classification, modelling, and simulation can achieve).","title":"Consider Context"},{"location":"aeg/chapter5/consider/#consider-context","text":"No AI system exists in a vaccuum. They are all embedded in a wider socio-technical environment which will affect the way the system's deployment functions. Therefore, considering the wider context the system operates in is imperative for responsible research and innovation in AI. This translates into think diligently about the conditions and circumstances surrounding the system, its operation and its outputs, including the norms, values, and interests that inform the people undertaking the development of the project and that shape and motivate the reasonable expectations of the project's stakeholders. Some of the questions to bear in mind when considering context are: How are these norms, values and interests influencing or steering the project and its outputs? How could they influence the the users\u2019 meaningful consent and expectations of privacy, confidentiality, and anonymity? How could they shape a project\u2019s reception and impacts across impacted communities? Considering these questions will ensure reflection within the project team, and will help to anticipate the potential negative impacts the use of an AI system may have. Considering context also involves taking into account the specific domain(s), geographical location(s), and jurisdiction(s) in which the project is situated and reflecting on the expectations of affected stakeholders in these specific contexts. Some relevant questions are: How are do the existing institutional norms and rules in a given domain or jurisdiction shape expectations regarding project goals, practices, and outputs? How do the unique social, cultural, legal, economic, and political environments in which different projects are embedded influence the conditions of data generation, the intentions and behaviours of the research subjects that are captured by extracted data, and the space of possible inferences that data analytics, modelling, and simulation can yield? In summary All in all, contextual considerations should, at minimum, track three vectors: The first involves considering the contextual determinants of the condition of the production of the project (e.g., thinking about the positionality of the team, the expectations of the revelant community of practice, and the external influences on the aims and means of research by funders, collaborators, and providers of data and research infrastructure). The second involves considering the context of the users of the system (e.g., thinking about subjects\u2019 reasonable expectations of gainful obscurity and \u2018privacy in public\u2019 and considering the changing contexts of their communications such as with whom they are interacting, where, how, and what kinds of data are being shared). The third involves considering the contexts of the social, cultural, legal, economic, and political environments in which different projects are embedded as well as the historical, geographic, sectoral, and jurisdictional specificities that configure such environments (e.g., thinking about the ways different social groups\u2014both within and between cultures\u2014understand and define key values, research variables, and studied concepts differently as well as the ways that these divergent understandings place limitations on what computational approaches to prediction, classification, modelling, and simulation can achieve).","title":"Consider context"},{"location":"aeg/chapter5/engage/","text":"Engage Inclusively \u00b6 If reflection on power dynamics, positionality, and the purpose of the project being developed is an inwards-facing process for the AI project team, stakeholder engagement and community involvement represents the outwards-facing side of the coin. As we have seen, engagement and involvement with the community can bolster a project\u2019s legitimacy, social license, and democratic governance as well as ensure that its outputs will possess an appropriate degree of public accountability and transparency. A diligent stakeholder engagement process can help teams to: identify stakeholder salience, undertake team positionality reflection, and facilitate proportionate community involvement and input throughout the research project workflow. This process can also safeguard the equity and the contextual accuracy of impact assessments and facilitate appropriate end-to-end processes of transparent project governance by supporting their iterative revisitation and re-evaluation. It is important to note, however, that all stakeholder engagement processes can run the risk either of being cosmetic or tokenistic. They can be employed to grant legitimacy to projects without substantially and meaningfully engaging with the impacted communities (i.e., being one-way information flows or nudging exercises that serve as public relations instruments).[@arnstein1969a]-[@tritter2006] To avoid such hazards of superficiality, team members should shore up a proportionate approach to stakeholder engagement through deliberate and precise goal setting. Factors that affect stakeholder engagement objective Assessment of risks of adverse impacts: As we have stressed throughout this course, stakeholder involvement in projects should be proportionate to the scope of their potential risks and hazards. Assessment of positionality: Stakeholder involvement should address positionality limitations. For instance, in cases where the identity characteristics of team members do not sufficiently reflect or represent significantly impacted groups, stakeholder participation can \u201cfill gaps\u201d in knowledge, domain expertise, and lived experience. Assessment of project needs: Stakeholder involvement should help team members strengthen their ability to frame questions and to tackle problems. After all, those impacted by the project are the most likely to know what their problems are, and thus, what issues the project should tackle. Team members should explore the optimal means for community members to actively contribute to their practices. Practical challenges will be encountered when trying to operationalise a stakeholder engagement process. For example, limits on available resources and tight timelines could be at cross-purposes with the degree of stakeholder involvement that is recommended by team-based assessments of research needs, potential hazards, and positionality limitations. Likewise, the chosen degree of appropriate public participation may be unrealistic or out-of-reach given the engagement barriers that arise from constraints on the capacity of vulnerable stakeholder groups to participate, difficulties in reaching marginalised, isolated, or socially excluded groups, and challenges to participation that are presented by digital divides. In these instances, project teams should take a deliberate and reflective approach to deciding on how to balance engagement goals with practical considerations and should, at all events, make explicit the rationale behind their choices and document this . Regardless of any potential trade-offs, the establishment of clear and explicit stakeholder engagement goals should be prioritised. Relevant questions to pose in establishing these goals include: Why are we engaging with stakeholders? What do we envision the ideal purpose and the expected outcomes of engagement activities to be? How can we best drawn on the insights and lived experience of participants to inform and shape our project?","title":"Engage Inclusively"},{"location":"aeg/chapter5/engage/#engage-inclusively","text":"If reflection on power dynamics, positionality, and the purpose of the project being developed is an inwards-facing process for the AI project team, stakeholder engagement and community involvement represents the outwards-facing side of the coin. As we have seen, engagement and involvement with the community can bolster a project\u2019s legitimacy, social license, and democratic governance as well as ensure that its outputs will possess an appropriate degree of public accountability and transparency. A diligent stakeholder engagement process can help teams to: identify stakeholder salience, undertake team positionality reflection, and facilitate proportionate community involvement and input throughout the research project workflow. This process can also safeguard the equity and the contextual accuracy of impact assessments and facilitate appropriate end-to-end processes of transparent project governance by supporting their iterative revisitation and re-evaluation. It is important to note, however, that all stakeholder engagement processes can run the risk either of being cosmetic or tokenistic. They can be employed to grant legitimacy to projects without substantially and meaningfully engaging with the impacted communities (i.e., being one-way information flows or nudging exercises that serve as public relations instruments).[@arnstein1969a]-[@tritter2006] To avoid such hazards of superficiality, team members should shore up a proportionate approach to stakeholder engagement through deliberate and precise goal setting. Factors that affect stakeholder engagement objective Assessment of risks of adverse impacts: As we have stressed throughout this course, stakeholder involvement in projects should be proportionate to the scope of their potential risks and hazards. Assessment of positionality: Stakeholder involvement should address positionality limitations. For instance, in cases where the identity characteristics of team members do not sufficiently reflect or represent significantly impacted groups, stakeholder participation can \u201cfill gaps\u201d in knowledge, domain expertise, and lived experience. Assessment of project needs: Stakeholder involvement should help team members strengthen their ability to frame questions and to tackle problems. After all, those impacted by the project are the most likely to know what their problems are, and thus, what issues the project should tackle. Team members should explore the optimal means for community members to actively contribute to their practices. Practical challenges will be encountered when trying to operationalise a stakeholder engagement process. For example, limits on available resources and tight timelines could be at cross-purposes with the degree of stakeholder involvement that is recommended by team-based assessments of research needs, potential hazards, and positionality limitations. Likewise, the chosen degree of appropriate public participation may be unrealistic or out-of-reach given the engagement barriers that arise from constraints on the capacity of vulnerable stakeholder groups to participate, difficulties in reaching marginalised, isolated, or socially excluded groups, and challenges to participation that are presented by digital divides. In these instances, project teams should take a deliberate and reflective approach to deciding on how to balance engagement goals with practical considerations and should, at all events, make explicit the rationale behind their choices and document this . Regardless of any potential trade-offs, the establishment of clear and explicit stakeholder engagement goals should be prioritised. Relevant questions to pose in establishing these goals include: Why are we engaging with stakeholders? What do we envision the ideal purpose and the expected outcomes of engagement activities to be? How can we best drawn on the insights and lived experience of participants to inform and shape our project?","title":"Engage Inclusively"},{"location":"aeg/chapter5/reflect/","text":"Reflect on Purpose, Positionality and Power \u00b6 Another crucial element of responsible research and innovation in AI supposes that the people who design, develop, and deploy a system engage in reflexive practices that scrutinise the way potential perspectival limitations and power imbalances can exercise influence on the equity and integrity projects and on the motivations, interests, and aims that steer them. The imperative of reflecting on purposes, positionality, power makes explicit the importance of this dimension of inward-facing reflection. This is a complement to the more outward-facing activities of stakeholder analysis, engagement, and impact assessment. As we have already discussed, all individual human beings come from unique places, experiences, and life contexts that shape their perspectives, motivations, and purposes. Reflecting on these contextual attributes is important insofar as it can help team members understand how their viewpoints might differ from those around them and, more importantly, from those who have diverging cultural and socioeconomic backgrounds and life experiences. Identifying and probing these differences enables individual team members to better understand how their own backgrounds, for better or worse, frame the way they see others, the way they approach and solve problems, and the way they carry out research and engage in innovation. By undertaking such efforts to recognise social position and differential privilege, they may gain a greater awareness of their own personal biases and unconscious assumptions. This then can enable them to better discern the origins of these biases and assumptions and to confront and challenge them in turn. Social scientists have long referred to this site of self-locating reflection as \u201cpositionality\u201d.[@bourke2014]-[@kezar2002]-[@merriam2001] When people take their own positionalities into account, and make this explicit, they can better grasp how the influence of their respective social and cultural positions potentially creates research strengths and limitations. On the one hand, one\u2019s positionality\u2014with respect to characteristics like ethnicity, race, age, gender, socioeconomic status, education and training levels, values, geographical background, etc.\u2014can have a positive effect on an individual\u2019s contributions to a project; the uniqueness of each person\u2019s lived experience and standpoint can play a constructive role in introducing insights and understandings that other team members do not have. On the other hand, one\u2019s positionality can assume a harmful role when hidden biases and prejudices that derive from a person\u2019s background, and from differential privileges and power imbalances, creep into decision-making processes undetected and subconsciously sway the purposes, trajectories, and approaches of projects. When taking positionality into account, team members should reflect on their own positionality matrix. They should ask: To what extent do my personal characteristics, group identifications, socioeconomic status, educational, training, & work background, team composition, & institutional frame represent sources of power and advantage or sources of marginalisation and disadvantage? How does this positionality influence the team's ability to identify & understand affected stakeholders and the potential impacts of the project? Answering these questions involves probing several other areas of self-ascription related to each researcher\u2019s contextual attributes: A solid grasp on positionality allows team members to better interrogate and reflect on the power dynamics that could unduly influence research purposes and trajectories. Such reflections on power should involve an investigation of how power operates, and where it manifests, both across the project lifecycle and in the real-world environments the project is situated. Leslie et al., propose a series of guiding questions that can be used as a reflective tool to help make potentially noxious power dynamics explicit:[@lesliedavid2022] What, if any, power imbalances exist between me (or my team) and the communities impacted by our project? Do the projects I currently pursue reinforce or challenge these imbalances? How, if at all, do these imbalances result in unjust exercises of power? Are my current activities entrenching or combating such exercises of power? What are my interests (or my team\u2019s interest) in collecting or procuring data and in using these to build models and answer questions? How, if at all, are these interests similar to or different from the interests of those in the communities that research impacts? How, if at all, do any power imbalances that exist between me (or my team) and impacted communities influence the pursuit of these interests in my (or my team\u2019s) agendas? How, if at all, do I (or my team) exploit power imbalances to pursue these interests? What other actors hold power and influence over the agendas I pursue and the ways I collect or procure data and build and implement models and data applications? How reliant am I on the data, tools, models, and digital infrastructure (connectivity, computing resources, and data assets) provided by other actors? What are the interests of these actors? How are they similar to or different from my interests and from those of the members of the communities impacted by my data work? What, if any, power imbalances exist between these actors and me (and my firm or organisation)? What is the history of these power imbalances? Are current policies and available resources reinforcing or contesting these imbalances? How, if at all, do these imbalances result in unjust exercises of power? Are current policies and available resources enabling or combating such exercises of power? What does the institutional context of my team look like (taking into account the authority structure within my team(s), wider policy-ownership and power hierarchies in my organisation, levels of decision-making autonomy, and opportunities to voice concerns)? Does this institutional context enable my practices to safeguard the public interest and to ensure that standards and governance regimes in the research ecosystem are working towards just and societally beneficial outcomes?","title":"Reflect on Purpose, Positionality, and Power"},{"location":"aeg/chapter5/reflect/#reflect-on-purpose-positionality-and-power","text":"Another crucial element of responsible research and innovation in AI supposes that the people who design, develop, and deploy a system engage in reflexive practices that scrutinise the way potential perspectival limitations and power imbalances can exercise influence on the equity and integrity projects and on the motivations, interests, and aims that steer them. The imperative of reflecting on purposes, positionality, power makes explicit the importance of this dimension of inward-facing reflection. This is a complement to the more outward-facing activities of stakeholder analysis, engagement, and impact assessment. As we have already discussed, all individual human beings come from unique places, experiences, and life contexts that shape their perspectives, motivations, and purposes. Reflecting on these contextual attributes is important insofar as it can help team members understand how their viewpoints might differ from those around them and, more importantly, from those who have diverging cultural and socioeconomic backgrounds and life experiences. Identifying and probing these differences enables individual team members to better understand how their own backgrounds, for better or worse, frame the way they see others, the way they approach and solve problems, and the way they carry out research and engage in innovation. By undertaking such efforts to recognise social position and differential privilege, they may gain a greater awareness of their own personal biases and unconscious assumptions. This then can enable them to better discern the origins of these biases and assumptions and to confront and challenge them in turn. Social scientists have long referred to this site of self-locating reflection as \u201cpositionality\u201d.[@bourke2014]-[@kezar2002]-[@merriam2001] When people take their own positionalities into account, and make this explicit, they can better grasp how the influence of their respective social and cultural positions potentially creates research strengths and limitations. On the one hand, one\u2019s positionality\u2014with respect to characteristics like ethnicity, race, age, gender, socioeconomic status, education and training levels, values, geographical background, etc.\u2014can have a positive effect on an individual\u2019s contributions to a project; the uniqueness of each person\u2019s lived experience and standpoint can play a constructive role in introducing insights and understandings that other team members do not have. On the other hand, one\u2019s positionality can assume a harmful role when hidden biases and prejudices that derive from a person\u2019s background, and from differential privileges and power imbalances, creep into decision-making processes undetected and subconsciously sway the purposes, trajectories, and approaches of projects. When taking positionality into account, team members should reflect on their own positionality matrix. They should ask: To what extent do my personal characteristics, group identifications, socioeconomic status, educational, training, & work background, team composition, & institutional frame represent sources of power and advantage or sources of marginalisation and disadvantage? How does this positionality influence the team's ability to identify & understand affected stakeholders and the potential impacts of the project? Answering these questions involves probing several other areas of self-ascription related to each researcher\u2019s contextual attributes: A solid grasp on positionality allows team members to better interrogate and reflect on the power dynamics that could unduly influence research purposes and trajectories. Such reflections on power should involve an investigation of how power operates, and where it manifests, both across the project lifecycle and in the real-world environments the project is situated. Leslie et al., propose a series of guiding questions that can be used as a reflective tool to help make potentially noxious power dynamics explicit:[@lesliedavid2022] What, if any, power imbalances exist between me (or my team) and the communities impacted by our project? Do the projects I currently pursue reinforce or challenge these imbalances? How, if at all, do these imbalances result in unjust exercises of power? Are my current activities entrenching or combating such exercises of power? What are my interests (or my team\u2019s interest) in collecting or procuring data and in using these to build models and answer questions? How, if at all, are these interests similar to or different from the interests of those in the communities that research impacts? How, if at all, do any power imbalances that exist between me (or my team) and impacted communities influence the pursuit of these interests in my (or my team\u2019s) agendas? How, if at all, do I (or my team) exploit power imbalances to pursue these interests? What other actors hold power and influence over the agendas I pursue and the ways I collect or procure data and build and implement models and data applications? How reliant am I on the data, tools, models, and digital infrastructure (connectivity, computing resources, and data assets) provided by other actors? What are the interests of these actors? How are they similar to or different from my interests and from those of the members of the communities impacted by my data work? What, if any, power imbalances exist between these actors and me (and my firm or organisation)? What is the history of these power imbalances? Are current policies and available resources reinforcing or contesting these imbalances? How, if at all, do these imbalances result in unjust exercises of power? Are current policies and available resources enabling or combating such exercises of power? What does the institutional context of my team look like (taking into account the authority structure within my team(s), wider policy-ownership and power hierarchies in my organisation, levels of decision-making autonomy, and opportunities to voice concerns)? Does this institutional context enable my practices to safeguard the public interest and to ensure that standards and governance regimes in the research ecosystem are working towards just and societally beneficial outcomes?","title":"Reflect on Purpose, Positionality and Power"},{"location":"aeg/chapter5/transparency/","text":"Transparency and Explainability \u00b6 Introduction to transparency and explainability \u00b6 Defining transparent AI \u00b6 Transparency as a principle of AI ethics differs a bit in meaning from the everyday use of the term. The common dictionary understanding of transparency defines it as either (1) the quality an object has when one can see clearly through it or (2) the quality of a situation or process that can be clearly justified and explained because it is open to inspection and free from secrets. Transparency as a principle of AI ethics encompasses both of these meanings: On the one hand, transparent AI involves the interpretability of a given AI system, i.e. the ability to know how and why a model performed the way it did in a specific context and therefore to understand the rationale behind its decision or behaviour. This sort of transparency is often referred to by way of the metaphor of \u2018opening the black box\u2019 of AI. It involves content clarification and intelligibility or explicability. On the other hand, transparent AI involves the justifiability of both of the processes that go into its design and implementation and of its outcome. It therefore involves the soundness of the justification of its use. In this more normative meaning, transparent AI is practically justifiable in an unrestricted way if one can demonstrate that both the design and implementation processes that have gone into the particular decision or behaviour of a system and the decision or behaviour itself are sustainable, safe, fair, and driven by responsibly managed data. Process-based vs outcome-based \u00b6 The two-pronged definition of transparency as a principle of AI ethics asks that the project team thinks about transparent AI in terms of the process behind it (the design and implementation practices that lead to an algorithmically supported outcome); and, in terms of its product (the content and justification of that outcome). This also means that explanations are provided to impacted stakeholders that demonstrate how the team and all others involved in the development of the system acted responsibly when choosing the processes behind its design and deployment; and make the reasoning behind the outcome of that decision clear. Process-based explanation of AI systems are about demonstrating that good governance processes and best practices have been followed throughout the design, development and use of the AI system. This entails demonstrating that considerations of sustainability, safety, fairness, and responsible data management were operative end-to-end in the project lifecycle. For example, if trying to explain the fairness and safety of a particular AI-assisted decision, one component of this explanation will involve establishing that adequate measures across the system\u2019s production and deployment have been taken to ensure that its outcome is fair and safe. Outcome-based explanations of AI systems are about clarifying the results of a specific decision. They involve explaining the reasoning behind a particular algorithmically generated outcome in plain, easily understandable, and everyday language that is socially meaningful to impacted stakeholders (understandable in terms of the contextual factors and relationships that it implicates). If there is meaningful human involvement in the decision-making process, it should also be made clear to the affected individual how and why a human judgement that is assisted by an AI output was reached. In addition, an adequate explanantion will also need to confirm that the actual outcome of an AI decision meets criteria previously established in the design process to ensure that the AI system is being used in a fair, safe, and ethical way. An explanation to affected stakeholders should also include a demonstration that a specific decision or behaviour of the system is sustainable, safe, fair, and driven by data that has been responsibly managed. Six main types of explanations \u00b6 Abstract Rationale explanation Responsibility explanation Data explanation Fairness explanation Safety and performance explanation Impact explanation Rationale explanation \u00b6 What does this explanation help people understand? It is about the \u2018why?\u2019 of an AI decision. It helps people understand the reasons that led to a decision outcome, in an accessible way. What does this type of explanation include? How the system performed and behaved to get to that decision outcome. How the different components in the AI system led it to transform inputs into outputs in a particular way. This will help communicate which features, interactions, and parameters were most significant. How these technical components of the logic underlying the result can provide supporting evidence for the decision reached. How this underlying logic can be conveyed as easily understandable reasons to decision recipients. How do the system\u2019s results apply to the concrete context and life situation of the affected individual. What rationale explanations might answer: Will the selected algorithmic model, or set of models, provide a degree of interpretability that corresponds with its impact on affected individuals? Are the supplementary explanation tools being used to help make the complex system explainable good enough to provide meaningful and accurate information about its underlying logic? What information goes into rationale explanations As with the other types of explanation, rationale explanations can be process-based or outcome-based. Process-based explanations clarify: How the procedures set up help provide meaningful explanations of the underlying logic of the AI model\u2019s results. How these procedures are suitable given the model\u2019s particular domain context and its possible impacts on the affected decision recipients and wider society. How the system\u2019s design and deployment workflow has been set up so that it is appropriately interpretable and explainable, including its data collection and preprocessing, model selection, explanation extraction, and explanation delivery procedures. Outcome-based explanations provide: The formal and logical rationale of the AI system \u2013 how the system is verified against its formal specifications. In this way, one can verify that the AI system will operate reliably and behave in accordance with its intended functionality. The technical rationale of the system\u2019s output \u2013 how the model\u2019s components (its variables and rules) transform inputs into outputs, so that the role these components play in producing that output is known. By understanding the roles and functions of the individual components, it is possible to identify the features and parameters that significantly influence a particular output. Translation of the system\u2019s workings \u2013 its input and output variables, parameters and so on \u2013 into accessible everyday language. This enables those in charge of the AI system to clarify, in plain and understandable terms, what role these factors play in reasoning about the real-world problem that the model is trying to address or solve. Clarification of how a statistical result is applied to the individual concerned. Responsibility explanation \u00b6 What does this explanation help people understand? It helps people understand \u2018who\u2019 is involved in the development and management of the AI model , and \u2018who\u2019 to contact for a human review of a decision . What does this type of explanation include? Who is accountable at each stage of the AI system\u2019s design and deployment, from defining outcomes for the system at its initial phase of design, through to providing the explanation to the affected individual at the end. Definitions of the mechanisms by which each of these people will be held accountable, as well as how the design and implementation processes of the AI system have been made traceable and auditable. What information goes into responsibility explanations Process-based explanations clarify: The roles and functions across the organisation that are involved in the various stages of developing and implementing an AI system, including any human involvement in the decision-making. If the system, or parts of it, are procured, thjen information about the providers or developers involved should be included. Broadly, what the roles do, why they are important, and where overall responsibility lies for management of the AI model \u2013 who is ultimately accountable. Who is responsible at each step from the design of an AI system through to its implementation to make sure that there is effective accountability throughout. Outcome-based explanations: Because a responsibility explanation largely has to do with the governance of the design and implementation of AI systems, it is, in a strict sense, entirely process-based. Even so, there is important information about post-decision procedures that should be provided: Cover information on how to request a human review of an AI-enabled decision or object to the use of AI, including details on who to contact, and what the next steps will be (e.g., how long it will take, what the human reviewer will take into account, how they will present their own decision and explanation). Give individuals a way to directly contact the role or team responsible for the review (this does not necessarily have to be a specific individual within the organisation). Data explanation \u00b6 What does this explanation help people understand? Data explanations are about the \u2018what\u2019 of AI-assisted decisions . They let people know what data about them were used in a particular AI decision, as well as any other sources of data. Generally, they also help individuals understand more about the data used to train and test the AI model. What does this type of explanation include? How the data used to train, test, and validate an AI model was managed and utilised from collection through processing and monitoring. Which data was used in a particular decision and how. What information goes into data explanations Process-based explanations include: What training/testing/validating data was collected, the sources of that data, and the methods that were used to collect it. Who took part in choosing the data to be collected or procured and who was involved in its recording or acquisition. How procured or third-party provided data was vetted. How data quality was assessed and the steps that were taken to address any quality issues discovered, such as completing or removing data. What the training/testing/validating split was and how it was determined. How data pre-processing, labelling, and augmentation supported the interpretability and explainability of the model. What measures were taken to ensure the data used to train, test, and validate the system was representative, relevant, accurately measured, and generalisable. How potential bias and discrimination in the dataset have been mitigated. Outcome-based explanations: Clarify the input data used for a specific decision, and the sources of that data. This is outcome-based because it refers to the AI system\u2019s result for a particular decision recipient. In some cases, the output data may also require an explanation, particularly where the decision recipient has been placed in a category which may not be clear to them. For example, in the case of anomaly detection for financial fraud identification, the output might be a distance measure which places them at a certain distance away from other people based on their transaction history. Such a classification may require an explanation. Fairness explanation \u00b6 What does this explanation help people understand? The fairness explanation is about helping people understand the steps have been taken (and will continue to be taken) to ensure an AI decisions are generally unbiased and equitable. It also gives people an understanding of whether or not they have been treated equitably themselves. What does this type of explanation include? An explanation of fairness can relate to several stages of the design, development and deployment of AI systems: A) Data fairness: The system is trained and tested on properly representative, relevant, accurately measured, and generalisable datasets (note that this dataset fairness component will overlap with data explanation). This may include showing that the data used is: as representative as possible of all those affected; sufficient in terms of its quantity and quality, so it represents the underlying population and the phenomenon being modelled; assessed and recorded through suitable, reliable and impartial sources of measurement and has been sourced through sound collection methods; up-to-date and accurately reflects the characteristics of individuals, populations and the phenomena that is being modeled; and relevant by calling on domain experts to help the team understand, assess and use the most appropriate sources and types of data to serve the project's objectives. B) Design fairness: It needs to be appropriately shown the AI model's architectures do not include target variables, features,processes, or analytical structures (correlations, interactions, and inferences) which are unreasonable or unjustifiable. This may include showing that the following has been done: Any underlying structural biases that may play a role in translating the objectives into target variables and measurable proxies have been identified. When defining the problem at the start of the AI project, these biases could influence what system designers expect target variables to measure and what they statistically represent. Biases in the data pre-processing phase have been mitigated by taking into account the sector or organisational context in which the AI system is being operated. When this process is automated or outsourced, one should be able to show that what was done has been reviewed and that oversight was maintained throughout. Information on the context of the metadata should also be included, so that those coming to the pre-processed data later on have access to the relevant properties when they undertake bias mitigation. Mitigated bias when the feature space was determined (i.e., when relevant features were selected as input variables for your model). Choices made about grouping or separating and including or excluding features, as well as more general judgements about the comprehensiveness or coarseness of the total set of features, may have consequences for protected groups of people. Mitigated bias when tuning parameters and setting metrics at the modelling, testing and evaluation stages (i.e., into the trained model). The AI development team should iterate the model and peer review it to help ensure that how they choose to adjust the dials and metrics of the model are in line with your objectives of mitigating bias. Mitigated bias by watching for hidden proxies for discriminatory features in the trained model, as these may act as influences on a model\u2019s output. Designers should also look into whether the significant correlations and inferences determined by the model\u2019s learning mechanisms are justifiable. C) Metric-based fairness: This is about making sure that the model does not have discriminatory or inequitable impact on the lives of the people it affects. This may include showing that: The formal definition(s) of fairness that has been chosen is made explicit, as well as the reason why this decision. Data scientists can apply different formalised fairness criteria to choose how specific groups in a selected set will receive benefits in comparison to others in the same set, or how the accuracy or precision of the model will be distributed among subgroups; and the method applied in operationalising theformalised fairness criteria, for example, by reweighting model parameters; embedding trade-offs in a classification procedure; or re-tooling algorithmic results to adjust for outcome preferences. D) Implementation fairness: The AI sysetem is deployed by users sufficiently trained to implement it responsibly and without bias. This may include showing that implementers of the AI system have been appropriately prepared and trained to: avoid automation bias (over-relying on the outputs of AI systems) or automation-distrust bias (under-relying on AI system outputs because of a lack of trust in them); use its results with an active awareness of the specific context in which they are being applied. They should understand the particular circumstances of the individual to which that output is being applied; and understand the limitations of the system. This includes understanding the statistical uncertainty associated with the result as well as the relevant error rates and performance metrics. What information goes into fairness explanations This explanation is about providing people with appropriately simplified and concise information on the considerations, measures and testing you carry out to make sure that your AI system is equitable and that bias has been optimally mitigated. Fairness considerations come into play through the whole lifecycle of an AI model, from inception to deployment, monitoring and review. Process-based explanations include: the chosen measures to mitigate risks of bias and discrimination at the data collection, preparation, model design and testing stages; how these measures were chosen and how managed informational barriers to bias-aware design such as limited access to data about protected or sensitive traits of concern have been managed; and the results of the initial (and ongoing) fairness testing, self-assessment, and external validation \u2013 showing that the chosen fairness measures are deliberately and effectively being integrated into model design. This can be done by showing that different groups of people receive similar outcomes, or that protected characteristics have not played a factor in the results. Outcome-based explanations include: details about how the formal fairness criteria were implemented in the case of a particular decision or output; presentation of the relevant fairness metrics and performance measurements in the delivery interface of your model. This should be geared to a non-technical audience and done in an easily understandable way; and explanations of how others similar to the individual were treated (i.e., whether they received the same decision outcome as the individual). For example, information generated from counter-factual scenarios could be used to show whether or not someone with similar characteristics, but of a different ethnicity or gender, would receive the same decision outcome as the individual. Safety and performance explanation \u00b6 What does this explanation help people understand? The safety and performance explanation helps people understand the measures that have been put in place, and the steps that have been taken (and are continuously bein taken) to maximise the accuracy, reliability, security and robustness of the decisions the AI model helps make. It can also be used to justify the type of AI system chosen, such as comparisons to other systems or human decision makers. Key concepts: Accuracy: the proportion of examples for which model generates a correct output. This component may also include other related performance measures such as precision, sensitivity (true positives), and specificity (true negatives). Individuals may want to understand how accurate, precise, and sensitive the output was in their particular case. Reliability: how dependably the AI system does what it was intended to do. If it did not do what it was programmed to carry out, individuals may want to know why, and whether this happened in the process of producing the decision that affected them. Security: the system is able to protect its architecture from unauthorised modification or damage of any of its component parts; the system remains continuously functional and accessible to its authorised users and keeps confidential and private information secure, even under hostile or adversarial conditions. Robustness: the system functions reliably and accurately in practice. Individuals may want to know how well the system works if things go wrong, how this has been anticipated and tested, and how the system has been immunised from adversarial attacks. What information goes into safety and performance explanations Process-based explanations include: For accuracy: How is accuracy measured (e.g., maximising precision to reduce the risk of false negatives). Why those measures were chosen, and what the assurance process behind it was. What was done at the data collection stage to ensure that the training data was up-to-date and reflective of the characteristics of the people to whom the results apply. What kinds of external validation has undertaken to test and confirm your model\u2019s \u2018ground truth\u2019. What the overall accuracy rate of the system was at testing stage. What is done to monitor this (e.g., measuring for concept drift over time). For reliability: How it is measured and what the assurance process behind it is. Results of the formal verification of the system\u2019s programming specifications, i.e., how encoded requirements have been mathematically verified. For security: - How it is measured and what the assurance process behind it is, e.g., how limitation have been set on who is able to access the system, when, and how. How the security of confidential and private information that is processed in the model has been managed. For robustness: How it is measured. Why the specific measures were chosen. What the assurance process behind it is, e.g., how the system has been stress-tested to understand how it responds to adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). Outcome-based explanations: While one might not be able to guarantee accuracy at an individual level, one should be able to provide assurance that, at run-time, an AI system operated reliably, securely, and robustly for a specific decision. In the case of accuracy and the other performance metrics, however the resultsof cross-validation (training/ testing splits) and any external validation carried out, should be included in the model's delivery interface. Other relevant information that could be included is: information related to the system\u2019s confusion matrix (the table that provides the range of performance metrics) and ROC curve (receiver operating characteristics)/ AUC (area under the curve). Include guidance for users and affected individuals that makes the meaning of these measurement methods, and specifically the ones that have been chosen, easily accessible and understandable. This should also include a clear representation of the uncertainty of the results (e.g., confidence intervals and error bars). Impact explanation \u00b6 What does this explanation help people understand? An impact explanation helps people understand how the effects that an AI decision-support system may have on an individual, i.e., what the outcome of the decision means for them, have been considered. It is also about helping individuals to understand the broader societal effects that the use of this AI system may have. This can help reassure people that the use of AI will be of benefit. Impact explanations are therefore often well suited to delivery before an AI-assisted decision has been made. What does this type of explanation include? Demonstrate that thought has been put into how an AI system will potentially affect individuals and wider society. It is important to clearly show affected individuals the process done to determine these possible impacts. What information goes into impact explanations Process-based explanations include: Showing the considerations givne to the AI system\u2019s potential effects, how these considerations were undertaken, and the measures and steps taken to mitigate possible negative impacts on society, and to amplify the positive effects. Information on the plan in place to monitor and re-assess impacts while the system is deployed should also be included. Outcome-based explanations: Although impact explanations are mainly about demonstrating that appropriate forethought has been given into the potential \u2018big picture\u2019 effects, it is also important to consider how to help recipients understand the impact of the AI-assisted decisions that specifically affect them. For instance, one might explain the consequences for the individual of the different possible decision outcomes and how, in some cases, changes in their behaviour would have brought about a different outcome with more positive impacts. This use of counterfactual assessment would help decision recipients make changes that could lead to a different outcome in the future or allow them to challenge the decision. Putting the Principle of Transparency and Explainability into Practice \u00b6 Success Task 1 (Project Planning): Select priority explanations by considering the domain, use case and potential impacts Task 2 (Data Extraction or Procurement, Data Analysis): Collect and pre-process data in an explanation-aware manner Task 3 (Model Selection): Build a system that is able to extract relevant information for a range of explanation types Task 4 (Model Reporting): Translate the rationale of the system\u2019s results into useable and easily understandable reasons Task 5: (User Training) Prepare implementers to deploy the AI system Task 6 (Model Reporting): Consider how to build and present an explanation There are a number of tasks both to help in the design and deployment of appropriately transparent and explainable AI systems and to assist in providing clarification of the results these systems produce to a range of impacted stakeholders (from operators, implementers, and auditors to decision recipients). These tasks make up Transparency and Explainability Assurance Management for AI projects, offering a systematic approach to: Designing, developing, and deploying AI projects in a transparent and explanation-aware fashion; and selecting, extracting and delivering explanations that are differentiated according to the needs and skills of the different audiences they are directed at. Task 1 (Project Planning): Select priority explanations by considering the domain, use case and potential impacts Understanding the different types of explanation will serve to identify the dimensions of an explanation that decision recipients will find useful. In most cases, explaining AI-assisted decisions involves identifying what is happening in the AI system and who is responsible. That means prioritising the rationale and responsibility explanation types. The setting and the sector are important in figuring out what kinds of explanation one should be able to provide. Therefore considering the domain context and use case is crucial to prioritise which explanations the team should be prepared to give. In addition, consider the potential impacts of the particular use of the AI system to determine which other types of explanation should be provided. This will also help in thinking about how much information is required, and how comprehensive it should be. Choosing what to prioritise is not an exact science. Hopefully the explanations prioritised will coincide with what the majority of the people impacted want to know, but it is unlikely that every individual will have all their questions answered. Having a clear and documented rationale for the explanations prioritised will probably also be useful for your own accountability or auditing purposes. Task 2 (Data Extraction or Procurement, Data Analysis): Collect and pre-process data in an explanation-aware manner The data collected and pre-processed before inputting it into the system has an important role to play in the ability to derive each explanation type. Careful labelling and selection of input data can help provide information for your rationale explanation. Providing details about who is responsible at each stage of data collection and pre-processing is part of being more transparent. This is part of the responsibility explanation (information can be drawn from Workflow Governance Map, covered in the AI accountability section of this course). Drawing from the dataset factsheet can aid in providing data explanations, including the following information: the source of the training data; how it was collected; assessments about its quality; and steps taken to address quality issues, such as completing or removing data Check the data used within the model to ensure it is sufficiently representative of those it is making decisions about. Another issue to consider is whether pre-processing techniques, such as re-weighting, are required. These decisions should be documented in the Bias Self-Assessments, and will help construct the fairness explanation. Task 3 (Model Selection): Build a system that is able to extract relevant information for a range of explanation types Deriving the rationale explanation is key to understanding an AI system (as well as complying with parts of the GDPR). It requires looking \u2018under the hood\u2019 and helps in gathering the information needed for some of the other explanations, such as safety and fairness. However, this is a complex task that requires knowing when to use more and less interpretable models and how to understand their outputs. To choose the right AI model for the particular explanation needs, one should think about the domain the system will be working in, and the potential impact of the system. When selecting a model for an AI project, it is important to consider whether: there are costs and benefits of using a newer and potentially less explainable AI model; the data used requires a more or less explainable system; the use case and domain context encourage choosing an inherently interpretable system; the processing needs lead to the selection of a \u2018black box\u2019 model; and the supplementary interpretability tools that help explain a \u2018black box\u2019 model (if chosen) are appropriate given the context. To extract explanations from inherently interpretable models, look at the logic of the model\u2019s mapping function by exploring it and its results directly. On the other hand, there are many techniques used to extract explanations from \u2018black box\u2019 systems. Make sure that they provide a reliable and accurate representation of the system\u2019s behaviour. Task 4 (Model Reporting): Translate the rationale of the system\u2019s results into useable and easily understandable reasons Once the rationale of the underlying logic of the AI model has been extracted, the statistical output needs to be incorporated into the wider decision-making process. Implementers of the outputs from the AI system will need to recognise the factors that they see as legitimate determinants of the outcome they are considering. Decision recipients should be able to easily understand how the statistical result has been applied to their particular case. Task 5: (User Training) Prepare implementers to deploy the AI system In cases where decisions are not fully automated, implementers need to be meaningfully involved. This means that they need to be appropriately trained to use the model\u2019s results responsibly and fairly. Their training should cover: - the basics of how machine learning works; - the limitations of AI and automated decision-support technologies; - the benefits and risks of deploying these systems to assist decision-making, particularly how they help humans come to judgements rather than replacing that judgement; and - how to manage cognitive biases, including both decision-automation bias and automation-distrust bias. Task 6 (Model Reporting): Consider how to build and present an explanation Gathering together the information gained when implementing Tasks 1-4 is the first step towards building an explanation. This includes reviewing the information and determine how this provides an evidence base for the process-based or outcome-based explanations. Additionally, it is important to revisit the contextual factors to establish which explanation types should be prioritised. The way an explanation is presented depends on the way AI-assisted decisions are made, and on how people might expect those responsible for the AI system to deliver explanations without using AI. Explanations can la 'layered' by proactively providing individuals with the prioritised explanations first and making additional explanations available in further layers. This helps to avoid information (or explanation) overload. Delivering explanations should be thought of as a conversation, rather than a one-way process. People should be able to discuss a decision with a competent human being. Providing an explanation at the right time is also important. Proactively engaging with customers by making information available on how the AI system is used and how it aids in making decisions, can increase the trust and awareness.","title":"Transparency & Explainability"},{"location":"aeg/chapter5/transparency/#transparency-and-explainability","text":"","title":"Transparency and Explainability"},{"location":"aeg/chapter5/transparency/#introduction-to-transparency-and-explainability","text":"","title":"Introduction to transparency and explainability"},{"location":"aeg/chapter5/transparency/#defining-transparent-ai","text":"Transparency as a principle of AI ethics differs a bit in meaning from the everyday use of the term. The common dictionary understanding of transparency defines it as either (1) the quality an object has when one can see clearly through it or (2) the quality of a situation or process that can be clearly justified and explained because it is open to inspection and free from secrets. Transparency as a principle of AI ethics encompasses both of these meanings: On the one hand, transparent AI involves the interpretability of a given AI system, i.e. the ability to know how and why a model performed the way it did in a specific context and therefore to understand the rationale behind its decision or behaviour. This sort of transparency is often referred to by way of the metaphor of \u2018opening the black box\u2019 of AI. It involves content clarification and intelligibility or explicability. On the other hand, transparent AI involves the justifiability of both of the processes that go into its design and implementation and of its outcome. It therefore involves the soundness of the justification of its use. In this more normative meaning, transparent AI is practically justifiable in an unrestricted way if one can demonstrate that both the design and implementation processes that have gone into the particular decision or behaviour of a system and the decision or behaviour itself are sustainable, safe, fair, and driven by responsibly managed data.","title":"Defining transparent AI"},{"location":"aeg/chapter5/transparency/#process-based-vs-outcome-based","text":"The two-pronged definition of transparency as a principle of AI ethics asks that the project team thinks about transparent AI in terms of the process behind it (the design and implementation practices that lead to an algorithmically supported outcome); and, in terms of its product (the content and justification of that outcome). This also means that explanations are provided to impacted stakeholders that demonstrate how the team and all others involved in the development of the system acted responsibly when choosing the processes behind its design and deployment; and make the reasoning behind the outcome of that decision clear. Process-based explanation of AI systems are about demonstrating that good governance processes and best practices have been followed throughout the design, development and use of the AI system. This entails demonstrating that considerations of sustainability, safety, fairness, and responsible data management were operative end-to-end in the project lifecycle. For example, if trying to explain the fairness and safety of a particular AI-assisted decision, one component of this explanation will involve establishing that adequate measures across the system\u2019s production and deployment have been taken to ensure that its outcome is fair and safe. Outcome-based explanations of AI systems are about clarifying the results of a specific decision. They involve explaining the reasoning behind a particular algorithmically generated outcome in plain, easily understandable, and everyday language that is socially meaningful to impacted stakeholders (understandable in terms of the contextual factors and relationships that it implicates). If there is meaningful human involvement in the decision-making process, it should also be made clear to the affected individual how and why a human judgement that is assisted by an AI output was reached. In addition, an adequate explanantion will also need to confirm that the actual outcome of an AI decision meets criteria previously established in the design process to ensure that the AI system is being used in a fair, safe, and ethical way. An explanation to affected stakeholders should also include a demonstration that a specific decision or behaviour of the system is sustainable, safe, fair, and driven by data that has been responsibly managed.","title":"Process-based vs outcome-based"},{"location":"aeg/chapter5/transparency/#six-main-types-of-explanations","text":"Abstract Rationale explanation Responsibility explanation Data explanation Fairness explanation Safety and performance explanation Impact explanation","title":"Six main types of explanations"},{"location":"aeg/chapter5/transparency/#rationale-explanation","text":"What does this explanation help people understand? It is about the \u2018why?\u2019 of an AI decision. It helps people understand the reasons that led to a decision outcome, in an accessible way. What does this type of explanation include? How the system performed and behaved to get to that decision outcome. How the different components in the AI system led it to transform inputs into outputs in a particular way. This will help communicate which features, interactions, and parameters were most significant. How these technical components of the logic underlying the result can provide supporting evidence for the decision reached. How this underlying logic can be conveyed as easily understandable reasons to decision recipients. How do the system\u2019s results apply to the concrete context and life situation of the affected individual. What rationale explanations might answer: Will the selected algorithmic model, or set of models, provide a degree of interpretability that corresponds with its impact on affected individuals? Are the supplementary explanation tools being used to help make the complex system explainable good enough to provide meaningful and accurate information about its underlying logic? What information goes into rationale explanations As with the other types of explanation, rationale explanations can be process-based or outcome-based. Process-based explanations clarify: How the procedures set up help provide meaningful explanations of the underlying logic of the AI model\u2019s results. How these procedures are suitable given the model\u2019s particular domain context and its possible impacts on the affected decision recipients and wider society. How the system\u2019s design and deployment workflow has been set up so that it is appropriately interpretable and explainable, including its data collection and preprocessing, model selection, explanation extraction, and explanation delivery procedures. Outcome-based explanations provide: The formal and logical rationale of the AI system \u2013 how the system is verified against its formal specifications. In this way, one can verify that the AI system will operate reliably and behave in accordance with its intended functionality. The technical rationale of the system\u2019s output \u2013 how the model\u2019s components (its variables and rules) transform inputs into outputs, so that the role these components play in producing that output is known. By understanding the roles and functions of the individual components, it is possible to identify the features and parameters that significantly influence a particular output. Translation of the system\u2019s workings \u2013 its input and output variables, parameters and so on \u2013 into accessible everyday language. This enables those in charge of the AI system to clarify, in plain and understandable terms, what role these factors play in reasoning about the real-world problem that the model is trying to address or solve. Clarification of how a statistical result is applied to the individual concerned.","title":"Rationale explanation"},{"location":"aeg/chapter5/transparency/#responsibility-explanation","text":"What does this explanation help people understand? It helps people understand \u2018who\u2019 is involved in the development and management of the AI model , and \u2018who\u2019 to contact for a human review of a decision . What does this type of explanation include? Who is accountable at each stage of the AI system\u2019s design and deployment, from defining outcomes for the system at its initial phase of design, through to providing the explanation to the affected individual at the end. Definitions of the mechanisms by which each of these people will be held accountable, as well as how the design and implementation processes of the AI system have been made traceable and auditable. What information goes into responsibility explanations Process-based explanations clarify: The roles and functions across the organisation that are involved in the various stages of developing and implementing an AI system, including any human involvement in the decision-making. If the system, or parts of it, are procured, thjen information about the providers or developers involved should be included. Broadly, what the roles do, why they are important, and where overall responsibility lies for management of the AI model \u2013 who is ultimately accountable. Who is responsible at each step from the design of an AI system through to its implementation to make sure that there is effective accountability throughout. Outcome-based explanations: Because a responsibility explanation largely has to do with the governance of the design and implementation of AI systems, it is, in a strict sense, entirely process-based. Even so, there is important information about post-decision procedures that should be provided: Cover information on how to request a human review of an AI-enabled decision or object to the use of AI, including details on who to contact, and what the next steps will be (e.g., how long it will take, what the human reviewer will take into account, how they will present their own decision and explanation). Give individuals a way to directly contact the role or team responsible for the review (this does not necessarily have to be a specific individual within the organisation).","title":"Responsibility explanation"},{"location":"aeg/chapter5/transparency/#data-explanation","text":"What does this explanation help people understand? Data explanations are about the \u2018what\u2019 of AI-assisted decisions . They let people know what data about them were used in a particular AI decision, as well as any other sources of data. Generally, they also help individuals understand more about the data used to train and test the AI model. What does this type of explanation include? How the data used to train, test, and validate an AI model was managed and utilised from collection through processing and monitoring. Which data was used in a particular decision and how. What information goes into data explanations Process-based explanations include: What training/testing/validating data was collected, the sources of that data, and the methods that were used to collect it. Who took part in choosing the data to be collected or procured and who was involved in its recording or acquisition. How procured or third-party provided data was vetted. How data quality was assessed and the steps that were taken to address any quality issues discovered, such as completing or removing data. What the training/testing/validating split was and how it was determined. How data pre-processing, labelling, and augmentation supported the interpretability and explainability of the model. What measures were taken to ensure the data used to train, test, and validate the system was representative, relevant, accurately measured, and generalisable. How potential bias and discrimination in the dataset have been mitigated. Outcome-based explanations: Clarify the input data used for a specific decision, and the sources of that data. This is outcome-based because it refers to the AI system\u2019s result for a particular decision recipient. In some cases, the output data may also require an explanation, particularly where the decision recipient has been placed in a category which may not be clear to them. For example, in the case of anomaly detection for financial fraud identification, the output might be a distance measure which places them at a certain distance away from other people based on their transaction history. Such a classification may require an explanation.","title":"Data explanation"},{"location":"aeg/chapter5/transparency/#fairness-explanation","text":"What does this explanation help people understand? The fairness explanation is about helping people understand the steps have been taken (and will continue to be taken) to ensure an AI decisions are generally unbiased and equitable. It also gives people an understanding of whether or not they have been treated equitably themselves. What does this type of explanation include? An explanation of fairness can relate to several stages of the design, development and deployment of AI systems: A) Data fairness: The system is trained and tested on properly representative, relevant, accurately measured, and generalisable datasets (note that this dataset fairness component will overlap with data explanation). This may include showing that the data used is: as representative as possible of all those affected; sufficient in terms of its quantity and quality, so it represents the underlying population and the phenomenon being modelled; assessed and recorded through suitable, reliable and impartial sources of measurement and has been sourced through sound collection methods; up-to-date and accurately reflects the characteristics of individuals, populations and the phenomena that is being modeled; and relevant by calling on domain experts to help the team understand, assess and use the most appropriate sources and types of data to serve the project's objectives. B) Design fairness: It needs to be appropriately shown the AI model's architectures do not include target variables, features,processes, or analytical structures (correlations, interactions, and inferences) which are unreasonable or unjustifiable. This may include showing that the following has been done: Any underlying structural biases that may play a role in translating the objectives into target variables and measurable proxies have been identified. When defining the problem at the start of the AI project, these biases could influence what system designers expect target variables to measure and what they statistically represent. Biases in the data pre-processing phase have been mitigated by taking into account the sector or organisational context in which the AI system is being operated. When this process is automated or outsourced, one should be able to show that what was done has been reviewed and that oversight was maintained throughout. Information on the context of the metadata should also be included, so that those coming to the pre-processed data later on have access to the relevant properties when they undertake bias mitigation. Mitigated bias when the feature space was determined (i.e., when relevant features were selected as input variables for your model). Choices made about grouping or separating and including or excluding features, as well as more general judgements about the comprehensiveness or coarseness of the total set of features, may have consequences for protected groups of people. Mitigated bias when tuning parameters and setting metrics at the modelling, testing and evaluation stages (i.e., into the trained model). The AI development team should iterate the model and peer review it to help ensure that how they choose to adjust the dials and metrics of the model are in line with your objectives of mitigating bias. Mitigated bias by watching for hidden proxies for discriminatory features in the trained model, as these may act as influences on a model\u2019s output. Designers should also look into whether the significant correlations and inferences determined by the model\u2019s learning mechanisms are justifiable. C) Metric-based fairness: This is about making sure that the model does not have discriminatory or inequitable impact on the lives of the people it affects. This may include showing that: The formal definition(s) of fairness that has been chosen is made explicit, as well as the reason why this decision. Data scientists can apply different formalised fairness criteria to choose how specific groups in a selected set will receive benefits in comparison to others in the same set, or how the accuracy or precision of the model will be distributed among subgroups; and the method applied in operationalising theformalised fairness criteria, for example, by reweighting model parameters; embedding trade-offs in a classification procedure; or re-tooling algorithmic results to adjust for outcome preferences. D) Implementation fairness: The AI sysetem is deployed by users sufficiently trained to implement it responsibly and without bias. This may include showing that implementers of the AI system have been appropriately prepared and trained to: avoid automation bias (over-relying on the outputs of AI systems) or automation-distrust bias (under-relying on AI system outputs because of a lack of trust in them); use its results with an active awareness of the specific context in which they are being applied. They should understand the particular circumstances of the individual to which that output is being applied; and understand the limitations of the system. This includes understanding the statistical uncertainty associated with the result as well as the relevant error rates and performance metrics. What information goes into fairness explanations This explanation is about providing people with appropriately simplified and concise information on the considerations, measures and testing you carry out to make sure that your AI system is equitable and that bias has been optimally mitigated. Fairness considerations come into play through the whole lifecycle of an AI model, from inception to deployment, monitoring and review. Process-based explanations include: the chosen measures to mitigate risks of bias and discrimination at the data collection, preparation, model design and testing stages; how these measures were chosen and how managed informational barriers to bias-aware design such as limited access to data about protected or sensitive traits of concern have been managed; and the results of the initial (and ongoing) fairness testing, self-assessment, and external validation \u2013 showing that the chosen fairness measures are deliberately and effectively being integrated into model design. This can be done by showing that different groups of people receive similar outcomes, or that protected characteristics have not played a factor in the results. Outcome-based explanations include: details about how the formal fairness criteria were implemented in the case of a particular decision or output; presentation of the relevant fairness metrics and performance measurements in the delivery interface of your model. This should be geared to a non-technical audience and done in an easily understandable way; and explanations of how others similar to the individual were treated (i.e., whether they received the same decision outcome as the individual). For example, information generated from counter-factual scenarios could be used to show whether or not someone with similar characteristics, but of a different ethnicity or gender, would receive the same decision outcome as the individual.","title":"Fairness explanation"},{"location":"aeg/chapter5/transparency/#safety-and-performance-explanation","text":"What does this explanation help people understand? The safety and performance explanation helps people understand the measures that have been put in place, and the steps that have been taken (and are continuously bein taken) to maximise the accuracy, reliability, security and robustness of the decisions the AI model helps make. It can also be used to justify the type of AI system chosen, such as comparisons to other systems or human decision makers. Key concepts: Accuracy: the proportion of examples for which model generates a correct output. This component may also include other related performance measures such as precision, sensitivity (true positives), and specificity (true negatives). Individuals may want to understand how accurate, precise, and sensitive the output was in their particular case. Reliability: how dependably the AI system does what it was intended to do. If it did not do what it was programmed to carry out, individuals may want to know why, and whether this happened in the process of producing the decision that affected them. Security: the system is able to protect its architecture from unauthorised modification or damage of any of its component parts; the system remains continuously functional and accessible to its authorised users and keeps confidential and private information secure, even under hostile or adversarial conditions. Robustness: the system functions reliably and accurately in practice. Individuals may want to know how well the system works if things go wrong, how this has been anticipated and tested, and how the system has been immunised from adversarial attacks. What information goes into safety and performance explanations Process-based explanations include: For accuracy: How is accuracy measured (e.g., maximising precision to reduce the risk of false negatives). Why those measures were chosen, and what the assurance process behind it was. What was done at the data collection stage to ensure that the training data was up-to-date and reflective of the characteristics of the people to whom the results apply. What kinds of external validation has undertaken to test and confirm your model\u2019s \u2018ground truth\u2019. What the overall accuracy rate of the system was at testing stage. What is done to monitor this (e.g., measuring for concept drift over time). For reliability: How it is measured and what the assurance process behind it is. Results of the formal verification of the system\u2019s programming specifications, i.e., how encoded requirements have been mathematically verified. For security: - How it is measured and what the assurance process behind it is, e.g., how limitation have been set on who is able to access the system, when, and how. How the security of confidential and private information that is processed in the model has been managed. For robustness: How it is measured. Why the specific measures were chosen. What the assurance process behind it is, e.g., how the system has been stress-tested to understand how it responds to adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). Outcome-based explanations: While one might not be able to guarantee accuracy at an individual level, one should be able to provide assurance that, at run-time, an AI system operated reliably, securely, and robustly for a specific decision. In the case of accuracy and the other performance metrics, however the resultsof cross-validation (training/ testing splits) and any external validation carried out, should be included in the model's delivery interface. Other relevant information that could be included is: information related to the system\u2019s confusion matrix (the table that provides the range of performance metrics) and ROC curve (receiver operating characteristics)/ AUC (area under the curve). Include guidance for users and affected individuals that makes the meaning of these measurement methods, and specifically the ones that have been chosen, easily accessible and understandable. This should also include a clear representation of the uncertainty of the results (e.g., confidence intervals and error bars).","title":"Safety and performance explanation"},{"location":"aeg/chapter5/transparency/#impact-explanation","text":"What does this explanation help people understand? An impact explanation helps people understand how the effects that an AI decision-support system may have on an individual, i.e., what the outcome of the decision means for them, have been considered. It is also about helping individuals to understand the broader societal effects that the use of this AI system may have. This can help reassure people that the use of AI will be of benefit. Impact explanations are therefore often well suited to delivery before an AI-assisted decision has been made. What does this type of explanation include? Demonstrate that thought has been put into how an AI system will potentially affect individuals and wider society. It is important to clearly show affected individuals the process done to determine these possible impacts. What information goes into impact explanations Process-based explanations include: Showing the considerations givne to the AI system\u2019s potential effects, how these considerations were undertaken, and the measures and steps taken to mitigate possible negative impacts on society, and to amplify the positive effects. Information on the plan in place to monitor and re-assess impacts while the system is deployed should also be included. Outcome-based explanations: Although impact explanations are mainly about demonstrating that appropriate forethought has been given into the potential \u2018big picture\u2019 effects, it is also important to consider how to help recipients understand the impact of the AI-assisted decisions that specifically affect them. For instance, one might explain the consequences for the individual of the different possible decision outcomes and how, in some cases, changes in their behaviour would have brought about a different outcome with more positive impacts. This use of counterfactual assessment would help decision recipients make changes that could lead to a different outcome in the future or allow them to challenge the decision.","title":"Impact explanation"},{"location":"aeg/chapter5/transparency/#putting-the-principle-of-transparency-and-explainability-into-practice","text":"Success Task 1 (Project Planning): Select priority explanations by considering the domain, use case and potential impacts Task 2 (Data Extraction or Procurement, Data Analysis): Collect and pre-process data in an explanation-aware manner Task 3 (Model Selection): Build a system that is able to extract relevant information for a range of explanation types Task 4 (Model Reporting): Translate the rationale of the system\u2019s results into useable and easily understandable reasons Task 5: (User Training) Prepare implementers to deploy the AI system Task 6 (Model Reporting): Consider how to build and present an explanation There are a number of tasks both to help in the design and deployment of appropriately transparent and explainable AI systems and to assist in providing clarification of the results these systems produce to a range of impacted stakeholders (from operators, implementers, and auditors to decision recipients). These tasks make up Transparency and Explainability Assurance Management for AI projects, offering a systematic approach to: Designing, developing, and deploying AI projects in a transparent and explanation-aware fashion; and selecting, extracting and delivering explanations that are differentiated according to the needs and skills of the different audiences they are directed at. Task 1 (Project Planning): Select priority explanations by considering the domain, use case and potential impacts Understanding the different types of explanation will serve to identify the dimensions of an explanation that decision recipients will find useful. In most cases, explaining AI-assisted decisions involves identifying what is happening in the AI system and who is responsible. That means prioritising the rationale and responsibility explanation types. The setting and the sector are important in figuring out what kinds of explanation one should be able to provide. Therefore considering the domain context and use case is crucial to prioritise which explanations the team should be prepared to give. In addition, consider the potential impacts of the particular use of the AI system to determine which other types of explanation should be provided. This will also help in thinking about how much information is required, and how comprehensive it should be. Choosing what to prioritise is not an exact science. Hopefully the explanations prioritised will coincide with what the majority of the people impacted want to know, but it is unlikely that every individual will have all their questions answered. Having a clear and documented rationale for the explanations prioritised will probably also be useful for your own accountability or auditing purposes. Task 2 (Data Extraction or Procurement, Data Analysis): Collect and pre-process data in an explanation-aware manner The data collected and pre-processed before inputting it into the system has an important role to play in the ability to derive each explanation type. Careful labelling and selection of input data can help provide information for your rationale explanation. Providing details about who is responsible at each stage of data collection and pre-processing is part of being more transparent. This is part of the responsibility explanation (information can be drawn from Workflow Governance Map, covered in the AI accountability section of this course). Drawing from the dataset factsheet can aid in providing data explanations, including the following information: the source of the training data; how it was collected; assessments about its quality; and steps taken to address quality issues, such as completing or removing data Check the data used within the model to ensure it is sufficiently representative of those it is making decisions about. Another issue to consider is whether pre-processing techniques, such as re-weighting, are required. These decisions should be documented in the Bias Self-Assessments, and will help construct the fairness explanation. Task 3 (Model Selection): Build a system that is able to extract relevant information for a range of explanation types Deriving the rationale explanation is key to understanding an AI system (as well as complying with parts of the GDPR). It requires looking \u2018under the hood\u2019 and helps in gathering the information needed for some of the other explanations, such as safety and fairness. However, this is a complex task that requires knowing when to use more and less interpretable models and how to understand their outputs. To choose the right AI model for the particular explanation needs, one should think about the domain the system will be working in, and the potential impact of the system. When selecting a model for an AI project, it is important to consider whether: there are costs and benefits of using a newer and potentially less explainable AI model; the data used requires a more or less explainable system; the use case and domain context encourage choosing an inherently interpretable system; the processing needs lead to the selection of a \u2018black box\u2019 model; and the supplementary interpretability tools that help explain a \u2018black box\u2019 model (if chosen) are appropriate given the context. To extract explanations from inherently interpretable models, look at the logic of the model\u2019s mapping function by exploring it and its results directly. On the other hand, there are many techniques used to extract explanations from \u2018black box\u2019 systems. Make sure that they provide a reliable and accurate representation of the system\u2019s behaviour. Task 4 (Model Reporting): Translate the rationale of the system\u2019s results into useable and easily understandable reasons Once the rationale of the underlying logic of the AI model has been extracted, the statistical output needs to be incorporated into the wider decision-making process. Implementers of the outputs from the AI system will need to recognise the factors that they see as legitimate determinants of the outcome they are considering. Decision recipients should be able to easily understand how the statistical result has been applied to their particular case. Task 5: (User Training) Prepare implementers to deploy the AI system In cases where decisions are not fully automated, implementers need to be meaningfully involved. This means that they need to be appropriately trained to use the model\u2019s results responsibly and fairly. Their training should cover: - the basics of how machine learning works; - the limitations of AI and automated decision-support technologies; - the benefits and risks of deploying these systems to assist decision-making, particularly how they help humans come to judgements rather than replacing that judgement; and - how to manage cognitive biases, including both decision-automation bias and automation-distrust bias. Task 6 (Model Reporting): Consider how to build and present an explanation Gathering together the information gained when implementing Tasks 1-4 is the first step towards building an explanation. This includes reviewing the information and determine how this provides an evidence base for the process-based or outcome-based explanations. Additionally, it is important to revisit the contextual factors to establish which explanation types should be prioritised. The way an explanation is presented depends on the way AI-assisted decisions are made, and on how people might expect those responsible for the AI system to deliver explanations without using AI. Explanations can la 'layered' by proactively providing individuals with the prioritised explanations first and making additional explanations available in further layers. This helps to avoid information (or explanation) overload. Delivering explanations should be thought of as a conversation, rather than a one-way process. People should be able to discuss a decision with a competent human being. Providing an explanation at the right time is also important. Proactively engaging with customers by making information available on how the AI system is used and how it aids in making decisions, can increase the trust and awareness.","title":"Putting the Principle of Transparency and Explainability into Practice"},{"location":"assets/images/illustrations/","text":"Reuse of Illustrations \u00b6 All of the illustrations in this folder were commissioned for the purpose of the Turing Commons project and were illustrated by Johnny Lighthands . As with the rest of the content in this repository, you are free to copy and redistribute the material in any medium or format, as well as remix, transform, and build upon the material for any purpose, even commercially. However, when doing so, you must give appropriate credit both to the illustrator and this project, provide a link to the license , and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. Creative Commons Attribution 4.0 International License .","title":"Reuse of Illustrations"},{"location":"assets/images/illustrations/#reuse-of-illustrations","text":"All of the illustrations in this folder were commissioned for the purpose of the Turing Commons project and were illustrated by Johnny Lighthands . As with the rest of the content in this repository, you are free to copy and redistribute the material in any medium or format, as well as remix, transform, and build upon the material for any purpose, even commercially. However, when doing so, you must give appropriate credit both to the illustrator and this project, provide a link to the license , and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. Creative Commons Attribution 4.0 International License .","title":"Reuse of Illustrations"},{"location":"assets/images/illustrations/high-res/","text":"Reuse of Illustrations \u00b6 All of the illustrations in this folder were commissioned for the purpose of the Turing Commons project and were illustrated by Johnny Lighthands . As with the rest of the content in this repository, you are free to copy and redistribute the material in any medium or format, as well as remix, transform, and build upon the material for any purpose, even commercially. However, when doing so, you must give appropriate credit both to the illustrator and this project, provide a link to the license , and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. Creative Commons Attribution 4.0 International License .","title":"Reuse of Illustrations"},{"location":"assets/images/illustrations/high-res/#reuse-of-illustrations","text":"All of the illustrations in this folder were commissioned for the purpose of the Turing Commons project and were illustrated by Johnny Lighthands . As with the rest of the content in this repository, you are free to copy and redistribute the material in any medium or format, as well as remix, transform, and build upon the material for any purpose, even commercially. However, when doing so, you must give appropriate credit both to the illustrator and this project, provide a link to the license , and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. Creative Commons Attribution 4.0 International License .","title":"Reuse of Illustrations"},{"location":"assets/pdfs/course-presentations/","text":"Slides \u00b6 In this directory you can find all of the slides used in our three courses: Responsible Research and Innovation Public Engagement of Data Science and AI AI Ethics and Governance","title":"Slides"},{"location":"assets/pdfs/course-presentations/#slides","text":"In this directory you can find all of the slides used in our three courses: Responsible Research and Innovation Public Engagement of Data Science and AI AI Ethics and Governance","title":"Slides"},{"location":"assets/pdfs/course-presentations/ai-ethics-and-governance-2022/","text":"AEG Presentation Slides \u00b6 The slides for the AEG course will be uploaded to this directory as they become available.","title":"AEG Presentation Slides"},{"location":"assets/pdfs/course-presentations/ai-ethics-and-governance-2022/#aeg-presentation-slides","text":"The slides for the AEG course will be uploaded to this directory as they become available.","title":"AEG Presentation Slides"},{"location":"blog/","text":"Blog \u00b6","title":"Blog"},{"location":"blog/#blog","text":"","title":"Blog"},{"location":"blog/posts/ai-ethics-announcement/","tags":["ai ethics"],"text":"Course Announcement: AI Ethics and Governance \u00b6 What do we mean when we talk about the ethics of artificial intelligence? If guided by popular culture and science fiction, we would probably turn to images of seemingly sentient robots and questions on the ethics of our (mis)treatment of them. 1 However fascinating they may be, we are not yet at a stage where we need to grapple with these considerations. But this does not mean that we should not care about AI ethics. Quite the contrary. As our society becomes increasingly reliant on algorithms and other data-driven technologies, their functionings and outcomes have immense consequences on individuals, communities, and society at large. *Figure 1. Illustration representing three types of bias\u2014social, statistical, and cognitive\u2014that can affect the design, development, and deployment of AI systems. There is no shortage of concerns, and these keep expanding as our uses of technology multiply. What do privacy and autonomy mean in a datafied society? How do we define and implement fair, non-biased outcomes when using AI systems? And what should the requirments of explainability and transparency be as AI and machine learning become increasingly complex and opaque? These are just some of the ethical questions that the ubiquity of AI gives rise to. We will explore these and other topics in the AI Ethics and Governance course delivered by the Turing Commons team. About the Course \u00b6 The course will take place between the 21st and the 25th of November, and will be delivered by Professor David Leslie and the Turing Commons team. The five day course is scheduled between 10am and 4pm (GMT), and each day will comprise a series of lectures, hands-on sessions through structured activities, and group discussions. The course will be delivered online using Zoom, and to ensure effective group discussion will be limited to 30 participants. After completing the course, participants should have an understanding of the following: - what practical ethics is and how it serves as a foundation for AI ethics; - how AI ethics can be understood in terms of AI harms which violate certain core values; - what the stakeholder engagement process is and what it entails; - what a stakeholder impact assessment is and how to carry one out; - the different elements of AI fairness; - the problem of bias in AI and how to address it; - the key elements of transparent and explainable AI; - what the CARE & ACT principles are and how to apply them in their own research. Following the live workshops, recordings of the taught components will be made available to all (along with relevant materials) for asynchronous self-study. Applications are now open for registration . Application deadline is November 7th. If you have any queries, please reach out to the Turing Commons project organiser, Claudia Fischer . We look forward to seeing you in AI Ethics and Governance soon! Some recent(ish) examples are the novels Machines like Me by Ian McEwan, Klara and the Sun by Kazuo Ishiguro, and the HBO TV show Westworld . \u21a9","title":"Course Announcement: AI Ethics and Governance"},{"location":"blog/posts/ai-ethics-announcement/#course-announcement-ai-ethics-and-governance","text":"What do we mean when we talk about the ethics of artificial intelligence? If guided by popular culture and science fiction, we would probably turn to images of seemingly sentient robots and questions on the ethics of our (mis)treatment of them. 1 However fascinating they may be, we are not yet at a stage where we need to grapple with these considerations. But this does not mean that we should not care about AI ethics. Quite the contrary. As our society becomes increasingly reliant on algorithms and other data-driven technologies, their functionings and outcomes have immense consequences on individuals, communities, and society at large. *Figure 1. Illustration representing three types of bias\u2014social, statistical, and cognitive\u2014that can affect the design, development, and deployment of AI systems. There is no shortage of concerns, and these keep expanding as our uses of technology multiply. What do privacy and autonomy mean in a datafied society? How do we define and implement fair, non-biased outcomes when using AI systems? And what should the requirments of explainability and transparency be as AI and machine learning become increasingly complex and opaque? These are just some of the ethical questions that the ubiquity of AI gives rise to. We will explore these and other topics in the AI Ethics and Governance course delivered by the Turing Commons team.","title":"Course Announcement: AI Ethics and Governance"},{"location":"blog/posts/ai-ethics-announcement/#about-the-course","text":"The course will take place between the 21st and the 25th of November, and will be delivered by Professor David Leslie and the Turing Commons team. The five day course is scheduled between 10am and 4pm (GMT), and each day will comprise a series of lectures, hands-on sessions through structured activities, and group discussions. The course will be delivered online using Zoom, and to ensure effective group discussion will be limited to 30 participants. After completing the course, participants should have an understanding of the following: - what practical ethics is and how it serves as a foundation for AI ethics; - how AI ethics can be understood in terms of AI harms which violate certain core values; - what the stakeholder engagement process is and what it entails; - what a stakeholder impact assessment is and how to carry one out; - the different elements of AI fairness; - the problem of bias in AI and how to address it; - the key elements of transparent and explainable AI; - what the CARE & ACT principles are and how to apply them in their own research. Following the live workshops, recordings of the taught components will be made available to all (along with relevant materials) for asynchronous self-study. Applications are now open for registration . Application deadline is November 7th. If you have any queries, please reach out to the Turing Commons project organiser, Claudia Fischer . We look forward to seeing you in AI Ethics and Governance soon! Some recent(ish) examples are the novels Machines like Me by Ian McEwan, Klara and the Sun by Kazuo Ishiguro, and the HBO TV show Westworld . \u21a9","title":"About the Course"},{"location":"blog/posts/public-engagement/","tags":["public engagement"],"text":"Peeling Back the Layers of Science and Technology \u00b6 In April 2022, we ran our first week-long course on Public Engagement of Data Science and AI with 25 fantastic researchers from across the globe. This post explores why we developed the course, outlines the activities we ran with the course participants, and concludes with next steps. Your browser does not support the video tag. \"Any sufficiently advanced technology is indistinguishable from magic.\" Arthur C. Clarke's phrase was made famous in the context of his science fiction stories, which were full of futuristic and magical-seeming technologies and alien worlds that captured the imagination of many of his readers. But the advances of science and technological innovation that we see all around us in our everyday lives can also seem magical at times, as well as feeling bewildering and obscure. For many researchers and developers, a key goal of public engagement is about supporting people to dispel the feeling of bewilderment through increased knowledge and understanding, while still maintaining a feeling of enthusiasm and excitement about the many possibilities and opportunities to use science and technology to improve society. This is no easy task! Course Summary \u00b6 Therefore, over the course of a week in April 2022, 30 participants from different research disciplines joined a course being run as part of the Turing Commons project on 'Public Engagement of Data Science and AI'. It was organised around a series of structured presentations, seminars, and group activities, as well as a capstone project which involved a hypothetical public engagement project. The course was designed with several learning objectives in mind, roughly split into two main sections. First, participants critically examined the practical and ethical values of public engagement with data science and AI. They explored what public engagement is and the different forms that it can take, as a way of creating a robust theoretical foundation. Next, the course turned to practical methods and tools for engaging with the public responsibly, such as how to carry out stakeholder analysis activities, structure a clear and accessible message, or communicate uncertainty in research. To support these learning objectives, Sir David Spiegelhalter \u2014Chair of the Winton Centre for Risk and Evidence Communication at the University of Cambridge\u2014gave an excellent talk on trustworthy communication about data and algorithms. His presentation focused on the difficulties of interpreting (and therefore communicating) data and statistics, drawing on many examples from his public engagement work during the Covid-19 pandemic. Designing a Hypothetical Case Study \u00b6 A primary goal for the course participants was to develop a hypothetical public engagement project by the end of the course. Working in three smaller groups, the participants built upon the material covered each day through a series of activities that tested the skills they were acquiring in the course while also providing an opportunity to share their different perspectives with one another. One of these activities was an in-depth stakeholder analysis focused on identifying, understanding, and analysing stakeholders, led by Cami Rinc\u00f3n (part of the Ethics and Responsible Innovation team). The exercise involved conducting a mock analysis intended to get participants thinking about the questions and issues they would confront were they to engage with stakeholders in a real project. For this, they had to identify affected stakeholders from a set of personas, build out their specific characteristics, and then scope the potential impacts of the project on them. From there, and taking into account the different roadblocks and enablers that could impact potential stakeholders, the groups decided on a public engagement objective as well as which of the various public engagement method(s) they would use to achieve their goals. Example of hypothetical personas identified as impacted stakeholders for a group working on a skin condition triaging app. On the final day of the course, the three groups presented their hypothetical project, supported by illustrations they had also co-designed with Eleonore Guerra . Illustration created by Eleonore Guerra for group working on a app which allows the local community to voice their views about local transport infrastructure. During their capstone presentations, the groups introduced their hypothetical project, and discussed its main ethical implications. They also presented their stakeholder personas and the project's potential impacts on them. Taking all of this into consideration, they then defined their public engagement objective and method, and explained how they would (hypothetically) carry it out. Public engagement workshop plan taking into account the potential concerns of impacted stakeholders. After the course ended, we asked participants for anonymous feedback on whether and how the course was useful for them. Overall, the course was incredibly well received: Quote Very clear presentations, guest speakers also added a great depth to the experience. The activities and group work helped understand the course content really well so I would say that the activities are the highlights. The teaching was very well thought out and enjoyed the interactivity aspect of the training. Concluding Remarks \u00b6 Public engagement is sometimes treated as an after-thought by researchers and scientists\u2014something that is important for funding or building a career, but mainly an ancillary activity that exists at the borders of research and development. As such, opportunities to learn about its social and ethical value, and also put principles into practice, are limited. This was the first time that this course was run, but as we continue to build out the content of the Turing Commons and grow its community, we hope that we will be able to both support and learn from more participants. In the meantime, a huge thanks to the first group who made the course such a pleasure to run.","title":"Peeling Back the Layers of Science and Technology"},{"location":"blog/posts/public-engagement/#peeling-back-the-layers-of-science-and-technology","text":"In April 2022, we ran our first week-long course on Public Engagement of Data Science and AI with 25 fantastic researchers from across the globe. This post explores why we developed the course, outlines the activities we ran with the course participants, and concludes with next steps. Your browser does not support the video tag. \"Any sufficiently advanced technology is indistinguishable from magic.\" Arthur C. Clarke's phrase was made famous in the context of his science fiction stories, which were full of futuristic and magical-seeming technologies and alien worlds that captured the imagination of many of his readers. But the advances of science and technological innovation that we see all around us in our everyday lives can also seem magical at times, as well as feeling bewildering and obscure. For many researchers and developers, a key goal of public engagement is about supporting people to dispel the feeling of bewilderment through increased knowledge and understanding, while still maintaining a feeling of enthusiasm and excitement about the many possibilities and opportunities to use science and technology to improve society. This is no easy task!","title":"Peeling Back the Layers of Science and Technology"},{"location":"blog/posts/public-engagement/#course-summary","text":"Therefore, over the course of a week in April 2022, 30 participants from different research disciplines joined a course being run as part of the Turing Commons project on 'Public Engagement of Data Science and AI'. It was organised around a series of structured presentations, seminars, and group activities, as well as a capstone project which involved a hypothetical public engagement project. The course was designed with several learning objectives in mind, roughly split into two main sections. First, participants critically examined the practical and ethical values of public engagement with data science and AI. They explored what public engagement is and the different forms that it can take, as a way of creating a robust theoretical foundation. Next, the course turned to practical methods and tools for engaging with the public responsibly, such as how to carry out stakeholder analysis activities, structure a clear and accessible message, or communicate uncertainty in research. To support these learning objectives, Sir David Spiegelhalter \u2014Chair of the Winton Centre for Risk and Evidence Communication at the University of Cambridge\u2014gave an excellent talk on trustworthy communication about data and algorithms. His presentation focused on the difficulties of interpreting (and therefore communicating) data and statistics, drawing on many examples from his public engagement work during the Covid-19 pandemic.","title":"Course Summary"},{"location":"blog/posts/public-engagement/#designing-a-hypothetical-case-study","text":"A primary goal for the course participants was to develop a hypothetical public engagement project by the end of the course. Working in three smaller groups, the participants built upon the material covered each day through a series of activities that tested the skills they were acquiring in the course while also providing an opportunity to share their different perspectives with one another. One of these activities was an in-depth stakeholder analysis focused on identifying, understanding, and analysing stakeholders, led by Cami Rinc\u00f3n (part of the Ethics and Responsible Innovation team). The exercise involved conducting a mock analysis intended to get participants thinking about the questions and issues they would confront were they to engage with stakeholders in a real project. For this, they had to identify affected stakeholders from a set of personas, build out their specific characteristics, and then scope the potential impacts of the project on them. From there, and taking into account the different roadblocks and enablers that could impact potential stakeholders, the groups decided on a public engagement objective as well as which of the various public engagement method(s) they would use to achieve their goals. Example of hypothetical personas identified as impacted stakeholders for a group working on a skin condition triaging app. On the final day of the course, the three groups presented their hypothetical project, supported by illustrations they had also co-designed with Eleonore Guerra . Illustration created by Eleonore Guerra for group working on a app which allows the local community to voice their views about local transport infrastructure. During their capstone presentations, the groups introduced their hypothetical project, and discussed its main ethical implications. They also presented their stakeholder personas and the project's potential impacts on them. Taking all of this into consideration, they then defined their public engagement objective and method, and explained how they would (hypothetically) carry it out. Public engagement workshop plan taking into account the potential concerns of impacted stakeholders. After the course ended, we asked participants for anonymous feedback on whether and how the course was useful for them. Overall, the course was incredibly well received: Quote Very clear presentations, guest speakers also added a great depth to the experience. The activities and group work helped understand the course content really well so I would say that the activities are the highlights. The teaching was very well thought out and enjoyed the interactivity aspect of the training.","title":"Designing a Hypothetical Case Study"},{"location":"blog/posts/public-engagement/#concluding-remarks","text":"Public engagement is sometimes treated as an after-thought by researchers and scientists\u2014something that is important for funding or building a career, but mainly an ancillary activity that exists at the borders of research and development. As such, opportunities to learn about its social and ethical value, and also put principles into practice, are limited. This was the first time that this course was run, but as we continue to build out the content of the Turing Commons and grow its community, we hope that we will be able to both support and learn from more participants. In the meantime, a huge thanks to the first group who made the course such a pleasure to run.","title":"Concluding Remarks"},{"location":"blog/posts/roadmap/","tags":["objectives"],"text":"Turing Commons Roadmap \u00b6 This blog post sets out a general roadmap and explores a series of objectives for the Turing Commons as we head into 2023. The Turing Commons started out, as all good projects do, with an idea and a name, presented to the Turing's Data Ethics Group in the Winter of 2019 (:tophat: to David Leslie and Christina Hitrova). With the important decision out of the way, our plan was to build a community platform (the \u201ccommons\u201d) to host and support a set of resources that were freely open and accessible to all people with an interest in the ways that data-driven technologies are changing society. Those who are familiar with Garret Hardin\u2019s influential analysis of the \u2018Tragedy of the Commons\u2019 2 , will appreciate why the \u201ccommons\u201d cannot be left unmanaged if it is to serve a sustainable and collective benefit. However, such governance or curation should not occur in the dark, in case it ends up serving the vested interests of a small minority. So, in the spirit of transparency and openness, this post sets out our current roadmap for how and where we plan to develop the Turing Commons, and how we will work with others to ensure that the resources are co-designed to serve genuine needs and challenges. Goals and Objectives \u00b6 From the start of this project, a key goal has been to develop high-quality resources for academic researchers whose work involves the design, development, or evaluation or data-driven technologies, including machine learning or artificial intelligence. To that end, we have produced and delivered three courses on the following topics: Responsible Research and Innovation Public Engagement for Data Science and AI AI Ethics and Governance For each course we started by hosting a series of workshops with 10-15 researchers to identify the topics, questions, and issues which were most important to them. The feedback we received from these workshops was used to develop our original courses. We have learned a lot from planning and delivering these courses. Following the delivery of our first course on Responsible Research and Innovation , we sought feedback from the participants, including a specific request for recommendations for improvements. Several participants suggested more time to explore the new ideas being presented. In response, we redesigned our second course to have a better balance between time spent delivering new material and time spent exploring the material during activities or group discussion. Others recommended closer integration of our activities with the core material. For this, we ensured that our next course has a clear thread which ran through each of the days, progressively building up to a capstone activity. 1 The benefit of these small but important changes was clear in the feedback from our second course: Participant Feedback The activities and group work helped understand the course content really well so I would say that the activities are the highlights. Being responsive to the changing needs of our course participants is a key objective for us, and is central to the above goal of creating high quality (and needs driven) resources. However, designing content and resources that are valuable for all of our participants has been challenging given the multi-disciplinary setting of our courses. For example, if we provided an illustrative example to help explain a key concept, those participants who happened to have a background in the respective area (e.g. healthcare) were able to grasp the idea more readily. Addressing this challenge leads us to the first of our new objectives. Modular and Tailored Resources \u00b6 At present, we are revisiting the content of our three courses and focusing on the following set of objectives: First Objectives Revise the content of each course based on what worked well, what did not, and what needs updating. Modularise the courses to allow more flexibility for participants who are unable or do not wish to take a 5-day long course. Design a more flexible set of materials and resources that can be tailored to different disciplines. Let\u2019s look at the (2) and (3). Asking researchers to block five days from their schedules to attend a course is demanding, and can also be exclusionary for some (e.g. those with parenting responsibilities, or part-time jobs). Designing our courses in this way allowed us to test our material as a whole, but our next step is to modularise the courses to enable a more flexible approach that supports different modes of learning. To this end, we have started splitting our original courses into three main components: Core modules: the primary topic areas covered in our courses Optional modules: additional content that can be brought across from other courses, or take specific concepts in different directions (e.g. data privacy and protection) Activities: the set of activities (both self-directed and group-based) that help users understand the module\u2019s topics Case Studies: illustrative case studies that anchor the topics in concrete cases and practical examples Separating these components allows us to design and develop a modular approach where users can tailor our content and resources to their own skills and training needs. For example, a researcher in robotics could select 3 of our recommended (core) modules on public engagement, combine them with introductory modules on AI ethics (from a separate course) and supplement them with the recommended activities and domain-specific case studies to support their learning. This results in a modular approach to building \u2018skills tracks\u2019 depicted in Figure 1. Figure 1: a schematic depicting the modular approach to our skills tracks, comprising core (and optional) modules, activities, and case studies In addition to being more flexible and tailored, this also enables us to focus on novel approaches to self-directed learning, which has so far been absent on our platform (e.g. creating an online learning environment for individuals who cannot attend hosted courses). While this sets out our general approach, there is an important component missing... who is involved in the redesign and redevelopment? Participatory Design and User Engagement \u00b6 Developing domain-specific case studies that can be used to tailor core modules to the needs of researchers in different disciplines requires domain-specific expertise. Therefore, the redesign and redevelopment of our content and resources has to be conducted in a participatory manner. Recently, we have started collaborating with the UKRI CDT in Biomedical AI (University of Edinburgh) and the UKRI CDT in AI for the study of Environmental Risks (University of Cambridge) to develop and evaluate two of our newly designed core modules for Responsible Research and Innovation. This participatory design work will include the co-creation of tailored case studies for their respective domains (e.g. predictive diagnostics for healthcare, earth monitoring and surveillance technologies). It will also allow us to work with domain experts to identify the most pressing needs and challenges related to skills and training gaps in responsible research and innovation. A key output from our planned workshops will be to evaluate two core modules on \u2018AI Fairness\u2019 and \u2018Explainable AI\u2019, which will be adapted to the context of the two Centres for Doctoral Training (CDT). In addition, we will evaluate activities for self-directed learning and group activities. These activities will also make use of the case studies (to be co-developed) that will help ground the content in practical and domain-specific examples. Although limited to two domains during our pilot phase, we intend to explore further areas following our initial evaluations. However, scaling in this way can be time-consuming and slow, so we are also researching and developing a new technical infrastructure to support the process. Building an Open Infrastructure \u00b6 Our current website is hosted by GitHub pages, and uses the fantastic Material for MkDocs as a static-site generator. This set-up has allowed us to focus on creating written content using Markdown, rather than worrying about web development. However, there are limitations to static sites, and some of these limitations also serve as barriers to our next objective: Second Objective Create an open platform that supports interactive and self-directed learning approaches that are customisable to different users and groups. As mentioned in the previous section a key milestone on our project\u2019s roadmap is the development of domain-specific case studies that enable users to tailor our core modules to their domain (e.g. robotics or journalism). Another milestone for our future roadmap is to develop a case study repository and API to build additional functionality into our website. Doing so will allow users to better tailor skills tracks to their needs by using the available case studies and modules that ground the content of the modules. And, where there are gaps in our resources, an open API will allow partners to more easily integrate their own contributions into the platform, in turn supporting and caring for the commons. This goal requires us to do two things: Develop a database (or data store) to serve case study data to our front-end Enable more interactivity in our website, which uses the database to present specific content to users based upon pre-specific information and preferences, such as Disciplinary background Research priorities Prior knowledge of topics Time available for learning Figure 2 shows how this could work if using something like a JSON file to dynamically adjust elements of a template for our case studies. Figure 2: an example of how a JSON file could populate a HTML document for a case study on decision support systems However, the modular design of these case studies will also allow for partial updating of additional parts of our website (e.g. information boxes in our online guidebooks). Meeting these two objectives can be achieved using our current infrastructure as a proof-of-concept. However, our longer term plans will require a more thorough redesign to add features such as the following: Public API access to allow users to submit new case studies to our repository (i.e. feeding into the \u201ccommons\u201d) User authentication to allow individuals to keep track of their progress, store notes, and build new skills tracks that are customisable to their needs Improved interactivity for activities and learning that rely on modern web frameworks with state management functionality (e.g. React) Enabling communication and collaboration between users through our platform Implementing these features will take time and resources, and we hope to ensure our roadmap remains open to external collaborators who can steer the project in new an exciting directions. This brings us to our final objective. Expanding the Commons \u00b6 Third Objective To further build on our current resources to meet the needs of additional groups and communities. Many of the topics and issues we have explored in our courses so far have importance beyond the academic community. We know this from some of our own public engagement work , as well as from discussions we have had with partners from the public, private and third sectors. Although we have started with an academic audience, this is not our end point. The following list contains examples of groups who we also wish to build resources with and for: Regulators and Policy-Makers Journalists and Science Communicators Members of the Public Industry Professionals (e.g. developers and data analysts) While there are topics in our current resources that will be of interest to some of these groups (e.g. public engagement for journalists; AI fairness for regulators), we cannot assume that the needs of the academic community will be met by our current offerings. Therefore, in the near future we have plans to work with these groups to co-design materials and resources that make the ethical, social, and legal issues of data-driven technologies clear and accessible to all. We have also recently taken part in a workshop with the UK Government\u2019s Science and Innovation Network , organised by the Turing\u2019s International team. The purpose of this workshop was to identify ways that we can work with international partners to support the global community and in turn co-create bidirectional forms of value. By expanding the scope of the Turing Commons in the above ways, we hope that our first objective also feeds into a wider-reaching goal. Overarching Goal To equip diverse groups and communities with the knowledge and understanding of data-driven technologies so they are able to participate fully in democratic forms of deliberation about how these technologies should be used to benefit individuals and society. We\u2019re excited about these plans and developments, and especially to working with new partners who are passionate about creating a responsible and trustworthy ecosystem for data-driven technologies. How can you get involved? \u00b6 At the moment, we are still working on creating structured forms of support to allow interested parties to get involved. For instance, we will be releasing updates on this blog and on social media over the coming weeks with details of how to contribute to this blog directly. For the time being, the best way to get involved is to just to reach out to us and start a conversation. Do you have an idea for a case study? Do you want to take part in one of our evaluations? Do you have time to help us maintain the current GitHub repository? See this blog post for more information. \u21a9 Hardin, G. (1968). The Tragedy of the Commons. Science, 162(3859), 1243\u20131248. http://www.jstor.org/stable/1724745 \u21a9","title":"Turing Commons Roadmap"},{"location":"blog/posts/roadmap/#turing-commons-roadmap","text":"This blog post sets out a general roadmap and explores a series of objectives for the Turing Commons as we head into 2023. The Turing Commons started out, as all good projects do, with an idea and a name, presented to the Turing's Data Ethics Group in the Winter of 2019 (:tophat: to David Leslie and Christina Hitrova). With the important decision out of the way, our plan was to build a community platform (the \u201ccommons\u201d) to host and support a set of resources that were freely open and accessible to all people with an interest in the ways that data-driven technologies are changing society. Those who are familiar with Garret Hardin\u2019s influential analysis of the \u2018Tragedy of the Commons\u2019 2 , will appreciate why the \u201ccommons\u201d cannot be left unmanaged if it is to serve a sustainable and collective benefit. However, such governance or curation should not occur in the dark, in case it ends up serving the vested interests of a small minority. So, in the spirit of transparency and openness, this post sets out our current roadmap for how and where we plan to develop the Turing Commons, and how we will work with others to ensure that the resources are co-designed to serve genuine needs and challenges.","title":"Turing Commons Roadmap"},{"location":"blog/posts/roadmap/#goals-and-objectives","text":"From the start of this project, a key goal has been to develop high-quality resources for academic researchers whose work involves the design, development, or evaluation or data-driven technologies, including machine learning or artificial intelligence. To that end, we have produced and delivered three courses on the following topics: Responsible Research and Innovation Public Engagement for Data Science and AI AI Ethics and Governance For each course we started by hosting a series of workshops with 10-15 researchers to identify the topics, questions, and issues which were most important to them. The feedback we received from these workshops was used to develop our original courses. We have learned a lot from planning and delivering these courses. Following the delivery of our first course on Responsible Research and Innovation , we sought feedback from the participants, including a specific request for recommendations for improvements. Several participants suggested more time to explore the new ideas being presented. In response, we redesigned our second course to have a better balance between time spent delivering new material and time spent exploring the material during activities or group discussion. Others recommended closer integration of our activities with the core material. For this, we ensured that our next course has a clear thread which ran through each of the days, progressively building up to a capstone activity. 1 The benefit of these small but important changes was clear in the feedback from our second course: Participant Feedback The activities and group work helped understand the course content really well so I would say that the activities are the highlights. Being responsive to the changing needs of our course participants is a key objective for us, and is central to the above goal of creating high quality (and needs driven) resources. However, designing content and resources that are valuable for all of our participants has been challenging given the multi-disciplinary setting of our courses. For example, if we provided an illustrative example to help explain a key concept, those participants who happened to have a background in the respective area (e.g. healthcare) were able to grasp the idea more readily. Addressing this challenge leads us to the first of our new objectives.","title":"Goals and Objectives"},{"location":"blog/posts/roadmap/#modular-and-tailored-resources","text":"At present, we are revisiting the content of our three courses and focusing on the following set of objectives: First Objectives Revise the content of each course based on what worked well, what did not, and what needs updating. Modularise the courses to allow more flexibility for participants who are unable or do not wish to take a 5-day long course. Design a more flexible set of materials and resources that can be tailored to different disciplines. Let\u2019s look at the (2) and (3). Asking researchers to block five days from their schedules to attend a course is demanding, and can also be exclusionary for some (e.g. those with parenting responsibilities, or part-time jobs). Designing our courses in this way allowed us to test our material as a whole, but our next step is to modularise the courses to enable a more flexible approach that supports different modes of learning. To this end, we have started splitting our original courses into three main components: Core modules: the primary topic areas covered in our courses Optional modules: additional content that can be brought across from other courses, or take specific concepts in different directions (e.g. data privacy and protection) Activities: the set of activities (both self-directed and group-based) that help users understand the module\u2019s topics Case Studies: illustrative case studies that anchor the topics in concrete cases and practical examples Separating these components allows us to design and develop a modular approach where users can tailor our content and resources to their own skills and training needs. For example, a researcher in robotics could select 3 of our recommended (core) modules on public engagement, combine them with introductory modules on AI ethics (from a separate course) and supplement them with the recommended activities and domain-specific case studies to support their learning. This results in a modular approach to building \u2018skills tracks\u2019 depicted in Figure 1. Figure 1: a schematic depicting the modular approach to our skills tracks, comprising core (and optional) modules, activities, and case studies In addition to being more flexible and tailored, this also enables us to focus on novel approaches to self-directed learning, which has so far been absent on our platform (e.g. creating an online learning environment for individuals who cannot attend hosted courses). While this sets out our general approach, there is an important component missing... who is involved in the redesign and redevelopment?","title":"Modular and Tailored Resources"},{"location":"blog/posts/roadmap/#participatory-design-and-user-engagement","text":"Developing domain-specific case studies that can be used to tailor core modules to the needs of researchers in different disciplines requires domain-specific expertise. Therefore, the redesign and redevelopment of our content and resources has to be conducted in a participatory manner. Recently, we have started collaborating with the UKRI CDT in Biomedical AI (University of Edinburgh) and the UKRI CDT in AI for the study of Environmental Risks (University of Cambridge) to develop and evaluate two of our newly designed core modules for Responsible Research and Innovation. This participatory design work will include the co-creation of tailored case studies for their respective domains (e.g. predictive diagnostics for healthcare, earth monitoring and surveillance technologies). It will also allow us to work with domain experts to identify the most pressing needs and challenges related to skills and training gaps in responsible research and innovation. A key output from our planned workshops will be to evaluate two core modules on \u2018AI Fairness\u2019 and \u2018Explainable AI\u2019, which will be adapted to the context of the two Centres for Doctoral Training (CDT). In addition, we will evaluate activities for self-directed learning and group activities. These activities will also make use of the case studies (to be co-developed) that will help ground the content in practical and domain-specific examples. Although limited to two domains during our pilot phase, we intend to explore further areas following our initial evaluations. However, scaling in this way can be time-consuming and slow, so we are also researching and developing a new technical infrastructure to support the process.","title":"Participatory Design and User Engagement"},{"location":"blog/posts/roadmap/#building-an-open-infrastructure","text":"Our current website is hosted by GitHub pages, and uses the fantastic Material for MkDocs as a static-site generator. This set-up has allowed us to focus on creating written content using Markdown, rather than worrying about web development. However, there are limitations to static sites, and some of these limitations also serve as barriers to our next objective: Second Objective Create an open platform that supports interactive and self-directed learning approaches that are customisable to different users and groups. As mentioned in the previous section a key milestone on our project\u2019s roadmap is the development of domain-specific case studies that enable users to tailor our core modules to their domain (e.g. robotics or journalism). Another milestone for our future roadmap is to develop a case study repository and API to build additional functionality into our website. Doing so will allow users to better tailor skills tracks to their needs by using the available case studies and modules that ground the content of the modules. And, where there are gaps in our resources, an open API will allow partners to more easily integrate their own contributions into the platform, in turn supporting and caring for the commons. This goal requires us to do two things: Develop a database (or data store) to serve case study data to our front-end Enable more interactivity in our website, which uses the database to present specific content to users based upon pre-specific information and preferences, such as Disciplinary background Research priorities Prior knowledge of topics Time available for learning Figure 2 shows how this could work if using something like a JSON file to dynamically adjust elements of a template for our case studies. Figure 2: an example of how a JSON file could populate a HTML document for a case study on decision support systems However, the modular design of these case studies will also allow for partial updating of additional parts of our website (e.g. information boxes in our online guidebooks). Meeting these two objectives can be achieved using our current infrastructure as a proof-of-concept. However, our longer term plans will require a more thorough redesign to add features such as the following: Public API access to allow users to submit new case studies to our repository (i.e. feeding into the \u201ccommons\u201d) User authentication to allow individuals to keep track of their progress, store notes, and build new skills tracks that are customisable to their needs Improved interactivity for activities and learning that rely on modern web frameworks with state management functionality (e.g. React) Enabling communication and collaboration between users through our platform Implementing these features will take time and resources, and we hope to ensure our roadmap remains open to external collaborators who can steer the project in new an exciting directions. This brings us to our final objective.","title":"Building an Open Infrastructure"},{"location":"blog/posts/roadmap/#expanding-the-commons","text":"Third Objective To further build on our current resources to meet the needs of additional groups and communities. Many of the topics and issues we have explored in our courses so far have importance beyond the academic community. We know this from some of our own public engagement work , as well as from discussions we have had with partners from the public, private and third sectors. Although we have started with an academic audience, this is not our end point. The following list contains examples of groups who we also wish to build resources with and for: Regulators and Policy-Makers Journalists and Science Communicators Members of the Public Industry Professionals (e.g. developers and data analysts) While there are topics in our current resources that will be of interest to some of these groups (e.g. public engagement for journalists; AI fairness for regulators), we cannot assume that the needs of the academic community will be met by our current offerings. Therefore, in the near future we have plans to work with these groups to co-design materials and resources that make the ethical, social, and legal issues of data-driven technologies clear and accessible to all. We have also recently taken part in a workshop with the UK Government\u2019s Science and Innovation Network , organised by the Turing\u2019s International team. The purpose of this workshop was to identify ways that we can work with international partners to support the global community and in turn co-create bidirectional forms of value. By expanding the scope of the Turing Commons in the above ways, we hope that our first objective also feeds into a wider-reaching goal. Overarching Goal To equip diverse groups and communities with the knowledge and understanding of data-driven technologies so they are able to participate fully in democratic forms of deliberation about how these technologies should be used to benefit individuals and society. We\u2019re excited about these plans and developments, and especially to working with new partners who are passionate about creating a responsible and trustworthy ecosystem for data-driven technologies.","title":"Expanding the Commons"},{"location":"blog/posts/roadmap/#how-can-you-get-involved","text":"At the moment, we are still working on creating structured forms of support to allow interested parties to get involved. For instance, we will be releasing updates on this blog and on social media over the coming weeks with details of how to contribute to this blog directly. For the time being, the best way to get involved is to just to reach out to us and start a conversation. Do you have an idea for a case study? Do you want to take part in one of our evaluations? Do you have time to help us maintain the current GitHub repository? See this blog post for more information. \u21a9 Hardin, G. (1968). The Tragedy of the Commons. Science, 162(3859), 1243\u20131248. http://www.jstor.org/stable/1724745 \u21a9","title":"How can you get involved?"},{"location":"ped/","text":"About this Course \u00b6 This course is designed to help you understand the practical and ethical value of public engagement with data science and AI. The course begins with an introduction to different forms of public engagement, while critically examining the different methods and approaches. Then, through a series of structured seminars and workshops, you will consider the impact of public engagement upon both practices of research and innovation as well as society more broadly. Following this general introduction, the course pivots to introduce and discuss practical methods of public engagement, including deliberative activities that help build consensus among stakeholder; transparent and explainable methods of data governance to support project activities; methods of data visualisation to support the communication of science and technology; and an awareness of social and psychological biases, which can negatively affect the goals of responsible public engagement. Table of Contents \u00b6 :material-chat-question:{ .lg .middle } What is Public Engagement? This chapter looks at foundational concepts and topics, including what public engagement is and what the various goals of public engagement are. :octicons-arrow-right-24: Go to chapter :material-scale-balance:{ .lg .middle } The Value(s) of Public Engagement This chapter explores and critically examines the ethical and social values that motivate and underpin public engagament. :octicons-arrow-right-24: Go to chapter :fontawesome-solid-people-group:{ .lg .middle } Facilitating Public Engagement This chapter introduces a wide variety of practical methods and processes for designing and facilitating public engagement in data science and AI. :octicons-arrow-right-24: Go to chapter :material-chat-processing:{ .lg .middle } Public Communication This chapter addresses some challenges that arise in the context of public communication (e.g., cognitive biases and scientific uncertainty) and considers ways to overcome them. :octicons-arrow-right-24: Go to chapter :fontawesome-solid-handshake:{ .lg .middle } Public Trust and Assurance The concluding chapter looks at the issue of public trust, exploring matters such as mis/disinformation, social media, and the use of social media for science and technology communication. :octicons-arrow-right-24: Go to chapter Who is this Guidebook For? \u00b6 Primarily, this guidebook is for researchers with an active interest in public engagement, specifically in the context of data science and artificial intelligence. This doesn't mean you have to be a data scientist, or use Python to develop machine learning algorithms. You could also be an ethicist, sociologist, or someone with an interest in law and public policy. This course has practical, and sometimes hands-on activities that are designed to a) encourage critical reflection and b) help you build practical understanding of the processes associated with effective and responsible public engagement in data science and AI. While they can be carried out as part of individual and self-directed learning, they are most suited to group discussion. Learning Objectives \u00b6 This guidebook has the following learning objectives: Critically examine what 'public engagement' is, the goals associated with different types of public engagement, and to identify the associated values. Understand the different stages of public engagement as they apply to the typical activities of a data science or AI research/innovation project. Explore practical methods and activities that can help build more effective forms of public engagement. Identify the elements of public engagement that help build a more trustworthy data and AI ecosystem.","title":"(PED) About this Course"},{"location":"ped/#about-this-course","text":"This course is designed to help you understand the practical and ethical value of public engagement with data science and AI. The course begins with an introduction to different forms of public engagement, while critically examining the different methods and approaches. Then, through a series of structured seminars and workshops, you will consider the impact of public engagement upon both practices of research and innovation as well as society more broadly. Following this general introduction, the course pivots to introduce and discuss practical methods of public engagement, including deliberative activities that help build consensus among stakeholder; transparent and explainable methods of data governance to support project activities; methods of data visualisation to support the communication of science and technology; and an awareness of social and psychological biases, which can negatively affect the goals of responsible public engagement.","title":"About this Course"},{"location":"ped/#table-of-contents","text":":material-chat-question:{ .lg .middle } What is Public Engagement? This chapter looks at foundational concepts and topics, including what public engagement is and what the various goals of public engagement are. :octicons-arrow-right-24: Go to chapter :material-scale-balance:{ .lg .middle } The Value(s) of Public Engagement This chapter explores and critically examines the ethical and social values that motivate and underpin public engagament. :octicons-arrow-right-24: Go to chapter :fontawesome-solid-people-group:{ .lg .middle } Facilitating Public Engagement This chapter introduces a wide variety of practical methods and processes for designing and facilitating public engagement in data science and AI. :octicons-arrow-right-24: Go to chapter :material-chat-processing:{ .lg .middle } Public Communication This chapter addresses some challenges that arise in the context of public communication (e.g., cognitive biases and scientific uncertainty) and considers ways to overcome them. :octicons-arrow-right-24: Go to chapter :fontawesome-solid-handshake:{ .lg .middle } Public Trust and Assurance The concluding chapter looks at the issue of public trust, exploring matters such as mis/disinformation, social media, and the use of social media for science and technology communication. :octicons-arrow-right-24: Go to chapter","title":"Table of Contents"},{"location":"ped/#who-is-this-guidebook-for","text":"Primarily, this guidebook is for researchers with an active interest in public engagement, specifically in the context of data science and artificial intelligence. This doesn't mean you have to be a data scientist, or use Python to develop machine learning algorithms. You could also be an ethicist, sociologist, or someone with an interest in law and public policy. This course has practical, and sometimes hands-on activities that are designed to a) encourage critical reflection and b) help you build practical understanding of the processes associated with effective and responsible public engagement in data science and AI. While they can be carried out as part of individual and self-directed learning, they are most suited to group discussion.","title":"Who is this Guidebook For?"},{"location":"ped/#learning-objectives","text":"This guidebook has the following learning objectives: Critically examine what 'public engagement' is, the goals associated with different types of public engagement, and to identify the associated values. Understand the different stages of public engagement as they apply to the typical activities of a data science or AI research/innovation project. Explore practical methods and activities that can help build more effective forms of public engagement. Identify the elements of public engagement that help build a more trustworthy data and AI ecosystem.","title":"Learning Objectives"},{"location":"ped/chapter1/","text":"What is Public Engagement? \u00b6 You may think that the answer to this question is so obvious that it is not worth asking the question in the first place. Or, perhaps you would give an answer along these lines: Public engagement is a process of dialogue between researchers and members of the public with the goal of either a) determining the attitudes or beliefs of the public or b) informing them of developments in science and technology. An answer such as this would capture a significant part of what public engagement involves. However, it would also miss some important nuance and detail. The purpose of this section is to develop a more careful and extensive understanding of what public engagement is and entails, which will then serve as the foundation for the remainder of the course. It begins with an introduction to and critical examination of public engagement, before turning to look at what public engagement is and the goals associated with it. The chapter is primarily theoretical in scope, but serves as an important foundation for the more practical chapters later in the guide. Without this foundation, your understanding of some of the concepts in later chapters may be impaired. Therefore, it is only advised for you to skip this chapter if you already have a good understanding of the concept of 'public engagement'. Chapter Outline \u00b6 Climbing the Ladder: From Informing to Empowering Goals of Public Engagement Learning Objectives In this chapter, you will: Learn what is meant by the term \u2018public engagement\u2019. Identify and explore some of the different approaches to public engagement and the goals associated with them. --8<-- \"includes/abbreviations.md\"","title":"Introduction"},{"location":"ped/chapter1/#what-is-public-engagement","text":"You may think that the answer to this question is so obvious that it is not worth asking the question in the first place. Or, perhaps you would give an answer along these lines: Public engagement is a process of dialogue between researchers and members of the public with the goal of either a) determining the attitudes or beliefs of the public or b) informing them of developments in science and technology. An answer such as this would capture a significant part of what public engagement involves. However, it would also miss some important nuance and detail. The purpose of this section is to develop a more careful and extensive understanding of what public engagement is and entails, which will then serve as the foundation for the remainder of the course. It begins with an introduction to and critical examination of public engagement, before turning to look at what public engagement is and the goals associated with it. The chapter is primarily theoretical in scope, but serves as an important foundation for the more practical chapters later in the guide. Without this foundation, your understanding of some of the concepts in later chapters may be impaired. Therefore, it is only advised for you to skip this chapter if you already have a good understanding of the concept of 'public engagement'.","title":"What is Public Engagement?"},{"location":"ped/chapter1/#chapter-outline","text":"Climbing the Ladder: From Informing to Empowering Goals of Public Engagement Learning Objectives In this chapter, you will: Learn what is meant by the term \u2018public engagement\u2019. Identify and explore some of the different approaches to public engagement and the goals associated with them. --8<-- \"includes/abbreviations.md\"","title":"Chapter Outline"},{"location":"ped/chapter1/goals/","text":"Goals of Public Engagement \u00b6 Before we explore some specific goals of public engagement, have a go at answering the following question on your own: Quote What are some goals of public engagement? In answering the above question, you may have come up with a valid goal of public engagement or, alternatively, a different framing for one of the subsequent goals that have been discussed in the relevant literature. Let's look at some of the most significant or influential goals. 1 Improving Public Knowledge and Awareness of Science \u00b6 This first, epistemic goal is often the most intuitive and familiar to scientific researchers and developers. In many cases this is due to institutional factors that promote uni-directional forms of engagement such as blog posts, news articles, or television and radio interviews. For many members of the public, these types of engagement can be both entertaining and informative when they are well produced. However, as already discussed, it is unclear whether this goal is an intrinsic good. That is, should the goal of improved public knowledge in science be treated as something valuable in and of itself, or is the value of improved scientific knowledge instrumental upon the practical benefits that this knowledge can bring? How we answer this question will affect how we evaluate this goal. On the one hand, if we think that improved knowledge is an instrumental good, then we may be sceptical about specific public education campaigns. For example, few users are likely to leverage technical knowledge about how object recognition algorithms work to deploy a system in their own home that can identify common household objects. But on the other hand, if we see improved knowledge as an intrinsic good, then any efforts to raise public awareness in science and technology should be treated as valuable. Public Deliberation \u00b6 This second goal is often associated with Jurgen Habermas\u2014a German philosopher and social theorist\u2014who developed the theory of discourse ethics and saw deliberation as Quote a time-consuming process of mutual enlightenment, for the \u2018general interest\u2019 on the basis of which alone a rational agreement between publicly competing opinions could freely be reached [@habermas1989] The general idea is that deliberation aims at consensus, such that the latter emerges from the sharing of public reasons and a drive towards mutual understanding between those engaged in dialogue. Here, consensus could involve a shared belief and agreement about the benefits or harms of some scientific or technological development. In practice, consensus does not have to involve unanimous agreement on a final outcome. Rather, it can also include acceptable domains or preferences and ranges of competing options, the credibility of disputed beliefs, and the legitimacy of competing values.[@dryzek2006] This avoids the misplaced criticism that deliberation aimed at consensus building results in the flattening of the range of options or covers up dissent and legitimate disagreement or difference. Establishing Trust, Legitimacy, and Social License \u00b6 Closely connected with the goal of consensus building or formation is the goal of establishing trust, legitimacy, and a social license. As a socially-embedded process, science and technology development can often depend on the support of members of the public. This is especially true where science and technology research and development is publicly funded and administered. A clear example of this is the recent case of contact-tracing apps used across the globe in response to the COVID-19 pandemic.[@leslie2020] Many formal healthcare organisations supported contact tracing because of its potential to support epidemiological research. However, the success of the public health programmes depended upon whether members of the public trusted the scientific process, including whether their personal data were handled in a responsible and ethical fashion. The choice of whether to use contact-tracing apps was, therefore, dependant upon a) how trustworthy the app was judged to be, b) whether the operator of the app had perceived legitimacy in the eyes of the public, and c) whether there was broad social license for the app in the first place. Public engagement was seen by many as crucial to establishing trust, legitimacy, and social license.[@aitken2020] Although this may be a valid goal, it is also important to note that like the first goal, it can be treated as an instrumentally valuable goal insofar as it supports subsequent goals, such as improving health outcomes or supporting epidemiological research. Improving Social Welfare \u00b6 Following on from the previous goal, we can also identify the goal of improved social welfare. It is often recognised that science and technology can improve many facets of society and people's lives. Cleaner air and water, improved health, more effective forms of communication, better governance and public policy\u2014all of these social goods can (and have) been improved by science and technology. Where local knowledge is required in order to realise these goals though, it is vital that members of the public (e.g. local communities) are able to participate in the scientific process. However, what constitutes improved 'social welfare' is itself a question that may require public engagement to satisfactorily address. For example, if a research team were exploring whether a drug could improve the health outcomes for a group of patients, they may need to work with these patients in order to understand which side effects or symptoms were most important to them when assessing if the drug had a positive impact on their health or well-being. Universal agreement on whether a medical intervention is a net positive in terms of health and well-being is not something that can easily be assumed. Safeguarding and Supporting Human Rights \u00b6 Consider the following text from the United Nation's Universal Declaration of Human Rights:[@nations2022] Article 27 Everyone has the right freely to participate in the cultural life of the community, to enjoy the arts and to share in scientific advancement and its benefits. Everyone has the right to the protection of the moral and material interests resulting from any scientific, literary or artistic production of which he is the author. The scope of human rights, and the obligations or duties that they impose upon various institutions is a lively area of debate among legal scholars and practitioners. However, when compared to other rights, Article 27 has received little attention. Although enshrined in a legal document, the manner in which these various articles translate into either practical safeguards against possible abuses or positive opportunities and capabilities for human flourishing is far from obvious. Citizen science is one way, among many, in which this right may be exercised. The term 'citizen science' is defined by Vayena and Tasioulas as, Quote [...] any form of active non-professional participation in science that goes beyond human subject research conducted by professional researchers.[@vayena2015] This inclusive definition makes room for myriad scientific activities, ranging from astronomy to zoology, or from self-tracking and experimentation through smart devices (e.g. wearables and smartphones) to participation in distributed forms of environment data collection and mapping 2 . The unprecedented technological means available to members of the public has undoubtedly played a significant role in increasing the participation in citizen science projects. However, the fact that access to science and technology remains unequally distributed throughout society suggests that technological means alone are insufficient for universally establishing the aforementioned human right. Therefore, it is worthwhile considering how such a goal could be better realised. In the next chapter we will look at the preconditions for effectively realising these goals, by exploring the values that support, underwrite, and motivate public engagement. The astute reader will notice that these goals are not mutually exclusive nor exhaustive. For instance, mutual understanding can also be aimed at building trust. \u21a9 see the Colouring London project as one visually-appealing example of crowdsourced data-science. \u21a9","title":"Goals of Public Engagement"},{"location":"ped/chapter1/goals/#goals-of-public-engagement","text":"Before we explore some specific goals of public engagement, have a go at answering the following question on your own: Quote What are some goals of public engagement? In answering the above question, you may have come up with a valid goal of public engagement or, alternatively, a different framing for one of the subsequent goals that have been discussed in the relevant literature. Let's look at some of the most significant or influential goals. 1","title":"Goals of Public Engagement"},{"location":"ped/chapter1/goals/#improving-public-knowledge-and-awareness-of-science","text":"This first, epistemic goal is often the most intuitive and familiar to scientific researchers and developers. In many cases this is due to institutional factors that promote uni-directional forms of engagement such as blog posts, news articles, or television and radio interviews. For many members of the public, these types of engagement can be both entertaining and informative when they are well produced. However, as already discussed, it is unclear whether this goal is an intrinsic good. That is, should the goal of improved public knowledge in science be treated as something valuable in and of itself, or is the value of improved scientific knowledge instrumental upon the practical benefits that this knowledge can bring? How we answer this question will affect how we evaluate this goal. On the one hand, if we think that improved knowledge is an instrumental good, then we may be sceptical about specific public education campaigns. For example, few users are likely to leverage technical knowledge about how object recognition algorithms work to deploy a system in their own home that can identify common household objects. But on the other hand, if we see improved knowledge as an intrinsic good, then any efforts to raise public awareness in science and technology should be treated as valuable.","title":"Improving Public Knowledge and Awareness of Science"},{"location":"ped/chapter1/goals/#public-deliberation","text":"This second goal is often associated with Jurgen Habermas\u2014a German philosopher and social theorist\u2014who developed the theory of discourse ethics and saw deliberation as Quote a time-consuming process of mutual enlightenment, for the \u2018general interest\u2019 on the basis of which alone a rational agreement between publicly competing opinions could freely be reached [@habermas1989] The general idea is that deliberation aims at consensus, such that the latter emerges from the sharing of public reasons and a drive towards mutual understanding between those engaged in dialogue. Here, consensus could involve a shared belief and agreement about the benefits or harms of some scientific or technological development. In practice, consensus does not have to involve unanimous agreement on a final outcome. Rather, it can also include acceptable domains or preferences and ranges of competing options, the credibility of disputed beliefs, and the legitimacy of competing values.[@dryzek2006] This avoids the misplaced criticism that deliberation aimed at consensus building results in the flattening of the range of options or covers up dissent and legitimate disagreement or difference.","title":"Public Deliberation"},{"location":"ped/chapter1/goals/#establishing-trust-legitimacy-and-social-license","text":"Closely connected with the goal of consensus building or formation is the goal of establishing trust, legitimacy, and a social license. As a socially-embedded process, science and technology development can often depend on the support of members of the public. This is especially true where science and technology research and development is publicly funded and administered. A clear example of this is the recent case of contact-tracing apps used across the globe in response to the COVID-19 pandemic.[@leslie2020] Many formal healthcare organisations supported contact tracing because of its potential to support epidemiological research. However, the success of the public health programmes depended upon whether members of the public trusted the scientific process, including whether their personal data were handled in a responsible and ethical fashion. The choice of whether to use contact-tracing apps was, therefore, dependant upon a) how trustworthy the app was judged to be, b) whether the operator of the app had perceived legitimacy in the eyes of the public, and c) whether there was broad social license for the app in the first place. Public engagement was seen by many as crucial to establishing trust, legitimacy, and social license.[@aitken2020] Although this may be a valid goal, it is also important to note that like the first goal, it can be treated as an instrumentally valuable goal insofar as it supports subsequent goals, such as improving health outcomes or supporting epidemiological research.","title":"Establishing Trust, Legitimacy, and Social License"},{"location":"ped/chapter1/goals/#improving-social-welfare","text":"Following on from the previous goal, we can also identify the goal of improved social welfare. It is often recognised that science and technology can improve many facets of society and people's lives. Cleaner air and water, improved health, more effective forms of communication, better governance and public policy\u2014all of these social goods can (and have) been improved by science and technology. Where local knowledge is required in order to realise these goals though, it is vital that members of the public (e.g. local communities) are able to participate in the scientific process. However, what constitutes improved 'social welfare' is itself a question that may require public engagement to satisfactorily address. For example, if a research team were exploring whether a drug could improve the health outcomes for a group of patients, they may need to work with these patients in order to understand which side effects or symptoms were most important to them when assessing if the drug had a positive impact on their health or well-being. Universal agreement on whether a medical intervention is a net positive in terms of health and well-being is not something that can easily be assumed.","title":"Improving Social Welfare"},{"location":"ped/chapter1/goals/#safeguarding-and-supporting-human-rights","text":"Consider the following text from the United Nation's Universal Declaration of Human Rights:[@nations2022] Article 27 Everyone has the right freely to participate in the cultural life of the community, to enjoy the arts and to share in scientific advancement and its benefits. Everyone has the right to the protection of the moral and material interests resulting from any scientific, literary or artistic production of which he is the author. The scope of human rights, and the obligations or duties that they impose upon various institutions is a lively area of debate among legal scholars and practitioners. However, when compared to other rights, Article 27 has received little attention. Although enshrined in a legal document, the manner in which these various articles translate into either practical safeguards against possible abuses or positive opportunities and capabilities for human flourishing is far from obvious. Citizen science is one way, among many, in which this right may be exercised. The term 'citizen science' is defined by Vayena and Tasioulas as, Quote [...] any form of active non-professional participation in science that goes beyond human subject research conducted by professional researchers.[@vayena2015] This inclusive definition makes room for myriad scientific activities, ranging from astronomy to zoology, or from self-tracking and experimentation through smart devices (e.g. wearables and smartphones) to participation in distributed forms of environment data collection and mapping 2 . The unprecedented technological means available to members of the public has undoubtedly played a significant role in increasing the participation in citizen science projects. However, the fact that access to science and technology remains unequally distributed throughout society suggests that technological means alone are insufficient for universally establishing the aforementioned human right. Therefore, it is worthwhile considering how such a goal could be better realised. In the next chapter we will look at the preconditions for effectively realising these goals, by exploring the values that support, underwrite, and motivate public engagement. The astute reader will notice that these goals are not mutually exclusive nor exhaustive. For instance, mutual understanding can also be aimed at building trust. \u21a9 see the Colouring London project as one visually-appealing example of crowdsourced data-science. \u21a9","title":"Safeguarding and Supporting Human Rights"},{"location":"ped/chapter1/ladder/","text":"Climbing the Ladder: From Informing to Empowering \u00b6 In 1969, Sherry Arnstein published 'A Ladder of Citizen Participation' [@arnstein1969]. The eponymous ladder was a typology of different forms of public participation in science and research, which was intended to be both \"provocative\" and also facilitate clearer dialogue on the objectives of different forms of public engagement 1 . Arnstein visualised her typology as a ladder to signify that the higher rungs represent increasing forms of \"citizen power\". In order from lowest to highest, the eight rungs are as follows: Arnstein's Ladder of Public Participation Manipulation Therapy Informing Consultation Placation Partnership Delegated Power Citizen Control With each step up the ladder, we move closer to a genuine form of public participation that seeks to empower citizens through distributed forms of knowledge production and enhanced capabilities. For instance, perhaps a local council sets out to engage and upskill residents to enable them to have greater control over how their data are used to improve local services. Such a project could represent any of the top three rungs, depending on how the project was designed and organised. Like the other rungs, this is because the top three rungs form a separate category that further delineates them from the lower levels. The categories are as follows: Nonparticipation (Manipulation, Therapy) These rungs describe forms of participation that have been \"contrived by some to substitute for genuine participation\". However, the goal of these non-participatory forms of public engagement are often to enable those in power (e.g., researchers) to \"educate\" participants. Tokenism (Informing, Consultation, Placation) These rungs afford participants a voice but only insofar as their views serve the interests of those who hear them. Participants still lack any real sense of power in such forms of engagement, as researchers still make the final decision based on pre-determined goals or values. Citizen Power (Partnership, Delegated Power, Citizen Control) The higher rungs empower participants to an increasing degree. Where members of the public can enter in partnerships with researchers, they will likely be granted autonomy over decisions. However, the extent to which this power is truly delegated or under the control of the participants may be limited at this level. Each step up the ladder from here represents increased power and autonomy over decision-making. Notice that even the lowest levels would, technically, constitute a form of engagement, while failing to count as genuine forms of participation. In the approximately 50 years since this article's publication many have engaged with Arnstein's original proposal, including critical perspectives that emphasise the model's limitations. The ladder is, after all, a simplification\u2014like all heuristic models. As such, it fails to capture variations across policy or research domains, such as education versus healthcare, where participants may not wish or be able to exercise power or autonomy over decisions but could benefit from other forms of empowerment. In addition, the model is unable to provide insights into what can be done to rebalance, rather than outsource, power in specific contexts. Nevertheless, in spite of these limitations, the model that Arnstein developed has been highly influential. For instance, NHS England have a simplified form of Arnstein's ladder, which is better suited to help structure patient and public involvement or activities (e.g., policy decisions). Although we will not use the rungs of the ladder as conceptual reference points in this course, the ethical and social significance of public empowerment will be a recurring theme in this course. However, it is not the only theme that we will consider, and Arnstein's ladder is not the only model that is worth considering. Other Models of Public Engagement \u00b6 Arnstein's ladder draws our attention to the ethical and social value of public empowerment . This is arguably a social good, but is not an unconditional good. That is, there may be cases where increased citizen control is neither desirable nor appropriate (e.g. when working with vulnerable groups, such as those with severe mental health disabilities). As conscientious and responsible researchers, however, we may still wish to do better than mere tokenism and also avoid complete non-participation. In such cases, it is important to have a clear understanding of what our goal for public engagement ought to be. In an article reviewing different approaches to science and technology communication, Bruce Lewinstein identifies four models, which can help us to better understand the goals of public engagement [@lewenstein2003]: The Deficit Model The Contextual Model The Lay Expertise Model The Public Participation Model The Deficit Model \u00b6 The first of these models reflects an assumption that public awareness of and knowledge about science and technology is, in general, poor. Public surveys are often cited in support of such a view, including claims such as the following: Quote [...] only 10 percent of Americans can define \"molecule,\" and that more than half believe that humans and dinosaurs lived on the Earth at the same time. From this starting point, those who adhere to the deficit model presume that attempts to improve public knowledge are invariably a good thing. But are they? Why, for example, do members of the public need to know that the luminosity of the Andromeda galaxy is $~2.6\u00d710^{10}$ $L_\u2609$, or that variational autoencoders are popular types of generative models in artificial intelligence? Moreover, why should the absence of this knowledge be treated as a deficit when it is likely to have no application in the daily lives of members of the public? The Contextual Model \u00b6 The way we respond to information differs depending on the context in which it is presented. Our personality type, for example, may affect how we perceive and evaluate risk. Social and cultural attitudes affect levels of trust in scientific authority. And, the media can play a substantial role in shaping our interest in science and technology\u2014Carl Sagan, for example, was highly renowned for the passion and excitement he was able to generate in often complex or dry scientific topics. The second model Lewinstein describes\u2014the contextual model\u2014therefore, recognises the need for tailoring information to specific audiences. This helps to address the deficit model's own deficit, as describe above. However, we may think that this revised model is just a more sophisticated version of the deficit model\u2014old wine in a new bottle! After all, there is still a presumed gap in understanding, but it needs to be addressed in a specific way. What these two models share is their presumed equivalence of 'public understanding of science and technology' with 'public appreciation of the benefits provided by science and technology to society'. In other words, the more that members of the public understand about science and technology the more they will come to appreciate its benefits. This may be true in some instances. And so the goal of enhanced literacy ought to be recognised as a valid objective for public engagement. However, public engagement under these two models reduces engagement to a uni-directional form of communication in which the goal of the scientist is simply to inform and educate. It is for this reason that such forms of engagement fall under the 'non-participatory' category of Arnstein's ladder. The Lay Expertise Model \u00b6 Turning the spotlight of attention back onto the scientific communities, a further concern with the previous two models that Lewinstein identifies is that they do not adequately address the social or political contexts in which science and technology are developed, including the conflicts of interest that scientific pursuits may have with local communities or expertise (e.g. labour groups). Far from being the sole arbiters of knowledge and expertise, scientists often fall prey to their own biases or limited perspective (i.e. positionality) and overlook diverse forms of knowledge that are rooted in local communities and practices (e.g. agriculture). Such local knowledge, including the data flows interconnected with the embedded processes of local knowledge production, may be as relevant (or even more valuable) than \"technical\" forms when attempting to address (or redefine) some problem. In cases where knowledge resides in local practices and expertise, public engagement may benefit from meaningful partnerships, leading to complementary and mutually enhancing forms of knowledge. A pragmatic goal of such endeavours, therefore, is to solve problems. However, in pursuing such a goal scientists must remain vigilant that their goal and formulation of the problem is not misaligned with the goal of the communities with whom they partner. The Public Participation Model \u00b6 Lewinstein's final model brings us back to Arnstein's ladder. However, the topics that Lewinstein addresses go beyond empowerment to include additional benefits such as heightened public trust and consensus formation. There are myriad activities that can support these objectives, including citizen juries, consensus building workshops, and deliberative polling. Therefore, this model also seeks to identify means for democratising science and technology in order to wrest control of research and innovation away from elite institutions and politicians. Once we understand these different activities, it becomes clear that public participation is not about outsourcing decision-making authority to the public. In other words, the people that participate are not making decisions on behalf of researchers, but their voices should be listened to insofar as they are impacted by the consequences of a research or innovation project. This is because scientific research and the design, development, and deployment of technology embody particular values that may not be shared by those who are directly or indirectly affected by the consequences of such activities. Obvious examples include military science and technology. Can you think of others? The field of science and technology studies has been critically examining scientific research and practice for several decades, exposing many latent value structures and biases in the process. 2 An awareness of such structures and biases is vital to understanding the goals of public engagement. We will explore this value laden nature of science and technology in relation to public engagement later. However, with this additional understanding of public engagement, let us now take a closer look at the goals of objectives of public engagement. Who (or what) are the \"Public\"? Before going any further, we should also stop to address a conceptual point about what the concept 'the Public' actually references. It is easy to fall into the trap of treating \"the public\" as some monolithic or homogenous entity, because of how its usage as a mass noun affects our judgement. However, as we all know, the public are a wonderfully diverse set of people, communities, and groups, of which we are a part. The more we can keep this in the forefront of our minds going forward, the less likely we will be to fall prey to myopic assumptions about public engagement. A note on the terms 'participation' and 'engagement': in some cases these terms can be used interchangeably. For instance, public participation is a form of engagement. However, the reverse is not always true, as Arnstein's ladder demonstrates. For instance, informing members of the public about novel scientific results or findings through print communication is best seen as engagement, rather than than participation. This is because the reader is unable to participate in the scientific process, even if they have been engaged in the later stages (i.e. scientific communication). Throughout this guide, we will default to the use of 'engagement' where an inclusive usage is appropriate. \u21a9 For an overview of key developments in science and technology studies, see this timeline \u21a9","title":"Climbing the Ladder"},{"location":"ped/chapter1/ladder/#climbing-the-ladder-from-informing-to-empowering","text":"In 1969, Sherry Arnstein published 'A Ladder of Citizen Participation' [@arnstein1969]. The eponymous ladder was a typology of different forms of public participation in science and research, which was intended to be both \"provocative\" and also facilitate clearer dialogue on the objectives of different forms of public engagement 1 . Arnstein visualised her typology as a ladder to signify that the higher rungs represent increasing forms of \"citizen power\". In order from lowest to highest, the eight rungs are as follows: Arnstein's Ladder of Public Participation Manipulation Therapy Informing Consultation Placation Partnership Delegated Power Citizen Control With each step up the ladder, we move closer to a genuine form of public participation that seeks to empower citizens through distributed forms of knowledge production and enhanced capabilities. For instance, perhaps a local council sets out to engage and upskill residents to enable them to have greater control over how their data are used to improve local services. Such a project could represent any of the top three rungs, depending on how the project was designed and organised. Like the other rungs, this is because the top three rungs form a separate category that further delineates them from the lower levels. The categories are as follows: Nonparticipation (Manipulation, Therapy) These rungs describe forms of participation that have been \"contrived by some to substitute for genuine participation\". However, the goal of these non-participatory forms of public engagement are often to enable those in power (e.g., researchers) to \"educate\" participants. Tokenism (Informing, Consultation, Placation) These rungs afford participants a voice but only insofar as their views serve the interests of those who hear them. Participants still lack any real sense of power in such forms of engagement, as researchers still make the final decision based on pre-determined goals or values. Citizen Power (Partnership, Delegated Power, Citizen Control) The higher rungs empower participants to an increasing degree. Where members of the public can enter in partnerships with researchers, they will likely be granted autonomy over decisions. However, the extent to which this power is truly delegated or under the control of the participants may be limited at this level. Each step up the ladder from here represents increased power and autonomy over decision-making. Notice that even the lowest levels would, technically, constitute a form of engagement, while failing to count as genuine forms of participation. In the approximately 50 years since this article's publication many have engaged with Arnstein's original proposal, including critical perspectives that emphasise the model's limitations. The ladder is, after all, a simplification\u2014like all heuristic models. As such, it fails to capture variations across policy or research domains, such as education versus healthcare, where participants may not wish or be able to exercise power or autonomy over decisions but could benefit from other forms of empowerment. In addition, the model is unable to provide insights into what can be done to rebalance, rather than outsource, power in specific contexts. Nevertheless, in spite of these limitations, the model that Arnstein developed has been highly influential. For instance, NHS England have a simplified form of Arnstein's ladder, which is better suited to help structure patient and public involvement or activities (e.g., policy decisions). Although we will not use the rungs of the ladder as conceptual reference points in this course, the ethical and social significance of public empowerment will be a recurring theme in this course. However, it is not the only theme that we will consider, and Arnstein's ladder is not the only model that is worth considering.","title":"Climbing the Ladder: From Informing to Empowering"},{"location":"ped/chapter1/ladder/#other-models-of-public-engagement","text":"Arnstein's ladder draws our attention to the ethical and social value of public empowerment . This is arguably a social good, but is not an unconditional good. That is, there may be cases where increased citizen control is neither desirable nor appropriate (e.g. when working with vulnerable groups, such as those with severe mental health disabilities). As conscientious and responsible researchers, however, we may still wish to do better than mere tokenism and also avoid complete non-participation. In such cases, it is important to have a clear understanding of what our goal for public engagement ought to be. In an article reviewing different approaches to science and technology communication, Bruce Lewinstein identifies four models, which can help us to better understand the goals of public engagement [@lewenstein2003]: The Deficit Model The Contextual Model The Lay Expertise Model The Public Participation Model","title":"Other Models of Public Engagement"},{"location":"ped/chapter1/ladder/#the-deficit-model","text":"The first of these models reflects an assumption that public awareness of and knowledge about science and technology is, in general, poor. Public surveys are often cited in support of such a view, including claims such as the following: Quote [...] only 10 percent of Americans can define \"molecule,\" and that more than half believe that humans and dinosaurs lived on the Earth at the same time. From this starting point, those who adhere to the deficit model presume that attempts to improve public knowledge are invariably a good thing. But are they? Why, for example, do members of the public need to know that the luminosity of the Andromeda galaxy is $~2.6\u00d710^{10}$ $L_\u2609$, or that variational autoencoders are popular types of generative models in artificial intelligence? Moreover, why should the absence of this knowledge be treated as a deficit when it is likely to have no application in the daily lives of members of the public?","title":"The Deficit Model"},{"location":"ped/chapter1/ladder/#the-contextual-model","text":"The way we respond to information differs depending on the context in which it is presented. Our personality type, for example, may affect how we perceive and evaluate risk. Social and cultural attitudes affect levels of trust in scientific authority. And, the media can play a substantial role in shaping our interest in science and technology\u2014Carl Sagan, for example, was highly renowned for the passion and excitement he was able to generate in often complex or dry scientific topics. The second model Lewinstein describes\u2014the contextual model\u2014therefore, recognises the need for tailoring information to specific audiences. This helps to address the deficit model's own deficit, as describe above. However, we may think that this revised model is just a more sophisticated version of the deficit model\u2014old wine in a new bottle! After all, there is still a presumed gap in understanding, but it needs to be addressed in a specific way. What these two models share is their presumed equivalence of 'public understanding of science and technology' with 'public appreciation of the benefits provided by science and technology to society'. In other words, the more that members of the public understand about science and technology the more they will come to appreciate its benefits. This may be true in some instances. And so the goal of enhanced literacy ought to be recognised as a valid objective for public engagement. However, public engagement under these two models reduces engagement to a uni-directional form of communication in which the goal of the scientist is simply to inform and educate. It is for this reason that such forms of engagement fall under the 'non-participatory' category of Arnstein's ladder.","title":"The Contextual Model"},{"location":"ped/chapter1/ladder/#the-lay-expertise-model","text":"Turning the spotlight of attention back onto the scientific communities, a further concern with the previous two models that Lewinstein identifies is that they do not adequately address the social or political contexts in which science and technology are developed, including the conflicts of interest that scientific pursuits may have with local communities or expertise (e.g. labour groups). Far from being the sole arbiters of knowledge and expertise, scientists often fall prey to their own biases or limited perspective (i.e. positionality) and overlook diverse forms of knowledge that are rooted in local communities and practices (e.g. agriculture). Such local knowledge, including the data flows interconnected with the embedded processes of local knowledge production, may be as relevant (or even more valuable) than \"technical\" forms when attempting to address (or redefine) some problem. In cases where knowledge resides in local practices and expertise, public engagement may benefit from meaningful partnerships, leading to complementary and mutually enhancing forms of knowledge. A pragmatic goal of such endeavours, therefore, is to solve problems. However, in pursuing such a goal scientists must remain vigilant that their goal and formulation of the problem is not misaligned with the goal of the communities with whom they partner.","title":"The Lay Expertise Model"},{"location":"ped/chapter1/ladder/#the-public-participation-model","text":"Lewinstein's final model brings us back to Arnstein's ladder. However, the topics that Lewinstein addresses go beyond empowerment to include additional benefits such as heightened public trust and consensus formation. There are myriad activities that can support these objectives, including citizen juries, consensus building workshops, and deliberative polling. Therefore, this model also seeks to identify means for democratising science and technology in order to wrest control of research and innovation away from elite institutions and politicians. Once we understand these different activities, it becomes clear that public participation is not about outsourcing decision-making authority to the public. In other words, the people that participate are not making decisions on behalf of researchers, but their voices should be listened to insofar as they are impacted by the consequences of a research or innovation project. This is because scientific research and the design, development, and deployment of technology embody particular values that may not be shared by those who are directly or indirectly affected by the consequences of such activities. Obvious examples include military science and technology. Can you think of others? The field of science and technology studies has been critically examining scientific research and practice for several decades, exposing many latent value structures and biases in the process. 2 An awareness of such structures and biases is vital to understanding the goals of public engagement. We will explore this value laden nature of science and technology in relation to public engagement later. However, with this additional understanding of public engagement, let us now take a closer look at the goals of objectives of public engagement. Who (or what) are the \"Public\"? Before going any further, we should also stop to address a conceptual point about what the concept 'the Public' actually references. It is easy to fall into the trap of treating \"the public\" as some monolithic or homogenous entity, because of how its usage as a mass noun affects our judgement. However, as we all know, the public are a wonderfully diverse set of people, communities, and groups, of which we are a part. The more we can keep this in the forefront of our minds going forward, the less likely we will be to fall prey to myopic assumptions about public engagement. A note on the terms 'participation' and 'engagement': in some cases these terms can be used interchangeably. For instance, public participation is a form of engagement. However, the reverse is not always true, as Arnstein's ladder demonstrates. For instance, informing members of the public about novel scientific results or findings through print communication is best seen as engagement, rather than than participation. This is because the reader is unable to participate in the scientific process, even if they have been engaged in the later stages (i.e. scientific communication). Throughout this guide, we will default to the use of 'engagement' where an inclusive usage is appropriate. \u21a9 For an overview of key developments in science and technology studies, see this timeline \u21a9","title":"The Public Participation Model"},{"location":"ped/chapter2/","text":"The Value(s) of Public Engagement \u00b6 Introduction \u00b6 In the first chapter we explored and considered answers to the following questions: Questions What is public engagement? What are the goals of public engagement? In discussing possible answers we took for granted the underlying values that support and motivate different forms of public engagement. For instance, values like shared understanding that are enacted through activities such as democratic deliberation. These values matter, and so we could also ask two additional questions: Additional Questions What are the different values that support and motivate public engagement? What does it mean to conduct responsible public engagement for data science and AI? Addressing these questions will be the main focus of this chapter. We will start with the first question to further our understanding of the social significance and importance of public engagement, as well as to ground the subsequent discussion of what constitutes 'responsible public engagement'. Chapter Outline \u00b6 Deliberative Values Responsible Public Engagement in Data Science and AI Learning Objectives This chapter is designed to help you identify and evaluate the different ethical and social values that are implicated in all public engagement activities. By the end of the chapter you should have a better understanding of why these values matter, and also be able to use the SAFE-D principles to assess whether a particular research or innovation project is responsible.","title":"Introduction"},{"location":"ped/chapter2/#the-values-of-public-engagement","text":"","title":"The Value(s) of Public Engagement"},{"location":"ped/chapter2/#introduction","text":"In the first chapter we explored and considered answers to the following questions: Questions What is public engagement? What are the goals of public engagement? In discussing possible answers we took for granted the underlying values that support and motivate different forms of public engagement. For instance, values like shared understanding that are enacted through activities such as democratic deliberation. These values matter, and so we could also ask two additional questions: Additional Questions What are the different values that support and motivate public engagement? What does it mean to conduct responsible public engagement for data science and AI? Addressing these questions will be the main focus of this chapter. We will start with the first question to further our understanding of the social significance and importance of public engagement, as well as to ground the subsequent discussion of what constitutes 'responsible public engagement'.","title":"Introduction"},{"location":"ped/chapter2/#chapter-outline","text":"Deliberative Values Responsible Public Engagement in Data Science and AI Learning Objectives This chapter is designed to help you identify and evaluate the different ethical and social values that are implicated in all public engagement activities. By the end of the chapter you should have a better understanding of why these values matter, and also be able to use the SAFE-D principles to assess whether a particular research or innovation project is responsible.","title":"Chapter Outline"},{"location":"ped/chapter2/deliberation/","text":"Deliberative Values \u00b6 Let's begin with a simple definition of 'deliberation': Quote We define deliberation minimally to mean mutual communication that involves weighing and reflecting on preferences, values, and interests regarding matters of common concern.[@bachtiger2018] (emphasis added) As defined, deliberation is separate to (but consistent with) processes such as decision-making, polling, and voting\u2014processes that are common components of public engagement. However, deliberation is a process that aims at mutual understanding and building consensus, prior to decision-making. As such, deliberative forms of engagement place an emphasis on the communicative value inherent in deliberation as a form of dialogue (or, \"mutual communication\"), instead of the practical value of decision-making. An illustrative example can help explain why this distinction matters: Conflicting Preferences A team of social research scientists working with the local government is undertaking research into public attitudes concerning the environmental impact of potential traffic policies. They have decided to engage a representative group of local residents to explore a range of policies under consideration, including the following: The local government would like to increase investment in access to electric vehicle charging points. However, because they have limited resources available, they are only able to deploy 100 charging points in five areas with the highest number of electric vehicles, while also creating a central hub used by those visiting the city centre. The local government would like to use automated number plate recognition on residential roads that are commonly used by commuters to bypass traffic on busier routes. This policy is supported by a strong body of empirical evidence that shows it can discourage non-essential journeys and also helps ease congestion in parts of the city, in turn lowering pollution. Consider what would happen if the research team simply choose to ask the residents to vote on whether these policies should be adopted. It's possible that they would both be approved by a majority who see them as positive investments in renewable energy and the environment. But upon deeper reflection, the policies could be quite divisive. The first policy, for instance, could be seen as problematic because of its impact upon socionecomic inequality . That is, the policy would likely favour already affluent neighbourhoods because of the prioritisation of areas that have the \"highest number of electric vehicles\". In turn, limited resources would be diverted from already disadvantaged communities, further widening socioeconomic inequality. In addition, the second policy could split the group according to whether they rely heavily on roads (e.g. for commuting or work) or prefer quieter streets because they are able to work remotely. Despite the positive environmental impact, there would likely be a significant level of inconvenience and disruption to some groups. Focusing solely on the positive value of environmental impact, therefore, would overlook additional values of fairness and welfare. In both cases, following deliberation, it is unlikely that such policies would receive unanimous approval or disapproval. However, through deliberation, a space could be created to allow all voices to be heard and for conflicting values to emerge. The manner in which these conflicts are handled depends, ultimately, on how the engagement activity is designed. One option would be to forego any voting or polling entirely, in favour of additional research, consultation, and public engagement (e.g. soliciting additional views). We will return to these topics in a later chapter. For the time being, it is sufficient to simply note that public deliberation about the means and ends of scientific and technological innovation, such as the implementation of research and innovation through public policy as above, need not be oriented solely towards practical decision-making. Instead, by focusing on the values inherent to communication and deliberation, we are able to identify and emphasise particular values that ought to guide deliberation in a democratic system. Identifying Values for Effective Deliberation \u00b6 In reviewing the literature on such values, Bachtiger et al.[@bachtiger2018] identify two generations of standards for good deliberation \u2014the latter of which have grown out of and developed upon the first. First Generation Second Generation Respect Unrevised Absence of power Unrevised Equality Inclusion, mutual respect, equal communicative freedoms, equal opportunity for influence Reasons Relevant considerations Aim at consensus Aim at both consensus and clarifying conflict Common good orientation Orentation to both common good and self-interest constrained by fairness Publicity Publicity in many conditions, but not all (e.g., in negotiations when representatives can be trusted) Accountability Accountability to constituents when elected, to other participants and citizens when not elected Sincerity Sincerity in matters of importance; allowable insincerity in greetings, compliments, and other communications intended to increase sociality Rather than reviewing each of these, we'll just consider two with an eye towards understanding the values expressed by such standards. First, 'absence of power' is a standard that serves as a precondition for values such as equality, diversity, and inclusivity (EDI). For example, the imbalance of power that exists between experts and members of the public can create a situation in which the latter feel uncomfortable expressing their honest thoughts or opinions for fear of being wrong. Similarly, deliberation within research teams can be impacted by unequal power relations between senior researchers and graduate students, preventing the emergence and exploration of novel ideas. Second, aiming at both consensus and clarifying conflict reinforces the inherent value of public communication and mutual understanding , offsetting the prioritisation of more instrumental values such as decision-making. In the case of our two policies above, for example, aiming at consensus, even if it cannot be reached unanimously, can at least create a situation in which the different voices in the debate are better able to appreciate what matters to other members of their community. This is an often under-appreciated value by research teams, who are focused on influencing policy or pursuing research objectives. However, deliberation that upholds the standards above can also increase trust and respect between communities (e.g. scientists and the public)\u2014an additional goal that we considered in the previous chapter. Why EDI matters? Equality, diversity, and inclusivity (EDI) as a collection of values has received widespread and growing attention in recent years. There are many reasons why this increased focus is both significant and vital. These reasons are typically contextual, and reflect moral priorities of the domains in which the values emerge (e.g. STEM research). In the context of public engagement, equal, diverse, and inclusive deliberation helps support a public that reflectively recognises shared needs and interests, produces the best solutions to the problems involved in meeting those needs and interests, brings individual into a close and fulfilling relationship with a community.[@chambers2018] As with the remaining standards identified by Bachtiger et al.,[@bachtiger2018] the above two express important values that are enacted through effective deliberation and public engagement. However, there are myriad barriers that can prevent such values from being realised. These include: Unbalanced relationships of power Increased polarisation that impedes consensus building Temporal constraints Cognitive biases The challenge for any researcher is, therefore, 'how to navigate these barriers and pursue value-based objectives of effective public engagement'. This is particularly challenging when the barriers originate, in part, because of competing research priorities. For instance, the priority of ensuring a sufficient sense of urgency on the completion of research deliverables can disincentivise lengthy deliberative activities. It is worth noting, therefore, that the standards and values identified above are normative ideals . That is, they are motivational and aspirational principles for good deliberation but may not be obtainable in practice {% cite bachtiger2018 %}. Values for Additional Forms of Public Engagement (Activity 2.1) We have considered values associated with the goal of democratic deliberation aimed at consensus building and mutual understanding. However, there are other goals for public engagement, as we saw in the previous chapter: Improved Public Awareness of Science and Technology Establishing Trust, Legitimacy, and Social License Improving Social Welfare Safeguarding and Supporting Human Rights What additional values (or standards) do you associate with these additional goals of public engagement? Engagement in Principle vs. Engagement in Practice \u00b6 Consider, the principle \"ensure all individuals can participate equally\", which safeguards vital social values of justice and inclusivity. While laudable as a principle, it is, of course, impossible to achieve in practice. There are simply too many cognitive biases or social inequalities that affect our ability to participate on an equal footing. Higher levels of educational ability, natural charisma, and lived experience can all impart advantages that separate participants based on their capacity for participation. Two points can be raised in response to this challenge. First, as researchers, it is always important to ensure that an ideal does not end up as the enemy of the good . Values are often expressed as ethical principles that prescribe what we ought to do under ideal circumstances (e.g. as rational agents with perfect information). While potentially valid as evaluative criteria (i.e. as post hoc means for determining what is right or wrong after the fact), they often fail to translate well as decision procedures. Nevertheless, they can still serve action-guiding roles in steering us towards ethical actions or behaviours. Second, as Bachtiger et al.[@bachtiger2018] acknowledge, there are often good reasons to strive harder in pursuit of some values over others. This allows a team to prioritise values that are particularly relevant for their project (e.g. those that matter to their stakeholder or affected users). For instance, perhaps a team of epidemiologists are concerned with understanding the social and environmental risk factors associated with a specific disease that disproportionately affects those from disadvantaged backgrounds. In this case, the research team would be justified in prioritising values that promote health equity or amplify the voices of marginalised stakeholders, even if pursuing such values is to the neglect of others. There is, ultimately, no simple procedure or algorithm for choosing between values. We will explore practical steps and procedures that can help with this often challenging process in later chapters, but the point to keep in mind at present is that in designing engagement activities or carrying out scientific or technological research and development you will always be making a choice between certain values. Having an awareness and understanding of these values will at least enable you to do so in an informed and principled manner.","title":"Deliberative Values"},{"location":"ped/chapter2/deliberation/#deliberative-values","text":"Let's begin with a simple definition of 'deliberation': Quote We define deliberation minimally to mean mutual communication that involves weighing and reflecting on preferences, values, and interests regarding matters of common concern.[@bachtiger2018] (emphasis added) As defined, deliberation is separate to (but consistent with) processes such as decision-making, polling, and voting\u2014processes that are common components of public engagement. However, deliberation is a process that aims at mutual understanding and building consensus, prior to decision-making. As such, deliberative forms of engagement place an emphasis on the communicative value inherent in deliberation as a form of dialogue (or, \"mutual communication\"), instead of the practical value of decision-making. An illustrative example can help explain why this distinction matters: Conflicting Preferences A team of social research scientists working with the local government is undertaking research into public attitudes concerning the environmental impact of potential traffic policies. They have decided to engage a representative group of local residents to explore a range of policies under consideration, including the following: The local government would like to increase investment in access to electric vehicle charging points. However, because they have limited resources available, they are only able to deploy 100 charging points in five areas with the highest number of electric vehicles, while also creating a central hub used by those visiting the city centre. The local government would like to use automated number plate recognition on residential roads that are commonly used by commuters to bypass traffic on busier routes. This policy is supported by a strong body of empirical evidence that shows it can discourage non-essential journeys and also helps ease congestion in parts of the city, in turn lowering pollution. Consider what would happen if the research team simply choose to ask the residents to vote on whether these policies should be adopted. It's possible that they would both be approved by a majority who see them as positive investments in renewable energy and the environment. But upon deeper reflection, the policies could be quite divisive. The first policy, for instance, could be seen as problematic because of its impact upon socionecomic inequality . That is, the policy would likely favour already affluent neighbourhoods because of the prioritisation of areas that have the \"highest number of electric vehicles\". In turn, limited resources would be diverted from already disadvantaged communities, further widening socioeconomic inequality. In addition, the second policy could split the group according to whether they rely heavily on roads (e.g. for commuting or work) or prefer quieter streets because they are able to work remotely. Despite the positive environmental impact, there would likely be a significant level of inconvenience and disruption to some groups. Focusing solely on the positive value of environmental impact, therefore, would overlook additional values of fairness and welfare. In both cases, following deliberation, it is unlikely that such policies would receive unanimous approval or disapproval. However, through deliberation, a space could be created to allow all voices to be heard and for conflicting values to emerge. The manner in which these conflicts are handled depends, ultimately, on how the engagement activity is designed. One option would be to forego any voting or polling entirely, in favour of additional research, consultation, and public engagement (e.g. soliciting additional views). We will return to these topics in a later chapter. For the time being, it is sufficient to simply note that public deliberation about the means and ends of scientific and technological innovation, such as the implementation of research and innovation through public policy as above, need not be oriented solely towards practical decision-making. Instead, by focusing on the values inherent to communication and deliberation, we are able to identify and emphasise particular values that ought to guide deliberation in a democratic system.","title":"Deliberative Values"},{"location":"ped/chapter2/deliberation/#identifying-values-for-effective-deliberation","text":"In reviewing the literature on such values, Bachtiger et al.[@bachtiger2018] identify two generations of standards for good deliberation \u2014the latter of which have grown out of and developed upon the first. First Generation Second Generation Respect Unrevised Absence of power Unrevised Equality Inclusion, mutual respect, equal communicative freedoms, equal opportunity for influence Reasons Relevant considerations Aim at consensus Aim at both consensus and clarifying conflict Common good orientation Orentation to both common good and self-interest constrained by fairness Publicity Publicity in many conditions, but not all (e.g., in negotiations when representatives can be trusted) Accountability Accountability to constituents when elected, to other participants and citizens when not elected Sincerity Sincerity in matters of importance; allowable insincerity in greetings, compliments, and other communications intended to increase sociality Rather than reviewing each of these, we'll just consider two with an eye towards understanding the values expressed by such standards. First, 'absence of power' is a standard that serves as a precondition for values such as equality, diversity, and inclusivity (EDI). For example, the imbalance of power that exists between experts and members of the public can create a situation in which the latter feel uncomfortable expressing their honest thoughts or opinions for fear of being wrong. Similarly, deliberation within research teams can be impacted by unequal power relations between senior researchers and graduate students, preventing the emergence and exploration of novel ideas. Second, aiming at both consensus and clarifying conflict reinforces the inherent value of public communication and mutual understanding , offsetting the prioritisation of more instrumental values such as decision-making. In the case of our two policies above, for example, aiming at consensus, even if it cannot be reached unanimously, can at least create a situation in which the different voices in the debate are better able to appreciate what matters to other members of their community. This is an often under-appreciated value by research teams, who are focused on influencing policy or pursuing research objectives. However, deliberation that upholds the standards above can also increase trust and respect between communities (e.g. scientists and the public)\u2014an additional goal that we considered in the previous chapter. Why EDI matters? Equality, diversity, and inclusivity (EDI) as a collection of values has received widespread and growing attention in recent years. There are many reasons why this increased focus is both significant and vital. These reasons are typically contextual, and reflect moral priorities of the domains in which the values emerge (e.g. STEM research). In the context of public engagement, equal, diverse, and inclusive deliberation helps support a public that reflectively recognises shared needs and interests, produces the best solutions to the problems involved in meeting those needs and interests, brings individual into a close and fulfilling relationship with a community.[@chambers2018] As with the remaining standards identified by Bachtiger et al.,[@bachtiger2018] the above two express important values that are enacted through effective deliberation and public engagement. However, there are myriad barriers that can prevent such values from being realised. These include: Unbalanced relationships of power Increased polarisation that impedes consensus building Temporal constraints Cognitive biases The challenge for any researcher is, therefore, 'how to navigate these barriers and pursue value-based objectives of effective public engagement'. This is particularly challenging when the barriers originate, in part, because of competing research priorities. For instance, the priority of ensuring a sufficient sense of urgency on the completion of research deliverables can disincentivise lengthy deliberative activities. It is worth noting, therefore, that the standards and values identified above are normative ideals . That is, they are motivational and aspirational principles for good deliberation but may not be obtainable in practice {% cite bachtiger2018 %}. Values for Additional Forms of Public Engagement (Activity 2.1) We have considered values associated with the goal of democratic deliberation aimed at consensus building and mutual understanding. However, there are other goals for public engagement, as we saw in the previous chapter: Improved Public Awareness of Science and Technology Establishing Trust, Legitimacy, and Social License Improving Social Welfare Safeguarding and Supporting Human Rights What additional values (or standards) do you associate with these additional goals of public engagement?","title":"Identifying Values for Effective Deliberation"},{"location":"ped/chapter2/deliberation/#engagement-in-principle-vs-engagement-in-practice","text":"Consider, the principle \"ensure all individuals can participate equally\", which safeguards vital social values of justice and inclusivity. While laudable as a principle, it is, of course, impossible to achieve in practice. There are simply too many cognitive biases or social inequalities that affect our ability to participate on an equal footing. Higher levels of educational ability, natural charisma, and lived experience can all impart advantages that separate participants based on their capacity for participation. Two points can be raised in response to this challenge. First, as researchers, it is always important to ensure that an ideal does not end up as the enemy of the good . Values are often expressed as ethical principles that prescribe what we ought to do under ideal circumstances (e.g. as rational agents with perfect information). While potentially valid as evaluative criteria (i.e. as post hoc means for determining what is right or wrong after the fact), they often fail to translate well as decision procedures. Nevertheless, they can still serve action-guiding roles in steering us towards ethical actions or behaviours. Second, as Bachtiger et al.[@bachtiger2018] acknowledge, there are often good reasons to strive harder in pursuit of some values over others. This allows a team to prioritise values that are particularly relevant for their project (e.g. those that matter to their stakeholder or affected users). For instance, perhaps a team of epidemiologists are concerned with understanding the social and environmental risk factors associated with a specific disease that disproportionately affects those from disadvantaged backgrounds. In this case, the research team would be justified in prioritising values that promote health equity or amplify the voices of marginalised stakeholders, even if pursuing such values is to the neglect of others. There is, ultimately, no simple procedure or algorithm for choosing between values. We will explore practical steps and procedures that can help with this often challenging process in later chapters, but the point to keep in mind at present is that in designing engagement activities or carrying out scientific or technological research and development you will always be making a choice between certain values. Having an awareness and understanding of these values will at least enable you to do so in an informed and principled manner.","title":"Engagement in Principle vs. Engagement in Practice"},{"location":"ped/chapter2/responsible/","text":"Responsible Public Engagement in Data Science and AI \u00b6 Quote The public have to live with the consequences of science and technology. As such, RRI is above all \"an opportunity for truly collective stewardship of our highly technologized future.\" (Sebastian Pfotenhauer quoted in Pain, 2017)[@pain2017] The above quote helps to motivate a need to consider our responsibility, as researchers, to the societal impact of science and technology. The practical tools and methods we will explore in subsequent chapters will help us meet our various obligations. However, before we shift to the practical considerations, there is one final question to consider: Question What does it mean to conduct responsible public engagement in data science and AI? Responsible research and innovation is increasingly important in science and technology. As a term, 'responsible research and innovation' (RRI) is most strongly associated with the European Commission's Framework Programmes for Research and Technological Development\u2014a set of funding programmes that support research in the European Union. Beginning with the seventh framework programme in 2010, and continuing on through Horizon 2020 (FP8), the term 'responsible research and innovation' became increasingly important for the European Commission's policy. Since then, other national funding bodies have also shown a commitment to RRI. For example, UKRI's Engineering and Physical Sciences Research Council have developed the AREA framework , which sets out four principles for RRI: Anticipate, Reflect, Engage, and Act (AREA). AREA Principles Anticipate - Describe and analyse the impacts, intended or otherwise, that might arise. Do not seek to predict but rather support the exploration of possible impacts (such as economic, social and environmental) and implications that may otherwise remain uncovered and little discussed. Reflect - Reflect on the purposes of, motivations for and potential implications of the research, together with the associated uncertainties, areas of ignorance, assumptions, framings, questions, dilemmas and social transformations these may bring. Engage - Open up such visions, impacts and questioning to broader deliberation, dialogue, engagement and debate in an inclusive way. Act - Use these processes to influence the direction and trajectory of the research and innovation process itself. However, research and innovation in data science and AI is unlike traditional scientific research or technological innovation in many ways. For example, the following considerations are pertinent and relate to the impact upon public engagement specifically: Effective public communication requires sufficient levels of data and digital literacy (e.g. understanding of statistics, data visualisation) Conceptual knowledge and understanding of relevant concepts such as 'autonomous and adaptive behaviour', 'intelligence', or ' algorithmic system' may vary widely, such that different stakeholders may not be talking about the same thing Participants will hold differing opinions on key topics, such as data privacy and protection, which could give rise to challenging attitudes such as algorithmic aversion . 1 Values and norms concerning the impact of novel data-driven technologies are still in flux, as they continue to affect different areas of society. Therefore, public attitudes may also vary, perhaps drastically, over time. As such, legitimate doubts could be raised about how universally these principles apply, and also whether they extend to considerations about public engagement? In response to these doubts, a justifiable argument could be made that some principles of RRI are more directly relevant to some of the applied sciences than more theoretical disciplines (e.g. fundamental physics). Two comments can be made: We should exercise caution in assenting to such a belief too rapidly given the many examples in the history of science that show how unintended consequences can arise as a result of advances in some scientific discipline that even experts could not foresee (see this example ). Although principles of RRI may apply more strongly in some areas of application or research, there are also principles (e.g. FAIR principles ) that support responsible public engagement that ought to apply universally (e.g. research integrity, promotion of equality, diversity, and inclusivity among researchers). Question How else does public engagement in data science and AI differ from public engagement in traditional scientific research or technological innovation? The Burden of Responsibility \u00b6 Exercising responsibility can be demanding. In the context of public engagement for research this is made more demanding by the existence of several barriers: High levels of competition for research funding Secrecy among project teams Temporary (fixed-term) contracts for researchers Time pressures Although some of the tools and methods we will consider in later chapters can help alleviate some of these burdens, it would be naive to pretend that a wider institutional change in attitudes and incentive structures is not also necessary. 2 On top of these barriers, public engagement in data science and AI is not always recognised as valuable for the furthering of one's scientific career by some specialists. However, consider the following quotation: Quote The lack of diverse jobs after a PhD, a shrinking funding budget and lack of public support all seem daunting to graduate students. In the face of these major hurdles, many scientists isolate themselves from these issues and focus on their own research. A recent study has shown, however, that increased interaction with reporters and the more Twitter mentions a study receives correlate with a higher h -index of the author, a metric for measuring the scientific impact of a publication. To determine whether media coverage of a published science article is causative of increased citations, a 1991 study looked at journal articles that would have been covered by the New York Times, but due to a writer\u2019s strike in the late 1970s, were not. Researchers found that the journal articles that were not covered had consistently less citations than other research articles covered by the Times. There is clear value for the science research community to publicise new findings to the public.[@pham2016] While this is only one form of engagement (i.e. communication) it serves as a pragmatic reason for pursuing public engagement. However, there are also ethical reasons for pursuing public engagement in data science, in spite of the aforementioned barriers. With these considerations in mind, let us now return to the original question, 'what does it mean to conduct responsible public engagement in data science and AI?' Although some of the principles alluded to above may also apply to responsible engagement in data science and AI (e.g. AREA principles), there are more specific principles that can help us with planning and delivering public engagement activities. We call these the SAFE-D principles . SAFE-D Principles \u00b6 We cover the SAFE-D principles in greater detail in our our course on responsible research and innovation . Here, it will suffice to simply summarise them and explain how they can support responsible public engagement through some illustrative examples. We will return to some of the examples in later activities. In general, the SAFE-D principles are designed to support responsible project governance in data science and AI research and innovation. Insofar as public engagement forms a key part of this research and innovation, the SAFE-D principles serve as useful ethical reflection points to help design responsible public engagement activities. Sustainability \u00b6 Sustainability can mean a couple of things. From a technical perspective, sustainability requires the outputs of a project to be safe, secure, robust, and reliable. For example, if an organisation is developing an autonomous vehicle, it should operate safely in the intended context of use. However, in the context of responsible data science and AI, there is also a social sustainability component. This aspect of sustainability requires a project\u2019s practices and outputs to be informed a) by ongoing consideration of the risks of harm to individuals and society, even after the system has been deployed and the project completed\u2014a long-term (or sustainable) safety\u2014but also b) by the shared values that motivate the pursuit of social goals. The final point in the above summary is significant for public engagement, and connects with the following question: Question Who Designs the Future? As society continues to stagger forward feeling the ongoing effects of the global pandemic, while struggling with the increasing complexity and uncertainty that a modern, data-driven society poses, the ability for particular voices to participate in imagining a collective vision for the future becomes increasingly difficult. This can lead to a growing distrust in vital social institutions as people become alienated from society. In response, organisations like Nesta in the UK are carrying out public engagement activities focused on building \" participatory futures \", which aim to include marginalised or systematically excluded voices in these conversations. Public engagement activities and events such as these can play a vital role in supporting sustainable research and innovation. Accountability \u00b6 In contrast to responsibility, accountability is often seen as a backwards-looking process of holding an individual or organisation to account. It is commonly associated with regulation and governance, but in the context of data science or AI research and innovation, it goes beyond these matters. In this context, 'accountability' refers to the transparency of processes and associated outcomes that enable people to understand how a project was conducted (e.g., project documentation), or why a specific decision was reached. But it can also refer to broader processes of responsible project governance that seek to establish clear roles of responsibility where full transparency may be inappropriate (e.g., confidential projects). Without this broader, end-to-end awareness of accountability throughout a project's lifecycle, it is possible that harms could arise such that specific individuals are stripped of their ability to seek redress for their grievances or violations of certain rights. Responsible public engagement, therefore, can serve as a way to ensure that a project and its associated outputs remain sufficiently accountable, by ensuring that all affected stakeholders are able to voice potential concerns. The following quote is a nice reflection point for the values contained within this principle: Quote Sometimes we want everyone\u2019s voice to be heard because we think that will make a better decision as a result, and sometimes we want everyone\u2019s voice to be heard simply because we think that everyone has a right to be heard.[@macgilvray2014] Fairness \u00b6 Consider the following statistic: Quote 15 per cent of scientists come from working class backgrounds; and in the US, children from the top 1 per cent of richest families (by income) are ten times as likely to have filed for a patent as those from families in the bottom half of the income distribution.[@saunders2018] While much has been done in the past decade to improve access to science and technology, there is, of course, still a lot more that needs to be done to ensure all individuals have an equal opportunity to participate in a core part of society. This is, ultimately, a matter of fairness. Fairness is inseparably connected with legal conceptions of equality and justice. In the context of data science or AI, this often leads to an emphasis on features such as non-discrimination, equitable outcomes of automated decisions, or procedural fairness through bias mitigation. However, these notions serve only a subset of broader ethical considerations pertaining to social justice, socioeconomic capabilities, diversity and inclusivity. As the above quote acknowledges, fairness is also a matter of addressing the socioeconomic barriers that prevent or make it difficult for certain individuals or communities to participate. Responsible public engagement, therefore, can help researchers or developers identify relevant barriers and work with affected people to co-design ways to improve capabilities. Explainability \u00b6 Explainability has received widespread attention in recent years from many within the AI community. This is because it is a key requirement for autonomous and informed decision-making in situations where data-driven systems interact with or influence human judgement and choice behaviour. For example, if an autonomous vehicle is involved in an accident or if a algorithmic system fails to recognise a fault in vital energy infrastructure, the reasons for these unexpected and undesirable behaviours needs to be identified. As such, there needs to be a) interpretability of the system itself (e.g., an ability to make sense of the underlying logic by which a system reaches a decision) and b) the ability to translate this information into a variety of explanations that are acceptable to relevant users and stakeholders. On this second point, public engagement can support evaluation and determination of both the accessibility and usability of different explanations, as well as providing input on possible trade-offs between, say, the interpretability and accuracy of possible system types (e.g., decision trees versus neural networks). 3 Data Quality, Integrity, Privacy and Protection \u00b6 Data quality, integrity, protection and privacy must all be established to be confident that a research or innovation project has been designed, developed, and deployed in a responsible manner. \u2018Data Quality\u2019 captures the static properties of data, such as whether they are a) relevant to and representative of the domain and use context, b) balanced and complete in terms of how well the dataset represents the underlying data generating process, and c) up-to-date and accurate as required by the project. \u2018Data Integrity\u2019 refers to more dynamic properties of data stewardship, such as how a dataset evolves over the course of a project lifecycle. In this manner, data integrity requires a) contemporaneous and attributable records from the start of a project (e.g., process logs; research statements), b) ensuring consistent and verifiable means of data analysis or processing during development, and c) taking steps to establish findable , accessible , interoperable , and reusable records towards the end of a project\u2019s lifecycle. \u2018Data protection and privacy\u2019 reflect ongoing developments and priorities as set out in relevant legislation and regulation of data practices as they pertain to fundamental rights and freedoms, democracy, and the rule of law. For example, the right for data subjects to have inaccurate personal data rectified or erased. These final properties are, typically, more technical than the others (e.g., requiring legal expertise to determine if a project violates any data protection laws). However, public engagement can help determine whether a project that is legal is also acceptable from a moral perspective. For example, if a research project uses a basis of informed consent to collect and process personal data, are the participants able to opt-out at a later stage and are their processes to make this decision easy and accessible for participants. Whether this is the case will, invariably, be a matter of working with participants to determine. --8<-- \"includes/abbreviations.md\" Algorithmic aversion refers to the reluctance of human agents to incorporate algorithmic tools as part of their decision-making processes due to misaligned expectations of the algorithm\u2019s performance (see Burton et al. 2020). \u21a9 There are increasing signs of reform in the funding landscape, however, with more funding bodies favouring applications that demonstrate commitments to responsible research and innovation or include early career researchers as co-investigators. \u21a9 For more on the explainability/accuracy trade-off see this page . \u21a9","title":"Responsible Public Engagement"},{"location":"ped/chapter2/responsible/#responsible-public-engagement-in-data-science-and-ai","text":"Quote The public have to live with the consequences of science and technology. As such, RRI is above all \"an opportunity for truly collective stewardship of our highly technologized future.\" (Sebastian Pfotenhauer quoted in Pain, 2017)[@pain2017] The above quote helps to motivate a need to consider our responsibility, as researchers, to the societal impact of science and technology. The practical tools and methods we will explore in subsequent chapters will help us meet our various obligations. However, before we shift to the practical considerations, there is one final question to consider: Question What does it mean to conduct responsible public engagement in data science and AI? Responsible research and innovation is increasingly important in science and technology. As a term, 'responsible research and innovation' (RRI) is most strongly associated with the European Commission's Framework Programmes for Research and Technological Development\u2014a set of funding programmes that support research in the European Union. Beginning with the seventh framework programme in 2010, and continuing on through Horizon 2020 (FP8), the term 'responsible research and innovation' became increasingly important for the European Commission's policy. Since then, other national funding bodies have also shown a commitment to RRI. For example, UKRI's Engineering and Physical Sciences Research Council have developed the AREA framework , which sets out four principles for RRI: Anticipate, Reflect, Engage, and Act (AREA). AREA Principles Anticipate - Describe and analyse the impacts, intended or otherwise, that might arise. Do not seek to predict but rather support the exploration of possible impacts (such as economic, social and environmental) and implications that may otherwise remain uncovered and little discussed. Reflect - Reflect on the purposes of, motivations for and potential implications of the research, together with the associated uncertainties, areas of ignorance, assumptions, framings, questions, dilemmas and social transformations these may bring. Engage - Open up such visions, impacts and questioning to broader deliberation, dialogue, engagement and debate in an inclusive way. Act - Use these processes to influence the direction and trajectory of the research and innovation process itself. However, research and innovation in data science and AI is unlike traditional scientific research or technological innovation in many ways. For example, the following considerations are pertinent and relate to the impact upon public engagement specifically: Effective public communication requires sufficient levels of data and digital literacy (e.g. understanding of statistics, data visualisation) Conceptual knowledge and understanding of relevant concepts such as 'autonomous and adaptive behaviour', 'intelligence', or ' algorithmic system' may vary widely, such that different stakeholders may not be talking about the same thing Participants will hold differing opinions on key topics, such as data privacy and protection, which could give rise to challenging attitudes such as algorithmic aversion . 1 Values and norms concerning the impact of novel data-driven technologies are still in flux, as they continue to affect different areas of society. Therefore, public attitudes may also vary, perhaps drastically, over time. As such, legitimate doubts could be raised about how universally these principles apply, and also whether they extend to considerations about public engagement? In response to these doubts, a justifiable argument could be made that some principles of RRI are more directly relevant to some of the applied sciences than more theoretical disciplines (e.g. fundamental physics). Two comments can be made: We should exercise caution in assenting to such a belief too rapidly given the many examples in the history of science that show how unintended consequences can arise as a result of advances in some scientific discipline that even experts could not foresee (see this example ). Although principles of RRI may apply more strongly in some areas of application or research, there are also principles (e.g. FAIR principles ) that support responsible public engagement that ought to apply universally (e.g. research integrity, promotion of equality, diversity, and inclusivity among researchers). Question How else does public engagement in data science and AI differ from public engagement in traditional scientific research or technological innovation?","title":"Responsible Public Engagement in Data Science and AI"},{"location":"ped/chapter2/responsible/#the-burden-of-responsibility","text":"Exercising responsibility can be demanding. In the context of public engagement for research this is made more demanding by the existence of several barriers: High levels of competition for research funding Secrecy among project teams Temporary (fixed-term) contracts for researchers Time pressures Although some of the tools and methods we will consider in later chapters can help alleviate some of these burdens, it would be naive to pretend that a wider institutional change in attitudes and incentive structures is not also necessary. 2 On top of these barriers, public engagement in data science and AI is not always recognised as valuable for the furthering of one's scientific career by some specialists. However, consider the following quotation: Quote The lack of diverse jobs after a PhD, a shrinking funding budget and lack of public support all seem daunting to graduate students. In the face of these major hurdles, many scientists isolate themselves from these issues and focus on their own research. A recent study has shown, however, that increased interaction with reporters and the more Twitter mentions a study receives correlate with a higher h -index of the author, a metric for measuring the scientific impact of a publication. To determine whether media coverage of a published science article is causative of increased citations, a 1991 study looked at journal articles that would have been covered by the New York Times, but due to a writer\u2019s strike in the late 1970s, were not. Researchers found that the journal articles that were not covered had consistently less citations than other research articles covered by the Times. There is clear value for the science research community to publicise new findings to the public.[@pham2016] While this is only one form of engagement (i.e. communication) it serves as a pragmatic reason for pursuing public engagement. However, there are also ethical reasons for pursuing public engagement in data science, in spite of the aforementioned barriers. With these considerations in mind, let us now return to the original question, 'what does it mean to conduct responsible public engagement in data science and AI?' Although some of the principles alluded to above may also apply to responsible engagement in data science and AI (e.g. AREA principles), there are more specific principles that can help us with planning and delivering public engagement activities. We call these the SAFE-D principles .","title":"The Burden of Responsibility"},{"location":"ped/chapter2/responsible/#safe-d-principles","text":"We cover the SAFE-D principles in greater detail in our our course on responsible research and innovation . Here, it will suffice to simply summarise them and explain how they can support responsible public engagement through some illustrative examples. We will return to some of the examples in later activities. In general, the SAFE-D principles are designed to support responsible project governance in data science and AI research and innovation. Insofar as public engagement forms a key part of this research and innovation, the SAFE-D principles serve as useful ethical reflection points to help design responsible public engagement activities.","title":"SAFE-D Principles"},{"location":"ped/chapter2/responsible/#sustainability","text":"Sustainability can mean a couple of things. From a technical perspective, sustainability requires the outputs of a project to be safe, secure, robust, and reliable. For example, if an organisation is developing an autonomous vehicle, it should operate safely in the intended context of use. However, in the context of responsible data science and AI, there is also a social sustainability component. This aspect of sustainability requires a project\u2019s practices and outputs to be informed a) by ongoing consideration of the risks of harm to individuals and society, even after the system has been deployed and the project completed\u2014a long-term (or sustainable) safety\u2014but also b) by the shared values that motivate the pursuit of social goals. The final point in the above summary is significant for public engagement, and connects with the following question: Question Who Designs the Future? As society continues to stagger forward feeling the ongoing effects of the global pandemic, while struggling with the increasing complexity and uncertainty that a modern, data-driven society poses, the ability for particular voices to participate in imagining a collective vision for the future becomes increasingly difficult. This can lead to a growing distrust in vital social institutions as people become alienated from society. In response, organisations like Nesta in the UK are carrying out public engagement activities focused on building \" participatory futures \", which aim to include marginalised or systematically excluded voices in these conversations. Public engagement activities and events such as these can play a vital role in supporting sustainable research and innovation.","title":"Sustainability"},{"location":"ped/chapter2/responsible/#accountability","text":"In contrast to responsibility, accountability is often seen as a backwards-looking process of holding an individual or organisation to account. It is commonly associated with regulation and governance, but in the context of data science or AI research and innovation, it goes beyond these matters. In this context, 'accountability' refers to the transparency of processes and associated outcomes that enable people to understand how a project was conducted (e.g., project documentation), or why a specific decision was reached. But it can also refer to broader processes of responsible project governance that seek to establish clear roles of responsibility where full transparency may be inappropriate (e.g., confidential projects). Without this broader, end-to-end awareness of accountability throughout a project's lifecycle, it is possible that harms could arise such that specific individuals are stripped of their ability to seek redress for their grievances or violations of certain rights. Responsible public engagement, therefore, can serve as a way to ensure that a project and its associated outputs remain sufficiently accountable, by ensuring that all affected stakeholders are able to voice potential concerns. The following quote is a nice reflection point for the values contained within this principle: Quote Sometimes we want everyone\u2019s voice to be heard because we think that will make a better decision as a result, and sometimes we want everyone\u2019s voice to be heard simply because we think that everyone has a right to be heard.[@macgilvray2014]","title":"Accountability"},{"location":"ped/chapter2/responsible/#fairness","text":"Consider the following statistic: Quote 15 per cent of scientists come from working class backgrounds; and in the US, children from the top 1 per cent of richest families (by income) are ten times as likely to have filed for a patent as those from families in the bottom half of the income distribution.[@saunders2018] While much has been done in the past decade to improve access to science and technology, there is, of course, still a lot more that needs to be done to ensure all individuals have an equal opportunity to participate in a core part of society. This is, ultimately, a matter of fairness. Fairness is inseparably connected with legal conceptions of equality and justice. In the context of data science or AI, this often leads to an emphasis on features such as non-discrimination, equitable outcomes of automated decisions, or procedural fairness through bias mitigation. However, these notions serve only a subset of broader ethical considerations pertaining to social justice, socioeconomic capabilities, diversity and inclusivity. As the above quote acknowledges, fairness is also a matter of addressing the socioeconomic barriers that prevent or make it difficult for certain individuals or communities to participate. Responsible public engagement, therefore, can help researchers or developers identify relevant barriers and work with affected people to co-design ways to improve capabilities.","title":"Fairness"},{"location":"ped/chapter2/responsible/#explainability","text":"Explainability has received widespread attention in recent years from many within the AI community. This is because it is a key requirement for autonomous and informed decision-making in situations where data-driven systems interact with or influence human judgement and choice behaviour. For example, if an autonomous vehicle is involved in an accident or if a algorithmic system fails to recognise a fault in vital energy infrastructure, the reasons for these unexpected and undesirable behaviours needs to be identified. As such, there needs to be a) interpretability of the system itself (e.g., an ability to make sense of the underlying logic by which a system reaches a decision) and b) the ability to translate this information into a variety of explanations that are acceptable to relevant users and stakeholders. On this second point, public engagement can support evaluation and determination of both the accessibility and usability of different explanations, as well as providing input on possible trade-offs between, say, the interpretability and accuracy of possible system types (e.g., decision trees versus neural networks). 3","title":"Explainability"},{"location":"ped/chapter2/responsible/#data-quality-integrity-privacy-and-protection","text":"Data quality, integrity, protection and privacy must all be established to be confident that a research or innovation project has been designed, developed, and deployed in a responsible manner. \u2018Data Quality\u2019 captures the static properties of data, such as whether they are a) relevant to and representative of the domain and use context, b) balanced and complete in terms of how well the dataset represents the underlying data generating process, and c) up-to-date and accurate as required by the project. \u2018Data Integrity\u2019 refers to more dynamic properties of data stewardship, such as how a dataset evolves over the course of a project lifecycle. In this manner, data integrity requires a) contemporaneous and attributable records from the start of a project (e.g., process logs; research statements), b) ensuring consistent and verifiable means of data analysis or processing during development, and c) taking steps to establish findable , accessible , interoperable , and reusable records towards the end of a project\u2019s lifecycle. \u2018Data protection and privacy\u2019 reflect ongoing developments and priorities as set out in relevant legislation and regulation of data practices as they pertain to fundamental rights and freedoms, democracy, and the rule of law. For example, the right for data subjects to have inaccurate personal data rectified or erased. These final properties are, typically, more technical than the others (e.g., requiring legal expertise to determine if a project violates any data protection laws). However, public engagement can help determine whether a project that is legal is also acceptable from a moral perspective. For example, if a research project uses a basis of informed consent to collect and process personal data, are the participants able to opt-out at a later stage and are their processes to make this decision easy and accessible for participants. Whether this is the case will, invariably, be a matter of working with participants to determine. --8<-- \"includes/abbreviations.md\" Algorithmic aversion refers to the reluctance of human agents to incorporate algorithmic tools as part of their decision-making processes due to misaligned expectations of the algorithm\u2019s performance (see Burton et al. 2020). \u21a9 There are increasing signs of reform in the funding landscape, however, with more funding bodies favouring applications that demonstrate commitments to responsible research and innovation or include early career researchers as co-investigators. \u21a9 For more on the explainability/accuracy trade-off see this page . \u21a9","title":"Data Quality, Integrity, Privacy and Protection"},{"location":"ped/chapter3/","text":"Facilitating Public Engagement for Data Science and AI \u00b6 Introduction \u00b6 In the previous chapters we discussed the following topics: Summary What are the goals and values of public engagement? What does it mean to conduct responsible public engagement for data science and AI? With the conceptual foundations laid, we now turn towards the more practical side of public engagement. This chapter begins by reviewing two frameworks that can help us locate activities and methods for public engagement within a broader project lifecycle for data science and AI. The objective is to use these models to support informed answers to the following questions: When should you engage? How should you engage? Chapter Outline \u00b6 When should you engage? How should you engage? Learning Objectives By the end of this chapter you should have a strong comprehension of a) a stakeholder analysis process, b) a model of a typical data science or AI project lifecycle, and c) a variety of methods for public engagement. Collectively, these will enable you to scaffold your own project planning activities to help answer How should you engage? When should you engage?","title":"Introduction"},{"location":"ped/chapter3/#facilitating-public-engagement-for-data-science-and-ai","text":"","title":"Facilitating Public Engagement for Data Science and AI"},{"location":"ped/chapter3/#introduction","text":"In the previous chapters we discussed the following topics: Summary What are the goals and values of public engagement? What does it mean to conduct responsible public engagement for data science and AI? With the conceptual foundations laid, we now turn towards the more practical side of public engagement. This chapter begins by reviewing two frameworks that can help us locate activities and methods for public engagement within a broader project lifecycle for data science and AI. The objective is to use these models to support informed answers to the following questions: When should you engage? How should you engage?","title":"Introduction"},{"location":"ped/chapter3/#chapter-outline","text":"When should you engage? How should you engage? Learning Objectives By the end of this chapter you should have a strong comprehension of a) a stakeholder analysis process, b) a model of a typical data science or AI project lifecycle, and c) a variety of methods for public engagement. Collectively, these will enable you to scaffold your own project planning activities to help answer How should you engage? When should you engage?","title":"Chapter Outline"},{"location":"ped/chapter3/how/","text":"How should you engage? \u00b6 There are many different methods for engagement in data science and AI. If you recall the discussion from chapter 1 , these range from the one-sided processes of 'informing' to the more empowering types of engagement that strengthen the capabilities of the public and enable them to participate in and contribute to more diverse forms of science and technology. The following table summarises a wide range of salient methods: Mode of Engagement Description Degree of Engagement Practical Strengths Practical Weaknesses :material-email: Newsletters (email) Regular emails (e.g.: fortnightly or monthly) that contain updates, relevant news, and calls to action in an inviting format. INFORM Can reach many people; can contain large amount of relevant information; can be made accessible and visually engaging. Might not reach certain portions of the population; can be demanding to design and produce with some periodicity; easily forwarded to spam/junk folders without project team knowing (leading to overinflated readership stats). :material-mailbox: Letters (post) Regular letters (e.g.: monthly) that contain the latest updates, relevant news and calls to action. INFORM Can reach parts of the population with no internet or digital access; can contain large amount of relevant information; can be made accessible and visually engaging. Might not engage certain portions of the population; Slow delivery and interaction times hampers the effective flow of information and the organisation of further engagement. :material-checkbox-blank-badge: App notifications Projects can rely on the design of apps that are pitched to stakeholders who are notified on their phone with relevant updates. INFORM Easy and cost-effective to distribute information to large numbers of people; Rapid information flows bolster the provision of relevant and timely news and updates. More significant initial investment in developing an app; will not be available to people without smartphones. :fontawesome-solid-people-group: Community fora Events in which panels of experts share their knowledge on issues and then stakeholders can ask questions. INFORM Can inform people with more relevant information by providing them with the opportunity to ask questions; brings community together in a shared space of public communication. More time-consuming and resource intensive to organise; might attract smaller numbers of people and self-selecting groups rather than representative subsets of the population; effectiveness is constrained by forum capacity. :fontawesome-solid-list-check: Online Surveys Survey sent via email, embedded in a website, shared via social media, etc. CONSULT Cost-effective; simple mass- distribution. Risk of pre-emptive evaluative framework when designing questions; Does not reach those without internet connection or computer/smartphone access. :material-phone-in-talk: Phone Interviews Structured or semi-structured interviews held over the phone. CONSULT PARTNER Opportunity for stakeholders to voice concerns more openly. Risk of pre-emptive evaluative framework when designing questions; Might exclude portions of the populations without phone access or with habits of infrequent phone use. :fontawesome-solid-door-closed: Door-to-door interviews Structured or semi-structured interviews held in-person at people\u2019s houses. CONSULT PARTNER Opportunity for stakeholders to voice concerns more openly; can allow participants the opportunity to form connections through empathy and face-to- face communication. Potential for limited interest to engage with interviewers; time-consuming; can be seen by interviewees as intrusive or burdensome. :fontawesome-solid-people-arrows-left-right: In-person interviews Short interviews conducted in- person in public spaces. CONSULT PARTNER Can reach many people and a representative subset of the population if stakeholders are appropriately defined and sortition is used. Less targeted; pertinent stakeholders must be identified by area; little time/interest to engage with interviewer; can be viewed by interviewees as time- consuming and burdensome. :material-selection-search: Focus Groups A group of stakeholders brought together and asked their opinions on a particular issue. Can be more or less formally structured. CONSULT PARTNER Can gather in-depth information; Can lead to new insights and directions that were not anticipated by the project team. Subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. :fontawesome-solid-users-between-lines: Online Workshops Workshops using digital tools such as collaborative platforms. CONSULT Opportunity to reach stakeholders across regions, increased accessibility depending on digital access. Potential barriers to accessing tools required for participation, potential for disengagement. :material-crowd: Crowdsourcing (Online) Well-designed tasks that can be undertaken by a distributed collective, with individuals working on separate components. CONSULT PARTNER Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access. Supports increased potential for diverse forms of expertise and experience. Can be misused as a method for outsourcing cheap labour; potential barriers to accessing tools required for participation; potential for disengagement; difficult to ensure accuracy and validity of input from participants. :material-github: Distributed Project Collaboration (Online) Online digital platforms, such as GitHub, enable new forms of citizen science and collaborative development on diverse projects (e.g., open source software, open science). CONSULT PARTNER EMPOWER Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access; increased potential for diverse forms of expertise and experience; empowers new communities to actively participate in shaping and building tools that have real value for their communities. Potential barriers to accessing digital tools required for participation, including high levels of digital literacy. :fontawesome-solid-building-columns: Citizen panel or assembly Large groups of people (dozens or even thousands) who are representative of a town/region. INFORM CONSULT PARTNER EMPOWER Provides an opportunity for co-production of outputs; can produce insights and directions that were not anticipated by the project team; can provide an information base for conducting further outreach (surveys, interviews, focus groups, etc.); can be broadly representative; can bolster a community\u2019s sense of democratic agency and solidarity. Participant rolls must be continuously updated to ensure panels or assemblies remains representative of the population throughout their lifespan; resource-intensive for establishment and maintenance; subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. :material-account-group: Citizen jury A small group of people (between 12 and 24), representative of the demographics of a given area, come together to deliberate on an issue (generally one clearly framed set of questions), over the period of 2 to 7 days ( Involve.org.uk . INFORM CONSULT PARTNER EMPOWER Can gather in-depth information; can produce insights and directions that were not anticipated by the project team; can bolster participants\u2019 sense of democratic agency and solidarity. Subject to hazards of group think; complex to facilitate; risk of pre-emptive evaluative framework; small sample of citizens involved risks low representativeness of wider range of public opinions and beliefs. As with all forms of engagement, deciding on the best method requires awareness of your audience. Consider the following cases: === \":octicons-log-16: Policymakers\" A research team has released results from an economics study that could have a positive impact on public policy. They decide to share these results with policymakers. The goal is to directly influence policy. Therefore, the results need to be clearly communicated and also connected to the policy goal. This connection is important to help ensure that policy-makers are able to evaluate the wider implications of the scientific findings. **Communication Goal:** to demonstrate how scientific findings can support evidence-based policy impact === \":octicons-people-16: General Public\" As part of an education outreach campaign to improve digital literacy among adolescents, a mental health charity are running workshops with secondary school students. They wish to communicate recent evidence about the impact of over-using social media on mental health. Rather than communicating complicated statistical information about the methods used in their study, the team develop a more accessible form of their findings and link these findings to practical steps that the students can take to protect themselves online. **Communication Goal:** to build awareness of possible risks associated with excessive social media usage and support behavioural change strategies === \":octicons-beaker-16: Researchers\" A PhD student working in a Physics department has results from a recent study that developed and tested a new method for the large-scale data mining of astronomical data. The PhD student wish to present this new method and the validation study at an upcoming international conference for data science. The audience will be technically literate, but will not have specialist expertise in astronomy. Therefore, the PhD student describes the method in the context of its original study buy also emphasises the generalisability for other sciences (e.g. genomics). **Communication Goal**: to advance academic career by gaining experience of presenting conference papers and also generating interest in a novel data science method A common need with all of the above cases is the tailoring of the communication strategy to the engagement goal. And, in all cases, a clear message is vital. This topic (and others) will be the focus of the next section. --> Education and Outreach: Getting Creative \u00b6 The table at the start of this section outlines several methods for informing , but these are not the only ones that are relevant in the context of science and technology education and outreach. Other examples include, Education and Outreach Websites and social media Presentations Posters and displays Exhibitions Theatre, Film, and Documentaries Festivals And, some forms of education and outreach can be very creative. Consider the following two examples: === \"Deadinburgh\" ![Image title](https://www.publicengagement.ac.uk/sites/default/files/case-study/deadinburgh-136_2.jpg) \"An unknown pathogen ravages Scotland\u2019s capital, turning the unlucky souls into bloodthirsty ambling beasts. You are one of the last uninfected citizens in a city under martial law, cut off from the rest of the UK. Now, with help from real scientists, you have only hours to decide how to save Edinburgh, and perhaps the world. The Enlightenment Caf\u00e9: Deadinburgh, produced by LAStheatre, introduces the audience to the worlds of epidemiology and biomedical science through a night of immersive theatre. In a theatrical world, with actors playing the infected hordes and besieged soldiers, the audience meet genuine scientists using real science to solve a fictitious disease. In the end the audience must decide whether to destroy the city, cull the infected, or search for a cure; the fate of the city is in their hands. Through the outbreak of a zombie epidemic Deadinburgh asks \u2018what does it really mean to be human\u2019 whilst offering parallels with real life science and procedures for managing disease outbreaks.\" [From the National Co-ordinating Centre for Public Engagement](https://www.publicengagement.ac.uk/case-studies/enlightenment-cafe-deadinburgh) === \"Billenium (Future Places Toolkit)\" <div class=\"result\" markdown> ![Image title](https://www.scienceopen.com/document_file/67887571-d27c-41b9-b3ad-6723886228bf/ScienceOpen/image/rfa05020003_fig2.jpg){ align=left width=300 } Billennium was a mobile augmented reality (AR) project that took place in Bristol, UK in 2018. Participants used mobile devices to be guided on an AR tour, not of their city in the present day, but of possible architectural futures. The tour was designed to help promote members of the public engage in discussion and futures thinking about how their city could be design together. [(Clarke, 2021)](https://www.scienceopen.com/hosted-document?doi=10.14324/RFA.05.2.03)[@clarke2021] </div>","title":"How should you engage"},{"location":"ped/chapter3/how/#how-should-you-engage","text":"There are many different methods for engagement in data science and AI. If you recall the discussion from chapter 1 , these range from the one-sided processes of 'informing' to the more empowering types of engagement that strengthen the capabilities of the public and enable them to participate in and contribute to more diverse forms of science and technology. The following table summarises a wide range of salient methods: Mode of Engagement Description Degree of Engagement Practical Strengths Practical Weaknesses :material-email: Newsletters (email) Regular emails (e.g.: fortnightly or monthly) that contain updates, relevant news, and calls to action in an inviting format. INFORM Can reach many people; can contain large amount of relevant information; can be made accessible and visually engaging. Might not reach certain portions of the population; can be demanding to design and produce with some periodicity; easily forwarded to spam/junk folders without project team knowing (leading to overinflated readership stats). :material-mailbox: Letters (post) Regular letters (e.g.: monthly) that contain the latest updates, relevant news and calls to action. INFORM Can reach parts of the population with no internet or digital access; can contain large amount of relevant information; can be made accessible and visually engaging. Might not engage certain portions of the population; Slow delivery and interaction times hampers the effective flow of information and the organisation of further engagement. :material-checkbox-blank-badge: App notifications Projects can rely on the design of apps that are pitched to stakeholders who are notified on their phone with relevant updates. INFORM Easy and cost-effective to distribute information to large numbers of people; Rapid information flows bolster the provision of relevant and timely news and updates. More significant initial investment in developing an app; will not be available to people without smartphones. :fontawesome-solid-people-group: Community fora Events in which panels of experts share their knowledge on issues and then stakeholders can ask questions. INFORM Can inform people with more relevant information by providing them with the opportunity to ask questions; brings community together in a shared space of public communication. More time-consuming and resource intensive to organise; might attract smaller numbers of people and self-selecting groups rather than representative subsets of the population; effectiveness is constrained by forum capacity. :fontawesome-solid-list-check: Online Surveys Survey sent via email, embedded in a website, shared via social media, etc. CONSULT Cost-effective; simple mass- distribution. Risk of pre-emptive evaluative framework when designing questions; Does not reach those without internet connection or computer/smartphone access. :material-phone-in-talk: Phone Interviews Structured or semi-structured interviews held over the phone. CONSULT PARTNER Opportunity for stakeholders to voice concerns more openly. Risk of pre-emptive evaluative framework when designing questions; Might exclude portions of the populations without phone access or with habits of infrequent phone use. :fontawesome-solid-door-closed: Door-to-door interviews Structured or semi-structured interviews held in-person at people\u2019s houses. CONSULT PARTNER Opportunity for stakeholders to voice concerns more openly; can allow participants the opportunity to form connections through empathy and face-to- face communication. Potential for limited interest to engage with interviewers; time-consuming; can be seen by interviewees as intrusive or burdensome. :fontawesome-solid-people-arrows-left-right: In-person interviews Short interviews conducted in- person in public spaces. CONSULT PARTNER Can reach many people and a representative subset of the population if stakeholders are appropriately defined and sortition is used. Less targeted; pertinent stakeholders must be identified by area; little time/interest to engage with interviewer; can be viewed by interviewees as time- consuming and burdensome. :material-selection-search: Focus Groups A group of stakeholders brought together and asked their opinions on a particular issue. Can be more or less formally structured. CONSULT PARTNER Can gather in-depth information; Can lead to new insights and directions that were not anticipated by the project team. Subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. :fontawesome-solid-users-between-lines: Online Workshops Workshops using digital tools such as collaborative platforms. CONSULT Opportunity to reach stakeholders across regions, increased accessibility depending on digital access. Potential barriers to accessing tools required for participation, potential for disengagement. :material-crowd: Crowdsourcing (Online) Well-designed tasks that can be undertaken by a distributed collective, with individuals working on separate components. CONSULT PARTNER Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access. Supports increased potential for diverse forms of expertise and experience. Can be misused as a method for outsourcing cheap labour; potential barriers to accessing tools required for participation; potential for disengagement; difficult to ensure accuracy and validity of input from participants. :material-github: Distributed Project Collaboration (Online) Online digital platforms, such as GitHub, enable new forms of citizen science and collaborative development on diverse projects (e.g., open source software, open science). CONSULT PARTNER EMPOWER Opportunity to engage stakeholders across regions, in an asynchronous manner, with increased accessibility depending on digital access; increased potential for diverse forms of expertise and experience; empowers new communities to actively participate in shaping and building tools that have real value for their communities. Potential barriers to accessing digital tools required for participation, including high levels of digital literacy. :fontawesome-solid-building-columns: Citizen panel or assembly Large groups of people (dozens or even thousands) who are representative of a town/region. INFORM CONSULT PARTNER EMPOWER Provides an opportunity for co-production of outputs; can produce insights and directions that were not anticipated by the project team; can provide an information base for conducting further outreach (surveys, interviews, focus groups, etc.); can be broadly representative; can bolster a community\u2019s sense of democratic agency and solidarity. Participant rolls must be continuously updated to ensure panels or assemblies remains representative of the population throughout their lifespan; resource-intensive for establishment and maintenance; subject to hazards of group think or peer pressure; complex to facilitate; can be steered by dynamics of differential power among participants. :material-account-group: Citizen jury A small group of people (between 12 and 24), representative of the demographics of a given area, come together to deliberate on an issue (generally one clearly framed set of questions), over the period of 2 to 7 days ( Involve.org.uk . INFORM CONSULT PARTNER EMPOWER Can gather in-depth information; can produce insights and directions that were not anticipated by the project team; can bolster participants\u2019 sense of democratic agency and solidarity. Subject to hazards of group think; complex to facilitate; risk of pre-emptive evaluative framework; small sample of citizens involved risks low representativeness of wider range of public opinions and beliefs. As with all forms of engagement, deciding on the best method requires awareness of your audience. Consider the following cases: === \":octicons-log-16: Policymakers\" A research team has released results from an economics study that could have a positive impact on public policy. They decide to share these results with policymakers. The goal is to directly influence policy. Therefore, the results need to be clearly communicated and also connected to the policy goal. This connection is important to help ensure that policy-makers are able to evaluate the wider implications of the scientific findings. **Communication Goal:** to demonstrate how scientific findings can support evidence-based policy impact === \":octicons-people-16: General Public\" As part of an education outreach campaign to improve digital literacy among adolescents, a mental health charity are running workshops with secondary school students. They wish to communicate recent evidence about the impact of over-using social media on mental health. Rather than communicating complicated statistical information about the methods used in their study, the team develop a more accessible form of their findings and link these findings to practical steps that the students can take to protect themselves online. **Communication Goal:** to build awareness of possible risks associated with excessive social media usage and support behavioural change strategies === \":octicons-beaker-16: Researchers\" A PhD student working in a Physics department has results from a recent study that developed and tested a new method for the large-scale data mining of astronomical data. The PhD student wish to present this new method and the validation study at an upcoming international conference for data science. The audience will be technically literate, but will not have specialist expertise in astronomy. Therefore, the PhD student describes the method in the context of its original study buy also emphasises the generalisability for other sciences (e.g. genomics). **Communication Goal**: to advance academic career by gaining experience of presenting conference papers and also generating interest in a novel data science method A common need with all of the above cases is the tailoring of the communication strategy to the engagement goal. And, in all cases, a clear message is vital. This topic (and others) will be the focus of the next section. -->","title":"How should you engage?"},{"location":"ped/chapter3/how/#education-and-outreach-getting-creative","text":"The table at the start of this section outlines several methods for informing , but these are not the only ones that are relevant in the context of science and technology education and outreach. Other examples include, Education and Outreach Websites and social media Presentations Posters and displays Exhibitions Theatre, Film, and Documentaries Festivals And, some forms of education and outreach can be very creative. Consider the following two examples: === \"Deadinburgh\" ![Image title](https://www.publicengagement.ac.uk/sites/default/files/case-study/deadinburgh-136_2.jpg) \"An unknown pathogen ravages Scotland\u2019s capital, turning the unlucky souls into bloodthirsty ambling beasts. You are one of the last uninfected citizens in a city under martial law, cut off from the rest of the UK. Now, with help from real scientists, you have only hours to decide how to save Edinburgh, and perhaps the world. The Enlightenment Caf\u00e9: Deadinburgh, produced by LAStheatre, introduces the audience to the worlds of epidemiology and biomedical science through a night of immersive theatre. In a theatrical world, with actors playing the infected hordes and besieged soldiers, the audience meet genuine scientists using real science to solve a fictitious disease. In the end the audience must decide whether to destroy the city, cull the infected, or search for a cure; the fate of the city is in their hands. Through the outbreak of a zombie epidemic Deadinburgh asks \u2018what does it really mean to be human\u2019 whilst offering parallels with real life science and procedures for managing disease outbreaks.\" [From the National Co-ordinating Centre for Public Engagement](https://www.publicengagement.ac.uk/case-studies/enlightenment-cafe-deadinburgh) === \"Billenium (Future Places Toolkit)\" <div class=\"result\" markdown> ![Image title](https://www.scienceopen.com/document_file/67887571-d27c-41b9-b3ad-6723886228bf/ScienceOpen/image/rfa05020003_fig2.jpg){ align=left width=300 } Billennium was a mobile augmented reality (AR) project that took place in Bristol, UK in 2018. Participants used mobile devices to be guided on an AR tour, not of their city in the present day, but of possible architectural futures. The tour was designed to help promote members of the public engage in discussion and futures thinking about how their city could be design together. [(Clarke, 2021)](https://www.scienceopen.com/hosted-document?doi=10.14324/RFA.05.2.03)[@clarke2021] </div>","title":"Education and Outreach: Getting Creative"},{"location":"ped/chapter3/when/","text":"When should you engage? \u00b6 A complete answer to the question of when you should engage will, ultimately, depend on the specific details of your project. For instance, if your project involves the collection of experimental data from research subjects, then a significant part of the engagement will likely happen early on in your projects lifecycle. In contrast, if your project only requires communication of results and findings (e.g., for the goal of increased public awareness), then engagement may happen towards the end. The goal in this chapter is not to provide an exhaustive list or flowchart that can help you answer the question for any project\u2014this would not be feasible. Instead, what we will be exploring are two frameworks that can help you answer this question through a series of deliberative processes. The first of these frameworks is based on a model of a typical ML/AI project lifecycle. Although most relevant in the context of innovation projects, it can also play a valuable role in research projects, such as those that use ML to support data science. The ML/AI Project Lifecycle \u00b6 A significant portion of modern AI methods are data-driven and rely on various types of machine learning to identify patterns in the data and construct predictive models that can classify, forecast, or optimise goals (among other things). 1 As such, data science projects are increasingly turning to machine learning algorithms to help generate knowledge or support scientific processes of discovery (e.g., drug discovery). Although a wide range of differences exist between research and innovation projects, the following model is a useful abstraction for reflecting on the typical activities associated with the lifecycle of a project that involves some form of ML or AI. As the above figure demonstrates, we can split a typical project into three over-arching stages: Project Design Model Development System Deployment 'Project Design' comprises those activities that set the foundation for the practical development of the model (e.g., desk-based research, experimental design, data cleaning, exploratory data analysis). 'Model Development' captures the technical processes of building a predictive model, as well as the important process of documenting how it was developed. This is not just important for the reproducibility of science, but is also vital for innovation (e.g. for regulatory compliance). Finally, 'System Deployment' captures the implementation and use of a model within a sociotechnical system. In the context of a scientific research project, this may include the application of the model to additional datasets to generate insights or knowledge. Whereas in the context of a commercial organisation, this could refer to a wide range of use cases (e.g., recommendation system for online bookings, predictive model for diagnoses in healthcare). This macroscopic perspective is helpful as a heuristic, but the main benefits of this model for facilitating pubic engagement comes when we look at the lower-level stages. Project Design Stages \u00b6 Stage Description Public Engagement Relevance Project Planning Preliminary activities designed to help scope out the aims, objectives, and processes involved with the project, including potential risks and benefits. May include impact assessment activities designed to identify possible risks to affected users (e.g., safety risks, data privacy violations). Problem Formulation The formulation of a clear statement about the over-arching problem the system or project addresses (e.g., a research statement or system specification) and a lower-level description of the computational procedure that instantiates it. Clarity on the problem or task being pursued can help a project team whether the system is likely to be socially acceptable prior to engagement (e.g. a facial recognition system deployed in a public space to monitor frequent visitors) Data Extraction or Procurement The design of an experimental method or decisions about data gathering and collection, based on the planning and problem formulation from the previous steps. Members of the public may have differing views on what data (and the methods by which they are collected) are acceptable to collect. Data Analysis Stages of exploratory and confirmatory data analysis designed to help researchers or developers identify relevant associations between input variables and target variables. Having an understanding of your data prior to model development can help identify whether groups of stakeholders have been systematically overlooked (i.e. sources of missing data). Model Development Stages \u00b6 Stage Description Public Engagement Relevance Preprocessing and Feature Engineering A process of cleaning, normalising, and refactoring data into the features that will be used in model training and testing, as well as the features that may be used in the final system. Choices made at this stage can affect the overall interpretability of a model, which can affect how easily users or stakeholders will be able to engage with the final system. Model Selection and Training The selection of a particular algorithm (or multiple algorithms) for training the model. Choices made at this stage can affect the overall interpretability of a model, which can affect how easily users or stakeholders will be able to engage with the final system. Model Testing and Validation Testing the model against a variety of metrics, which may include those that assess how accurate a model is for different sub-groups of a population. This is important where issues of fairness or equality may arise. If a model is more performant for one group than another, this should be discussed with the affected and impacted stakeholders during engagement. Model Documentation A process of documenting both the formal and non-formal properties of both the model and the processes by which it was developed (e.g., source of data, algorithms used, evaluation metrics). Access to documented evidence can be useful for a wide variety of public engagement goals, including general communication of results to evidence to support deliberative exercises. System Deployment Stages \u00b6 Stage Description Public Engagement Relevance Model Productionalisation The process of putting a model into production. That is, implementing a model within a system that enables and structures interaction with the model (e.g. a recommender system that converts a user's existing movie ratings into recommendations for future watches. Engaging affected stakeholders can help identify challenges or barriers to productionalisation that have not been anticipated (e.g., approval processes, software compatibility, user experience concerns). User Training Training for those individuals or groups who are either required to operate a data-driven system (perhaps in a safety critical context) or who are likely to use the system (e.g. consumers). Where a research or innovation project results in a change to existing processes, it is important that the users are engaged in order to ensure that the system is used correctly. Otherwise, its overall efficacy can be significantly impaired. System Use and Monitoring Ongoing monitoring and feedback from the system, either automated or probed, to ensure that issues such as model drift have not affected performance or resulted in harms to individuals or groups. Ongoing engagement with users and stakeholders can help identify whether the original project's objectives have been met, or if the model/system are still performing as intended. Model Updating or Deprovisioning An algorithmic model that that adapts its behaviour over time or context may require updating or deprovisioning (i.e. removing from the production environment). If the updating or deprovisioning results in a loss of service\u2014possibly to a business critical system\u2014then users and stakeholders will likely need to be consulted, either to ensure no harms arise or to identify the steps needed to bring a new or revised project to fruition. As we will see shortly, the above framework helps establish a common reference point that can anchor reflection and deliberative activities related to public engagement. The following illustrative example helps to demonstrate this point. Shaping Attitudes to Public Policy A group of researchers working for a news organisation are carrying out a project to explore how the political attitudes of members of the public may be influenced by algorithmically generated articles. One of their goals is to determine whether the emotional valence of an article affects the likelihood of the reader agreeing with or disagreeing with public policies that are being discussed by the news articles. Using the AI project lifecycle model, they identify three initial stages and activities during their preliminary project planning: Stakeholder identification and engagement to support experimental design (Project Planning) Participant data gathering and informed consent (Data Extraction or Procurement) Experiment and participant feedback (Model Testing and Validation) However, when they begin the initial stakeholder identification and engagement they hit a roadblock. During discussion and deliberation with potential users of the news platform, the research team ask the participants to share their attitudes towards algorithmically generated content. A majority of the participants have a critical attitude towards the use of AI to generate news contents, and express strong disapproval of the proposed idea of influencing attitudes towards public policies through what they see as \"emotional manipulation\". On the basis of this feedback, the research team decide to change their research plan by co-designing an alternate system with the participants. The revised system still uses generative AI, but as a decision support for a human journalist. The team also add a further engagement activity during the model documentation stage to help mitigate any further aversion towards the system by ensuring sufficient public understanding and awareness of how the system operates. We will reference this model and its constituent stages repeatedly as we continue through the remainder of the course. Stakeholder Analysis \u00b6 Regardless of the type of engagement, a core part of any public engagement is knowing your audience. Important Ask \"who\" before \"how\"! This section will explore a three-step process, which begins with 'Project Planning' and is designed to help you identify and understand your audience (or, stakeholders), summarised in the following graphic. Principle of Proportionality The following steps are designed to be fairly exhaustive. Therefore, it is important to recognise that they represent an ideal, which will need to be tailored to the specifics of a research and innovation project. In short, some questions or forms of engagement/evidence gathering may not be required for particular research or innovation projects. For example, a simple research project focusing on improving data literacy will not require the same types of extensive impact assessments as an innovation project developing a new safety critical AI system for use in healthcare. 1 Preliminary Project Scoping and Stakeholder Analysis \u00b6 The first step occurs within the project team and involves a number of questions or self-assessments that help lay the groundwork for the remainder of the project: [x] outline key project components, including [x] a high-level description of the ML/AI system being employed [x] the domain or context of use [x] the data to be used in the project (e.g. for model training) [x] identify individuals or groups who may be affected by your project [x] identify individuals or groups who may affect your project [x] scope potential stakeholder impacts [x] evaluate the salience and contextual characteristics of identified stakeholders The answers and evidence accumulated from answering these questions will help your team with subsequent activities. 2 Positionality Reflection \u00b6 All individual human beings come from unique places, experiences, and life contexts that have shaped their thinking and perspectives. Reflecting on this variation can help us understand how our viewpoints might differ from those around us, and from those who have diverging cultural and socioeconomic backgrounds and life experiences. Social scientists have long referred to this kind of self-locating reflection as \u201cpositionality.\u201d When team members take their own positionalities into account, and make this explicit, they can better grasp how the influence of their respective social and cultural positions may affect the engagement process. The following 'positionality matrix' is designed to help with this process: An example of how an individual's or group's characteristics may affect a research or innovation project is if a project team fail to identify how their own educational backgrounds enable them to understand and comprehend technical or complex concepts with comparative ease, or how the lack of cultural diversity in the team prevent them from recognising the significance of alternative values (e.g., prioritising technological policies that support social cohesion over individual autonomy). 3 Stakeholder Engagement Objectives and Methods \u00b6 The final step is to establish engagement objectives that enable the appropriate degree of stakeholder engagement and co-production in project evaluation, and methods that support the achievement of defined objectives. We will look at the latter of these in detail in the next section, so let's just address the former here. If we think back to the first chapter, and the different models of participation (e.g., Arnstein's ladder of engagement)[@arnstein1969], then we can identify the following four goals that can help us specify our project's objectives: The use of these goals to support the identification of engagement objectives should also be informed by a) the following variations on participation, and also b) the methods of participation available to you and those who you are engaging (see next chapter ) Degree of Participation Description Means of Participation Level of Agency :material-information: INFORM Stakeholders are made aware of decisions and developments. External input is not sought out. Information flows in one direction. This an be done through newsletters, the post, app notifications or community forums. :material-battery-low: LOW Stakeholders are considered information subjects rather than active agents. :octicons-comment-discussion-16: CONSULT Stakeholders can voice their views on pre-determined areas of focus, which are considered in decision-making. Engagement occurs through online surveys or short phone interviews, door-to- door or in public spaces. Broader listening events can support consultations. :material-battery-low: LOW Stakeholders are included as sources of information input under narrow, highly controlled conditions of participation. :material-handshake: PARTNER Stakeholders and teams share agency over the determination of areas of focus and decision making. External input is sought out for collaboration and co- production. Stakeholders are collaborators in projects. They are engaged trough focus groups. :material-battery-medium: MODERATE Stakeholders exercise a moderate level of agency in helping to set agendas through collaborative decision making. :material-podium: EMPOWER Stakeholders are engaged with as decision-makers and are expected to gather pertinent information and be proactive in co-operation. Co-production exercises occur through citizens\u2019 juries, citizens\u2019 assemblies, and participatory co-design. Teams provide support for stakeholders\u2019 decision making. :material-battery-high: HIGH Stakeholders exercise a high level of agency and control over agenda-setting and decision making. This is in contrast to symbolic or rules-based methods where a human is required to program the underlying logic that determines the machine's behaviour. \u21a9","title":"When should you engage"},{"location":"ped/chapter3/when/#when-should-you-engage","text":"A complete answer to the question of when you should engage will, ultimately, depend on the specific details of your project. For instance, if your project involves the collection of experimental data from research subjects, then a significant part of the engagement will likely happen early on in your projects lifecycle. In contrast, if your project only requires communication of results and findings (e.g., for the goal of increased public awareness), then engagement may happen towards the end. The goal in this chapter is not to provide an exhaustive list or flowchart that can help you answer the question for any project\u2014this would not be feasible. Instead, what we will be exploring are two frameworks that can help you answer this question through a series of deliberative processes. The first of these frameworks is based on a model of a typical ML/AI project lifecycle. Although most relevant in the context of innovation projects, it can also play a valuable role in research projects, such as those that use ML to support data science.","title":"When should you engage?"},{"location":"ped/chapter3/when/#the-mlai-project-lifecycle","text":"A significant portion of modern AI methods are data-driven and rely on various types of machine learning to identify patterns in the data and construct predictive models that can classify, forecast, or optimise goals (among other things). 1 As such, data science projects are increasingly turning to machine learning algorithms to help generate knowledge or support scientific processes of discovery (e.g., drug discovery). Although a wide range of differences exist between research and innovation projects, the following model is a useful abstraction for reflecting on the typical activities associated with the lifecycle of a project that involves some form of ML or AI. As the above figure demonstrates, we can split a typical project into three over-arching stages: Project Design Model Development System Deployment 'Project Design' comprises those activities that set the foundation for the practical development of the model (e.g., desk-based research, experimental design, data cleaning, exploratory data analysis). 'Model Development' captures the technical processes of building a predictive model, as well as the important process of documenting how it was developed. This is not just important for the reproducibility of science, but is also vital for innovation (e.g. for regulatory compliance). Finally, 'System Deployment' captures the implementation and use of a model within a sociotechnical system. In the context of a scientific research project, this may include the application of the model to additional datasets to generate insights or knowledge. Whereas in the context of a commercial organisation, this could refer to a wide range of use cases (e.g., recommendation system for online bookings, predictive model for diagnoses in healthcare). This macroscopic perspective is helpful as a heuristic, but the main benefits of this model for facilitating pubic engagement comes when we look at the lower-level stages.","title":"The ML/AI Project Lifecycle"},{"location":"ped/chapter3/when/#project-design-stages","text":"Stage Description Public Engagement Relevance Project Planning Preliminary activities designed to help scope out the aims, objectives, and processes involved with the project, including potential risks and benefits. May include impact assessment activities designed to identify possible risks to affected users (e.g., safety risks, data privacy violations). Problem Formulation The formulation of a clear statement about the over-arching problem the system or project addresses (e.g., a research statement or system specification) and a lower-level description of the computational procedure that instantiates it. Clarity on the problem or task being pursued can help a project team whether the system is likely to be socially acceptable prior to engagement (e.g. a facial recognition system deployed in a public space to monitor frequent visitors) Data Extraction or Procurement The design of an experimental method or decisions about data gathering and collection, based on the planning and problem formulation from the previous steps. Members of the public may have differing views on what data (and the methods by which they are collected) are acceptable to collect. Data Analysis Stages of exploratory and confirmatory data analysis designed to help researchers or developers identify relevant associations between input variables and target variables. Having an understanding of your data prior to model development can help identify whether groups of stakeholders have been systematically overlooked (i.e. sources of missing data).","title":"Project Design Stages"},{"location":"ped/chapter3/when/#model-development-stages","text":"Stage Description Public Engagement Relevance Preprocessing and Feature Engineering A process of cleaning, normalising, and refactoring data into the features that will be used in model training and testing, as well as the features that may be used in the final system. Choices made at this stage can affect the overall interpretability of a model, which can affect how easily users or stakeholders will be able to engage with the final system. Model Selection and Training The selection of a particular algorithm (or multiple algorithms) for training the model. Choices made at this stage can affect the overall interpretability of a model, which can affect how easily users or stakeholders will be able to engage with the final system. Model Testing and Validation Testing the model against a variety of metrics, which may include those that assess how accurate a model is for different sub-groups of a population. This is important where issues of fairness or equality may arise. If a model is more performant for one group than another, this should be discussed with the affected and impacted stakeholders during engagement. Model Documentation A process of documenting both the formal and non-formal properties of both the model and the processes by which it was developed (e.g., source of data, algorithms used, evaluation metrics). Access to documented evidence can be useful for a wide variety of public engagement goals, including general communication of results to evidence to support deliberative exercises.","title":"Model Development Stages"},{"location":"ped/chapter3/when/#system-deployment-stages","text":"Stage Description Public Engagement Relevance Model Productionalisation The process of putting a model into production. That is, implementing a model within a system that enables and structures interaction with the model (e.g. a recommender system that converts a user's existing movie ratings into recommendations for future watches. Engaging affected stakeholders can help identify challenges or barriers to productionalisation that have not been anticipated (e.g., approval processes, software compatibility, user experience concerns). User Training Training for those individuals or groups who are either required to operate a data-driven system (perhaps in a safety critical context) or who are likely to use the system (e.g. consumers). Where a research or innovation project results in a change to existing processes, it is important that the users are engaged in order to ensure that the system is used correctly. Otherwise, its overall efficacy can be significantly impaired. System Use and Monitoring Ongoing monitoring and feedback from the system, either automated or probed, to ensure that issues such as model drift have not affected performance or resulted in harms to individuals or groups. Ongoing engagement with users and stakeholders can help identify whether the original project's objectives have been met, or if the model/system are still performing as intended. Model Updating or Deprovisioning An algorithmic model that that adapts its behaviour over time or context may require updating or deprovisioning (i.e. removing from the production environment). If the updating or deprovisioning results in a loss of service\u2014possibly to a business critical system\u2014then users and stakeholders will likely need to be consulted, either to ensure no harms arise or to identify the steps needed to bring a new or revised project to fruition. As we will see shortly, the above framework helps establish a common reference point that can anchor reflection and deliberative activities related to public engagement. The following illustrative example helps to demonstrate this point. Shaping Attitudes to Public Policy A group of researchers working for a news organisation are carrying out a project to explore how the political attitudes of members of the public may be influenced by algorithmically generated articles. One of their goals is to determine whether the emotional valence of an article affects the likelihood of the reader agreeing with or disagreeing with public policies that are being discussed by the news articles. Using the AI project lifecycle model, they identify three initial stages and activities during their preliminary project planning: Stakeholder identification and engagement to support experimental design (Project Planning) Participant data gathering and informed consent (Data Extraction or Procurement) Experiment and participant feedback (Model Testing and Validation) However, when they begin the initial stakeholder identification and engagement they hit a roadblock. During discussion and deliberation with potential users of the news platform, the research team ask the participants to share their attitudes towards algorithmically generated content. A majority of the participants have a critical attitude towards the use of AI to generate news contents, and express strong disapproval of the proposed idea of influencing attitudes towards public policies through what they see as \"emotional manipulation\". On the basis of this feedback, the research team decide to change their research plan by co-designing an alternate system with the participants. The revised system still uses generative AI, but as a decision support for a human journalist. The team also add a further engagement activity during the model documentation stage to help mitigate any further aversion towards the system by ensuring sufficient public understanding and awareness of how the system operates. We will reference this model and its constituent stages repeatedly as we continue through the remainder of the course.","title":"System Deployment Stages"},{"location":"ped/chapter3/when/#stakeholder-analysis","text":"Regardless of the type of engagement, a core part of any public engagement is knowing your audience. Important Ask \"who\" before \"how\"! This section will explore a three-step process, which begins with 'Project Planning' and is designed to help you identify and understand your audience (or, stakeholders), summarised in the following graphic. Principle of Proportionality The following steps are designed to be fairly exhaustive. Therefore, it is important to recognise that they represent an ideal, which will need to be tailored to the specifics of a research and innovation project. In short, some questions or forms of engagement/evidence gathering may not be required for particular research or innovation projects. For example, a simple research project focusing on improving data literacy will not require the same types of extensive impact assessments as an innovation project developing a new safety critical AI system for use in healthcare.","title":"Stakeholder Analysis"},{"location":"ped/chapter3/when/#1-preliminary-project-scoping-and-stakeholder-analysis","text":"The first step occurs within the project team and involves a number of questions or self-assessments that help lay the groundwork for the remainder of the project: [x] outline key project components, including [x] a high-level description of the ML/AI system being employed [x] the domain or context of use [x] the data to be used in the project (e.g. for model training) [x] identify individuals or groups who may be affected by your project [x] identify individuals or groups who may affect your project [x] scope potential stakeholder impacts [x] evaluate the salience and contextual characteristics of identified stakeholders The answers and evidence accumulated from answering these questions will help your team with subsequent activities.","title":"1 Preliminary Project Scoping and Stakeholder Analysis"},{"location":"ped/chapter3/when/#2-positionality-reflection","text":"All individual human beings come from unique places, experiences, and life contexts that have shaped their thinking and perspectives. Reflecting on this variation can help us understand how our viewpoints might differ from those around us, and from those who have diverging cultural and socioeconomic backgrounds and life experiences. Social scientists have long referred to this kind of self-locating reflection as \u201cpositionality.\u201d When team members take their own positionalities into account, and make this explicit, they can better grasp how the influence of their respective social and cultural positions may affect the engagement process. The following 'positionality matrix' is designed to help with this process: An example of how an individual's or group's characteristics may affect a research or innovation project is if a project team fail to identify how their own educational backgrounds enable them to understand and comprehend technical or complex concepts with comparative ease, or how the lack of cultural diversity in the team prevent them from recognising the significance of alternative values (e.g., prioritising technological policies that support social cohesion over individual autonomy).","title":"2 Positionality Reflection"},{"location":"ped/chapter3/when/#3-stakeholder-engagement-objectives-and-methods","text":"The final step is to establish engagement objectives that enable the appropriate degree of stakeholder engagement and co-production in project evaluation, and methods that support the achievement of defined objectives. We will look at the latter of these in detail in the next section, so let's just address the former here. If we think back to the first chapter, and the different models of participation (e.g., Arnstein's ladder of engagement)[@arnstein1969], then we can identify the following four goals that can help us specify our project's objectives: The use of these goals to support the identification of engagement objectives should also be informed by a) the following variations on participation, and also b) the methods of participation available to you and those who you are engaging (see next chapter ) Degree of Participation Description Means of Participation Level of Agency :material-information: INFORM Stakeholders are made aware of decisions and developments. External input is not sought out. Information flows in one direction. This an be done through newsletters, the post, app notifications or community forums. :material-battery-low: LOW Stakeholders are considered information subjects rather than active agents. :octicons-comment-discussion-16: CONSULT Stakeholders can voice their views on pre-determined areas of focus, which are considered in decision-making. Engagement occurs through online surveys or short phone interviews, door-to- door or in public spaces. Broader listening events can support consultations. :material-battery-low: LOW Stakeholders are included as sources of information input under narrow, highly controlled conditions of participation. :material-handshake: PARTNER Stakeholders and teams share agency over the determination of areas of focus and decision making. External input is sought out for collaboration and co- production. Stakeholders are collaborators in projects. They are engaged trough focus groups. :material-battery-medium: MODERATE Stakeholders exercise a moderate level of agency in helping to set agendas through collaborative decision making. :material-podium: EMPOWER Stakeholders are engaged with as decision-makers and are expected to gather pertinent information and be proactive in co-operation. Co-production exercises occur through citizens\u2019 juries, citizens\u2019 assemblies, and participatory co-design. Teams provide support for stakeholders\u2019 decision making. :material-battery-high: HIGH Stakeholders exercise a high level of agency and control over agenda-setting and decision making. This is in contrast to symbolic or rules-based methods where a human is required to program the underlying logic that determines the machine's behaviour. \u21a9","title":"3 Stakeholder Engagement Objectives and Methods"},{"location":"ped/chapter4/","text":"Practical Guidance \u00b6 Introduction \u00b6 This chapter builds on the guidance for planning and facilitating different types of public engagement by delving deeper into some of the practical tools and methods. In the first section we look at the communication of data science and AI as a process of storytelling. This includes a brief look at data visualisation tools; the need to consider the limits of your audience's attention; and the importance of narrative structure. In the second section we consider the challenge of communicating uncertainty. Science and technology communication often requires the communication of probabilistic information, statistics, and risky or uncertain outcomes. There are a range of challenges associated with this form of communication when it comes to public engagement. Therefore, we take some time to identify what these challenges are, and what we can do to overcome them. Chapter Outline \u00b6 Storytelling with Data Communicating Uncertainty Learning Objectives By the end of this chapter you should a) have a good appreciation of how the perspective of storytelling can help you develop more effective forms of communication and engagement, and b) understand some of the challenges that arise when attempting to communicate probabilistic or statistical information to members of the public.","title":"Introduction"},{"location":"ped/chapter4/#practical-guidance","text":"","title":"Practical Guidance"},{"location":"ped/chapter4/#introduction","text":"This chapter builds on the guidance for planning and facilitating different types of public engagement by delving deeper into some of the practical tools and methods. In the first section we look at the communication of data science and AI as a process of storytelling. This includes a brief look at data visualisation tools; the need to consider the limits of your audience's attention; and the importance of narrative structure. In the second section we consider the challenge of communicating uncertainty. Science and technology communication often requires the communication of probabilistic information, statistics, and risky or uncertain outcomes. There are a range of challenges associated with this form of communication when it comes to public engagement. Therefore, we take some time to identify what these challenges are, and what we can do to overcome them.","title":"Introduction"},{"location":"ped/chapter4/#chapter-outline","text":"Storytelling with Data Communicating Uncertainty Learning Objectives By the end of this chapter you should a) have a good appreciation of how the perspective of storytelling can help you develop more effective forms of communication and engagement, and b) understand some of the challenges that arise when attempting to communicate probabilistic or statistical information to members of the public.","title":"Chapter Outline"},{"location":"ped/chapter4/storytelling/","text":"Storytelling with Data \u00b6 \"Data tells a story!\" This phrase, which gets thrown around a lot, does contain a kernel of truth. But telling a story with data is not always straightforward, and data do not tell stories on their own. It takes careful analysis and informed deliberation to make sure that data tell the right story, and that the story is engaging to those who want or need to hear the story. Consider the following graph, for instance: Now, ask yourself the following questions: Questions What story do you think this graph is trying to tell? Which part of the graph caught your attention initially? Cole Nussbaumer Knaflic's excellent book, Storytelling with Data is full of examples such as this one, in which badly designed or ordinary graphs are transformed into engaging and insightful visualisations that help tell a story and realise various goals.[@knaflic2015] The previous graph, for example, is altered into the following one, where a) more supportive messaging helps explain how a new program improved children's interest and engagement in science, and b) better use of colours help to direct the audience's attention to the values of significance (i.e., the increased percentages of children who were either 'interested' or 'excited' in science following the intervention). In this section, we are going to look at three lessons (based on Knaflic's book)[@knaflic2015] to help tell more engaging stories with data: Choose the right tool for the job Understand the limits of attention Construct a clear narrative Choosing the right tools \u00b6 You would not choose a hammer to drill in a screw. Sure, it may get the screw into the wall, but it's certainly not the most effective tool for the job. In the context of data visualisation, there are no shortages of graphs (tools) and libraries (toolkits) to help you create a multitude of different graphs. For example, the popular Seaborn data visualization library for Python: But, similar to the hammer analogy, it's easy to end up with choice paralysis in situations like this, especially when there are so many options available. Example Although far from being an exhaustive list, the following options are a varied set of examples for data visualisation tools: Tableau : powerful tool for data analysis, BI, and visualisation Canva : an online graphic design platform with a simple interface D3.js : a javascript library well suited to web-based visuals Google Charts : another free option for embedding (many different) charts into webpages Datawrapper : a popular option among cvommunicators (e.g. journalists) with a good range of interactive and responsive charts Bokeh : an Python library for creating interactive visualisations with a variant for R Flourish : easily turn your data into animated charts, maps, and interactive stories. So, how do you choose the right tool? How do you select the right visualisation? Well, simply put, you have a clear answer to the questions from the preceding chapters: Who is your audience? What matters to them (or, what do they value? Why are you engaging them? What is your goal? When you can satisfactorily answer these questions you will probably know which tool is the right one top use. However, there are still a couple of hazards that could get in your way. ATTENTION!!! \u00b6 You're about to take part in a short experiment. Watch the following video: Did you spot the change? As the video emphasises, our limited cognitive capacity forces us to pay attention to the details that we expect to be most salient to the current tasks we are engaged in. When you are communicating with or engaging an audience, they will also be in a similar predicament\u2014forced to \"make (sub-conscious) choices\" about what to pay attention to. Therefore, it is important for you to have an understanding of how you can reduce cognitive load and make it easier for your audience to pay attention to what matters . Preattentive attributes are one way to achieve this. Knaflic's book, again, has useful guidance here about the role of preattentive attributes \u2014elements of visual design that are rapidly processed by the lowest levels of our visual systems, signifying something of salience to the individual. The following 12 preattentive attributes are well-known among designers, and often exploited by those in advertising! === \"Graph with no preattentive attributes\" ![Graph A without colour](../../assets/images/graphics/coloura.png) === \"Graph with colour added\" ![Graph B with colour](../../assets/images/graphics/colourb.png) === \"Graph using colour to build hierarchy\" ![Graph C using colour to build hierarchy](../../assets/images/graphics/colourc.png) Constructing Narrative Structure \u00b6 The Hero's journey is a well-known template in literature and story-telling more generally. In brief, it identifies several stages for a protagonist's journey, starting with a departure (or, call to adventure) that serves as a hook, which leads into a series of trials, initiations, or challenges for the protagonist to overcome, before returning home as a changed or transformed individual. This template can be seen in many areas of popular culture, ranging from science-fiction films (e.g. Star Wars) to classic works of literature (e.g. Jane Eyre). There are, of course, many challenges to the validity of this template and its application to specific stories\u2014in both academia and geek culture! However, as a scaffold for both constructing and interpreting meaning the template serves a purpose. Unfortunately, there is no comparable template that we can point to for building narrative in the context of public engagement. But there are principles that can help steer us in the right direction, which include some we have already explored. Consider the following visual communication from the Education Endowment Foundation\u2014an independent charity with the goal of improving educational attainment of the poorest pupils in English schools. The purpose of this graphic is to clearly summarise the evidence base for different education interventions, by focusing on three properties: Cost Strength of Evidence Impact Each of these three elements helps to build and communicate a clear narrative: Our research has uncovered a variety of interventions and policy options for improving education. These options differ in three ways: Their cost The strength of their supporting evidence The potential impact from implementing the chosen policy The information we have gathered and organised can support evidence-based policy making Obviously there is a lot more behind these three properties, but they serve as an accessible overview to help direct the attention of the audience. This is achieved through the use of clear visuals, clear messaging (and narrative structure), and a clear understanding of the audience and goals for the project\u2014in the above case, policy-makers. Moreover, the message is linked to an implied causal process\u2014a key component in narrative. Specifically, doing A (implementing a policy) is likely to cause B (the associated impact). This latter point may strike data scientists as problematic, as the idea that correlation does not imply causation is drilled into all scientists early on in their career. As such, suggesting evidence of causation is typically seen as an instance of overstating research findings. But when it comes to good storytelling and narrative, an underlying causal process is often implied or assumed by the audience. This is not necessarily a problem, as long as you are clear to differentiate between any causal processes referred to in your research, and the causal account assumed in your engagement's narrative. Although not a replacement for the Hero's journey, the following set of principles from Northeastern University serve as both a good summary of what we have learned so far, and also reinforce the importance of clear narrative structure (see principles in bold): 10 Principles for How to Communicate Effectively Know your audience Identify the goals of communication Start with the most important information Avoid jargon Be relatable Provide visuals Stick to three points Talk about the scientific process Focus on the bigger impact Develop an elevator pitch","title":"Storytelling with Data"},{"location":"ped/chapter4/storytelling/#storytelling-with-data","text":"\"Data tells a story!\" This phrase, which gets thrown around a lot, does contain a kernel of truth. But telling a story with data is not always straightforward, and data do not tell stories on their own. It takes careful analysis and informed deliberation to make sure that data tell the right story, and that the story is engaging to those who want or need to hear the story. Consider the following graph, for instance: Now, ask yourself the following questions: Questions What story do you think this graph is trying to tell? Which part of the graph caught your attention initially? Cole Nussbaumer Knaflic's excellent book, Storytelling with Data is full of examples such as this one, in which badly designed or ordinary graphs are transformed into engaging and insightful visualisations that help tell a story and realise various goals.[@knaflic2015] The previous graph, for example, is altered into the following one, where a) more supportive messaging helps explain how a new program improved children's interest and engagement in science, and b) better use of colours help to direct the audience's attention to the values of significance (i.e., the increased percentages of children who were either 'interested' or 'excited' in science following the intervention). In this section, we are going to look at three lessons (based on Knaflic's book)[@knaflic2015] to help tell more engaging stories with data: Choose the right tool for the job Understand the limits of attention Construct a clear narrative","title":"Storytelling with Data"},{"location":"ped/chapter4/storytelling/#choosing-the-right-tools","text":"You would not choose a hammer to drill in a screw. Sure, it may get the screw into the wall, but it's certainly not the most effective tool for the job. In the context of data visualisation, there are no shortages of graphs (tools) and libraries (toolkits) to help you create a multitude of different graphs. For example, the popular Seaborn data visualization library for Python: But, similar to the hammer analogy, it's easy to end up with choice paralysis in situations like this, especially when there are so many options available. Example Although far from being an exhaustive list, the following options are a varied set of examples for data visualisation tools: Tableau : powerful tool for data analysis, BI, and visualisation Canva : an online graphic design platform with a simple interface D3.js : a javascript library well suited to web-based visuals Google Charts : another free option for embedding (many different) charts into webpages Datawrapper : a popular option among cvommunicators (e.g. journalists) with a good range of interactive and responsive charts Bokeh : an Python library for creating interactive visualisations with a variant for R Flourish : easily turn your data into animated charts, maps, and interactive stories. So, how do you choose the right tool? How do you select the right visualisation? Well, simply put, you have a clear answer to the questions from the preceding chapters: Who is your audience? What matters to them (or, what do they value? Why are you engaging them? What is your goal? When you can satisfactorily answer these questions you will probably know which tool is the right one top use. However, there are still a couple of hazards that could get in your way.","title":"Choosing the right tools"},{"location":"ped/chapter4/storytelling/#attention","text":"You're about to take part in a short experiment. Watch the following video: Did you spot the change? As the video emphasises, our limited cognitive capacity forces us to pay attention to the details that we expect to be most salient to the current tasks we are engaged in. When you are communicating with or engaging an audience, they will also be in a similar predicament\u2014forced to \"make (sub-conscious) choices\" about what to pay attention to. Therefore, it is important for you to have an understanding of how you can reduce cognitive load and make it easier for your audience to pay attention to what matters . Preattentive attributes are one way to achieve this. Knaflic's book, again, has useful guidance here about the role of preattentive attributes \u2014elements of visual design that are rapidly processed by the lowest levels of our visual systems, signifying something of salience to the individual. The following 12 preattentive attributes are well-known among designers, and often exploited by those in advertising! === \"Graph with no preattentive attributes\" ![Graph A without colour](../../assets/images/graphics/coloura.png) === \"Graph with colour added\" ![Graph B with colour](../../assets/images/graphics/colourb.png) === \"Graph using colour to build hierarchy\" ![Graph C using colour to build hierarchy](../../assets/images/graphics/colourc.png)","title":"ATTENTION!!!"},{"location":"ped/chapter4/storytelling/#constructing-narrative-structure","text":"The Hero's journey is a well-known template in literature and story-telling more generally. In brief, it identifies several stages for a protagonist's journey, starting with a departure (or, call to adventure) that serves as a hook, which leads into a series of trials, initiations, or challenges for the protagonist to overcome, before returning home as a changed or transformed individual. This template can be seen in many areas of popular culture, ranging from science-fiction films (e.g. Star Wars) to classic works of literature (e.g. Jane Eyre). There are, of course, many challenges to the validity of this template and its application to specific stories\u2014in both academia and geek culture! However, as a scaffold for both constructing and interpreting meaning the template serves a purpose. Unfortunately, there is no comparable template that we can point to for building narrative in the context of public engagement. But there are principles that can help steer us in the right direction, which include some we have already explored. Consider the following visual communication from the Education Endowment Foundation\u2014an independent charity with the goal of improving educational attainment of the poorest pupils in English schools. The purpose of this graphic is to clearly summarise the evidence base for different education interventions, by focusing on three properties: Cost Strength of Evidence Impact Each of these three elements helps to build and communicate a clear narrative: Our research has uncovered a variety of interventions and policy options for improving education. These options differ in three ways: Their cost The strength of their supporting evidence The potential impact from implementing the chosen policy The information we have gathered and organised can support evidence-based policy making Obviously there is a lot more behind these three properties, but they serve as an accessible overview to help direct the attention of the audience. This is achieved through the use of clear visuals, clear messaging (and narrative structure), and a clear understanding of the audience and goals for the project\u2014in the above case, policy-makers. Moreover, the message is linked to an implied causal process\u2014a key component in narrative. Specifically, doing A (implementing a policy) is likely to cause B (the associated impact). This latter point may strike data scientists as problematic, as the idea that correlation does not imply causation is drilled into all scientists early on in their career. As such, suggesting evidence of causation is typically seen as an instance of overstating research findings. But when it comes to good storytelling and narrative, an underlying causal process is often implied or assumed by the audience. This is not necessarily a problem, as long as you are clear to differentiate between any causal processes referred to in your research, and the causal account assumed in your engagement's narrative. Although not a replacement for the Hero's journey, the following set of principles from Northeastern University serve as both a good summary of what we have learned so far, and also reinforce the importance of clear narrative structure (see principles in bold): 10 Principles for How to Communicate Effectively Know your audience Identify the goals of communication Start with the most important information Avoid jargon Be relatable Provide visuals Stick to three points Talk about the scientific process Focus on the bigger impact Develop an elevator pitch","title":"Constructing Narrative Structure"},{"location":"ped/chapter4/uncertainty/","text":"Communicating Uncertainty \u00b6 There are many forms of uncertainty that we have to consider when planning and facilitating forms of public engagement. There is the conceptual uncertainty involved with key terms that are necessary for shared knowledge and understanding (e.g. what does 'intelligence' mean in the context of AI?). There is the normative uncertainty that is implicated when deliberating about or choosing on the right or best course of action (e.g. citizen juries). And, there is the scientific uncertainty associated with data analysis, experiments, and scientific knowledge. 1 In this section, we are just going to look at the final option, but it's good to be aware of the other two. The Challenges of Scientific Uncertainty \u00b6 Roll up! Roll up! Come and see a true scientific mystery: the marvelous, mystifying marmokreb! Alright, it's not much to look at, but it is a true scientific mystery in one respect. As Michael Blastland[@blastland2020] recounts in The Hidden Half , the marmorkrebs caused quite the stir in the late 20th Century when it was discovered that lone females were able to lay eggs without fertilisation. In short, the marmorkrebs did not need to mate, such that offspring were natural clones of their mothers\u2014a process known as parthogenesis . However, it was not the parthogenic property\u2014unique among the ~15,000 species of decapod\u2014that baffled the scientific establishment. Rather, the mystery lay in what the marbled marmokrebs meant 2 for the nature-nurture debate. Because they were natural clones, the marmokrebs were ideal subjects for an experiment that raised two lineages in identical test conditions, designed to treat the environment as a control condition and investigate how and which genes contribute to observed behaviour or physiological traits. As Blastland puts it, The aim as far as humanly possible was to eliminate every variation that anyone could think of. They were born into the most boring uniformity humans could contrive. And yet, the following figure shows three marmorkrebs from the same lineage, who were raised in the exact same conditions. A figure of three marmorkrebs displaying significant variation (Reprinted from Vogt et al. 2008) Although the variation in size is striking enough, what is not shown in this image are the many other traits that differed throughout the population, such as distinct marbling or lifespan. And so it was that these seemingly simple crayfish undermined one of the key assumptions about phenotypic variation: if it isn't genes it's environment; if it isn't the environment it's genes. At this point, you may be considering possible explanations for the cause of the variation: is it epigenetics, or uncontrolled differences in the marmorkrebs environment that were overlooked by the researchers? We won't go into each possible causal explanation here, though I do suggest you read Blastland's book in case you find yourself over-confident that you know the answer! The purpose of mentioning this case of the confounding crayfish is to emphasise that there is a lot in science that we simply do not know. Sometimes, this uncertainty can be accounted for by following the scientific method and conducting ongoing experimentation. At other times, the observable phenomena place too much stress on the dominant scientific paradigm, causing the scientific community to rethink core assumptions\u2014a sociological process brought to light by the philosopher of science, Thomas Kuhn.[@kuhn1970] As researchers and developers trained in the scientific method, you are likely familiar with a variety of methods and tools for navigating scientific uncertainty. But when it comes to communicating this uncertainty to different stakeholders, the knowledge and understanding of these methods cannot be assumed. And this can lead to a breakdown in communication and trust. This is where the systematic framework from van der Bles et al. (2019)[@vanderbles2019] comes in handy. A Framework for Communicating Uncertainty \u00b6 The framework presented in (van der Bles et al., 2019)[@vanderbles2019] is grounded in a review of the, admittedly \"scattered\" empirical evidence pertaining to the psychological and behaviour effects of communicating uncertainty. Their framework comprises five stages that are summarised as follows: Who communicates what in what form to whom, and to what effect Let's (briefly) look at each of these in turn. 3 Who \u00b6 The first stage of their framework directs us to consider who is a) assessing the uncertainty, and b) who is responsible for communicating the uncertainty. As we will see, this is important for identifying the format of communication when communicating to different audiences (e.g. lay people versus those with technical expertise) but also for assessing credibility of the source reporting the information (e.g. media, scientists, commercial organisations). What \u00b6 A particularly valuable contribution from their framework is the categorisation of what the uncertainty pertains to into the following taxonomy: Object of uncertainty Facts (e.g. is the uncertainty about a specific event, such as whether there has been a data leak) Numbers (e.g. is the uncertainty about specific quantities or parameters of a model) Hypotheses (e.g. is the uncertainty about an underlying assumption in a scientific theory) Source of uncertainty Variability in population being sampled Measurement error Limited knowledge Expert disagreement Level of uncertainty Direct (i.e. is the uncertaity about the object itself) Indirect (i.e. is the uncertainty related to the quality of the evidence) Magnitude of uncertainty In what form \u00b6 The format used to communicate the uncertainty is likely to affect the audience in a variety of ways (see 'to what effect'). As the following figure indicates, this form is associated with varying levels of precision. . Decreased precision is not always undesirable. For instance, sometimes it represents a worthwhile trade-off between precision and accessibility of communication to a non-technical or time-stretched audience. However, as the diagram shows, it can sometimes spill over into forms of explicit denial. To whom \u00b6 In chapter 2 we saw how the relationship between researchers and the public can embody a variety of communicative values, but also prevent obstacles when imbalances of power arise. Similar characteristics are present in the framework from van der Bles et al. (2019)[@vanderbles2019]. characteristics of the audience (e.g. data literacy, relevant demographic variables) relationship of the audience to what is being communicated (e.g. skeptical attitude) relationship of the audience to the people doing the communication (e.g. trustworthy or credible relationship) As an example, van der Bles et al. (2019)[@vanderbles2019] cite a study from Dieckmann et al. (2017)[@dieckmann2017] that found evidence of varying responses to ambiguous information about average global surface temperatures. For audiences, who were more accepting of climate change, they took the information to be evidence of a normal distribution where higher values of surface temperature were likely. Whereas, for audiences who were less accepting of climate change, the same information was taken to reflect a uniform distribution where lower temperatures were more likely. To what effect \u00b6 The final stage of the framework considers the effect of not knowing, and identifies the following categories based on the extant empirical literature: Effect on cognition (e.g. are consistent inferences or judgements elicited from verbal statements of uncertainty) Effect on emotion (e.g. do uncertainty ranges elicit higher levels of anxiety in specific audience, rather than point estimates) Effect on trust (e.g. is perceived over-confidence in the communication of uncertainty likely to build confidence or undermine trust) Effect on decision-making (e.g. if uncertainty is communicated, will decision-making be delayed) The above framework helps to systematise our understanding of uncertainty, which can be very valuable when trying to consider how best to communicate with specific audiences or stakeholders. However, it also raises the following question: Question Given the wide range of factors involved, and the scattered nature of the empirical evidence, how should researchers choose the best course of action when communicating uncertainty? There is, again, and unfortunately, no simple answer to this question. Deciding on the right course of action is likely to be a team effort, and one which considers all of the topics we have covered so far (e.g. goals, values, audience, available methods, time and resources). There are, however, some interesting tools that link back to the first section in this chapter, which deserve a mention. === \"Plotting Hypothetical Outcomes\" <div class=\"result\" markdown> ![Animation of a hypothetical outcome plot](../../assets/images/animations/regression.gif){ align=right width=300 } One method is to use packages built into software like Python and R to generate visual animations that explicitly depict alternate or hypothetical outcomes alongside, say, the most likely model of the data generation process. See [this page for more details](uncertainty_example.ipynb). </div> === \"Interactive Heat Maps\" <div class=\"result\" markdown> ![Example climate impact map](../../assets/images/graphics/climate-map.gif){ align=left width=50% } Heat maps are commonly used for communicating probabilistic information. The [Climate Impact Lab](https://impactlab.org/map/#usmeas=change-from-hist&usyear=2020-2039&gmeas=change-from-hist&gyear=2040-2059&usvar=mortality&tab=global&gvar=mortality) take the traditional heat map a step further though by adding interactivity to allow users to alter variables and see the possible impact that emissions, for example, could have across the globe. </div> === \"Uncertainty.io\" <div class=\"result\" markdown> ![From Line by Lee Ufan (1978)](../../assets/images/graphics/from-line.jpeg){ align=right width=300 } Still in need of some creative inspiration? [Uncertainty.io](https://www.uncertainty.io/art/) is a queryable interface to references of more than 400 works of fine art that \"have a unique ability to convey uncertainty using a range of approaches and techniques.\" </div> We can split this into both epistemic and aleatory uncertainty, where the former relates to our uncertain knowledge of the world, and the latter refers to fundamental indeterminacy of the world itself. In this section we will only consider the former (i.e. epistemic uncertainty). \u21a9 An optional activity is to see who is the fastest at saying this phrase five times \ud83d\ude09. \u21a9 See the original article for further details and a review of the empirical evidence. \u21a9","title":"Communicating Uncertainty"},{"location":"ped/chapter4/uncertainty/#communicating-uncertainty","text":"There are many forms of uncertainty that we have to consider when planning and facilitating forms of public engagement. There is the conceptual uncertainty involved with key terms that are necessary for shared knowledge and understanding (e.g. what does 'intelligence' mean in the context of AI?). There is the normative uncertainty that is implicated when deliberating about or choosing on the right or best course of action (e.g. citizen juries). And, there is the scientific uncertainty associated with data analysis, experiments, and scientific knowledge. 1 In this section, we are just going to look at the final option, but it's good to be aware of the other two.","title":"Communicating Uncertainty"},{"location":"ped/chapter4/uncertainty/#the-challenges-of-scientific-uncertainty","text":"Roll up! Roll up! Come and see a true scientific mystery: the marvelous, mystifying marmokreb! Alright, it's not much to look at, but it is a true scientific mystery in one respect. As Michael Blastland[@blastland2020] recounts in The Hidden Half , the marmorkrebs caused quite the stir in the late 20th Century when it was discovered that lone females were able to lay eggs without fertilisation. In short, the marmorkrebs did not need to mate, such that offspring were natural clones of their mothers\u2014a process known as parthogenesis . However, it was not the parthogenic property\u2014unique among the ~15,000 species of decapod\u2014that baffled the scientific establishment. Rather, the mystery lay in what the marbled marmokrebs meant 2 for the nature-nurture debate. Because they were natural clones, the marmokrebs were ideal subjects for an experiment that raised two lineages in identical test conditions, designed to treat the environment as a control condition and investigate how and which genes contribute to observed behaviour or physiological traits. As Blastland puts it, The aim as far as humanly possible was to eliminate every variation that anyone could think of. They were born into the most boring uniformity humans could contrive. And yet, the following figure shows three marmorkrebs from the same lineage, who were raised in the exact same conditions. A figure of three marmorkrebs displaying significant variation (Reprinted from Vogt et al. 2008) Although the variation in size is striking enough, what is not shown in this image are the many other traits that differed throughout the population, such as distinct marbling or lifespan. And so it was that these seemingly simple crayfish undermined one of the key assumptions about phenotypic variation: if it isn't genes it's environment; if it isn't the environment it's genes. At this point, you may be considering possible explanations for the cause of the variation: is it epigenetics, or uncontrolled differences in the marmorkrebs environment that were overlooked by the researchers? We won't go into each possible causal explanation here, though I do suggest you read Blastland's book in case you find yourself over-confident that you know the answer! The purpose of mentioning this case of the confounding crayfish is to emphasise that there is a lot in science that we simply do not know. Sometimes, this uncertainty can be accounted for by following the scientific method and conducting ongoing experimentation. At other times, the observable phenomena place too much stress on the dominant scientific paradigm, causing the scientific community to rethink core assumptions\u2014a sociological process brought to light by the philosopher of science, Thomas Kuhn.[@kuhn1970] As researchers and developers trained in the scientific method, you are likely familiar with a variety of methods and tools for navigating scientific uncertainty. But when it comes to communicating this uncertainty to different stakeholders, the knowledge and understanding of these methods cannot be assumed. And this can lead to a breakdown in communication and trust. This is where the systematic framework from van der Bles et al. (2019)[@vanderbles2019] comes in handy.","title":"The Challenges of Scientific Uncertainty"},{"location":"ped/chapter4/uncertainty/#a-framework-for-communicating-uncertainty","text":"The framework presented in (van der Bles et al., 2019)[@vanderbles2019] is grounded in a review of the, admittedly \"scattered\" empirical evidence pertaining to the psychological and behaviour effects of communicating uncertainty. Their framework comprises five stages that are summarised as follows: Who communicates what in what form to whom, and to what effect Let's (briefly) look at each of these in turn. 3","title":"A Framework for Communicating Uncertainty"},{"location":"ped/chapter4/uncertainty/#who","text":"The first stage of their framework directs us to consider who is a) assessing the uncertainty, and b) who is responsible for communicating the uncertainty. As we will see, this is important for identifying the format of communication when communicating to different audiences (e.g. lay people versus those with technical expertise) but also for assessing credibility of the source reporting the information (e.g. media, scientists, commercial organisations).","title":"Who"},{"location":"ped/chapter4/uncertainty/#what","text":"A particularly valuable contribution from their framework is the categorisation of what the uncertainty pertains to into the following taxonomy: Object of uncertainty Facts (e.g. is the uncertainty about a specific event, such as whether there has been a data leak) Numbers (e.g. is the uncertainty about specific quantities or parameters of a model) Hypotheses (e.g. is the uncertainty about an underlying assumption in a scientific theory) Source of uncertainty Variability in population being sampled Measurement error Limited knowledge Expert disagreement Level of uncertainty Direct (i.e. is the uncertaity about the object itself) Indirect (i.e. is the uncertainty related to the quality of the evidence) Magnitude of uncertainty","title":"What"},{"location":"ped/chapter4/uncertainty/#in-what-form","text":"The format used to communicate the uncertainty is likely to affect the audience in a variety of ways (see 'to what effect'). As the following figure indicates, this form is associated with varying levels of precision. . Decreased precision is not always undesirable. For instance, sometimes it represents a worthwhile trade-off between precision and accessibility of communication to a non-technical or time-stretched audience. However, as the diagram shows, it can sometimes spill over into forms of explicit denial.","title":"In what form"},{"location":"ped/chapter4/uncertainty/#to-whom","text":"In chapter 2 we saw how the relationship between researchers and the public can embody a variety of communicative values, but also prevent obstacles when imbalances of power arise. Similar characteristics are present in the framework from van der Bles et al. (2019)[@vanderbles2019]. characteristics of the audience (e.g. data literacy, relevant demographic variables) relationship of the audience to what is being communicated (e.g. skeptical attitude) relationship of the audience to the people doing the communication (e.g. trustworthy or credible relationship) As an example, van der Bles et al. (2019)[@vanderbles2019] cite a study from Dieckmann et al. (2017)[@dieckmann2017] that found evidence of varying responses to ambiguous information about average global surface temperatures. For audiences, who were more accepting of climate change, they took the information to be evidence of a normal distribution where higher values of surface temperature were likely. Whereas, for audiences who were less accepting of climate change, the same information was taken to reflect a uniform distribution where lower temperatures were more likely.","title":"To whom"},{"location":"ped/chapter4/uncertainty/#to-what-effect","text":"The final stage of the framework considers the effect of not knowing, and identifies the following categories based on the extant empirical literature: Effect on cognition (e.g. are consistent inferences or judgements elicited from verbal statements of uncertainty) Effect on emotion (e.g. do uncertainty ranges elicit higher levels of anxiety in specific audience, rather than point estimates) Effect on trust (e.g. is perceived over-confidence in the communication of uncertainty likely to build confidence or undermine trust) Effect on decision-making (e.g. if uncertainty is communicated, will decision-making be delayed) The above framework helps to systematise our understanding of uncertainty, which can be very valuable when trying to consider how best to communicate with specific audiences or stakeholders. However, it also raises the following question: Question Given the wide range of factors involved, and the scattered nature of the empirical evidence, how should researchers choose the best course of action when communicating uncertainty? There is, again, and unfortunately, no simple answer to this question. Deciding on the right course of action is likely to be a team effort, and one which considers all of the topics we have covered so far (e.g. goals, values, audience, available methods, time and resources). There are, however, some interesting tools that link back to the first section in this chapter, which deserve a mention. === \"Plotting Hypothetical Outcomes\" <div class=\"result\" markdown> ![Animation of a hypothetical outcome plot](../../assets/images/animations/regression.gif){ align=right width=300 } One method is to use packages built into software like Python and R to generate visual animations that explicitly depict alternate or hypothetical outcomes alongside, say, the most likely model of the data generation process. See [this page for more details](uncertainty_example.ipynb). </div> === \"Interactive Heat Maps\" <div class=\"result\" markdown> ![Example climate impact map](../../assets/images/graphics/climate-map.gif){ align=left width=50% } Heat maps are commonly used for communicating probabilistic information. The [Climate Impact Lab](https://impactlab.org/map/#usmeas=change-from-hist&usyear=2020-2039&gmeas=change-from-hist&gyear=2040-2059&usvar=mortality&tab=global&gvar=mortality) take the traditional heat map a step further though by adding interactivity to allow users to alter variables and see the possible impact that emissions, for example, could have across the globe. </div> === \"Uncertainty.io\" <div class=\"result\" markdown> ![From Line by Lee Ufan (1978)](../../assets/images/graphics/from-line.jpeg){ align=right width=300 } Still in need of some creative inspiration? [Uncertainty.io](https://www.uncertainty.io/art/) is a queryable interface to references of more than 400 works of fine art that \"have a unique ability to convey uncertainty using a range of approaches and techniques.\" </div> We can split this into both epistemic and aleatory uncertainty, where the former relates to our uncertain knowledge of the world, and the latter refers to fundamental indeterminacy of the world itself. In this section we will only consider the former (i.e. epistemic uncertainty). \u21a9 An optional activity is to see who is the fastest at saying this phrase five times \ud83d\ude09. \u21a9 See the original article for further details and a review of the empirical evidence. \u21a9","title":"To what effect"},{"location":"ped/chapter5/","text":"Public Trust and Assurance \u00b6 Introduction \u00b6 To conclude, we turn to look at some of the outstanding issues raised in the previous chapter. Specifically, we look at some of the explanatory factors for why members of the public may be skeptical of public engagement or even distrust science and technology. These factors are important to understand, even if there are no easy answers or solutions. We explore why public trust matters in the contect of public engagement, and also what can be done to provide assurance that you and your team have conducted your project in a responsible and trustworthy manner. Chapter Outline \u00b6 Public Trust in Science and Technology Learning Objectives By the end of this chapter you will understand several reasons why members of the public may be distrustful of science and technology. This recognition, alongside the lessons learned over the course, will help you identify means or methods in your own work that could help build trust, rather than undermine it.","title":"Introduction"},{"location":"ped/chapter5/#public-trust-and-assurance","text":"","title":"Public Trust and Assurance"},{"location":"ped/chapter5/#introduction","text":"To conclude, we turn to look at some of the outstanding issues raised in the previous chapter. Specifically, we look at some of the explanatory factors for why members of the public may be skeptical of public engagement or even distrust science and technology. These factors are important to understand, even if there are no easy answers or solutions. We explore why public trust matters in the contect of public engagement, and also what can be done to provide assurance that you and your team have conducted your project in a responsible and trustworthy manner.","title":"Introduction"},{"location":"ped/chapter5/#chapter-outline","text":"Public Trust in Science and Technology Learning Objectives By the end of this chapter you will understand several reasons why members of the public may be distrustful of science and technology. This recognition, alongside the lessons learned over the course, will help you identify means or methods in your own work that could help build trust, rather than undermine it.","title":"Chapter Outline"},{"location":"ped/chapter5/trust/","text":"Public Trust in Science and Technology \u00b6 Going back to the beginning of the course, establishing social trust in science is one of the goals of public engagement. Not only does a lack of trust diminish science's legitimacy in the public eye, it also makes the more practical goals of science much harder to achieve . If laypeople distrust science and are incredulous or suspicious of its claims, it will be quite difficult to put scientific results into wide use.[@grasswick2010] Vaccine hesitancy is a clear example of this. If a relevant portion of the public distrusts the science behind vaccine efficacy and safety, they will be less likely to get vaccinated no matter how readily available the vaccine is. Not only does this leave the unvaccinated with a higher risk of falling ill themselves, it also negatively impacts the overall level of immunity in the population. Another example from the Covid-19 pandemic comes from contact-tracing apps which were designed to track the spread of the virus. These apps enjoyed widespread support from healthcare organisations as they had the potential to develop and strenghten epidemiological research. However, the reception from the general population was mixed. In particular, a relevant portion of the public was concerned with whether their data was being handled responsibly, and this sometimes led to poor adoption rates from the general population, once again highlighting the difficulty of implementing scientific research and processes when people do not trust science. As important as citizen trust in science is, there are many challenges that can make science less trustworthy to individuals and communities. As scientists and researchers, it is important to become aware of them and reflect on how they might be mitigated. 10 challenges to public trust in science \u00b6 Over the next sections we will look at different challenges or barriers to public trust in science and technology. Although we will go through each issue separately, in practice there is a lot of overlap and interrelations between them, and some of them feed into or amplify each other. 1. Understanding of science \u00b6 The first challenge to fostering public trust in science we will look at has to do with science education. In particular, with the general public's poor understanding of what science is and how it works, which can lead them to misinterpret the normal workings of science as its failings. On the first day we looked at the Deficit Model[@lewenstein2003] which assumed scientific literacy amounted to knowledge of scientific findings. As we saw, there are problems with this view since arguably members of the public have no reason to know the detailed workings of generative models in machine learning or complicated equations in theoretical physics. There is however, another way to think about scientific literacy; not in terms of how much scientific knowledge the public has, but instead on whether they grasp the nature of the scientific process. As Douglas (2012) argues, what should be at the core of scientific education is not science as a set of facts about areas of knowledge, but instead a thorough understanding of what science is as an epistemic endeavour .[@douglas2017] The most important thing to understand about the scientific process is that science is jointly critical and inductive in nature (ibid). Science seeks to build an empirical understanding of the world through proposing explanatory theories and then testing them in the best way possible (ibid). Therefore, science must always rely on induction to reach its conclusions. As such, there is no irrevocable proving of facts in science. Instead, one can only falsify (or fail to falsify) its theories. A classical example is that of the black swan. The phrase dates back to the Roman poet Juvenal who used it to describe a rare bird presumed to be non-existent. The phrase was popular in 16th century London to refer to impossible events or inexisting ojects. Because no one had ever seen a black swan (at least among Europeans), it was assumed that black swans did not exist. Such an analysis relies on induction: it goes from particular instances of (a lack) of sightings of black swans to the general conclusion that black swans do not exist. However, in 1697 Dutch explorers became the first Europeans to see a black swan in Western Australia and their non-existence was thus disproven. The black swan story helps illustrate the limits of induction: no matter how many sightings of (only) white swans, one cannot irrevocably prove that black swans do not exist. To do so, one would need to examine all swans in existence, which is not possible. And science always works like this: we can only generalise findings from particular instances. This means a definite and irrevocable proof is unattainable. Given its inductive nature, science must also be continually open to criticism: it must always allow for the testing of its claims under the light of new evidence. The very reason for the advancement of scientific research is the fact that the status quo is continually challenged and tested as opposed to being dogmatic and definitive in its conclusions. However, the argument goes, the problem is that many people do not understand science as an ever-evolving process which produces only provisional results. Instead they perceive science to be a set of facts about the world (the speed of light is 300,000,000 m/s, humans cells have 23 pairs of chromosomes, etc.). This leads to public confusion about how science works, and it can make people interpret the normal ( even crucial ) workings of science as evidence of scientific failure. A good example is expert disagremeent and debate. As we just saw, this is a key component of a healthy scientific community. Only if scientists are always open to criticism and willing to change their minds, can science continually improve its understanding of the world. As Douglas notes, \"[e]xperts changing their minds is also evidence of science functioning properly , not evidence of experts being fickle of weak-minded\".[@douglas2017] However, expert disagreement can be met with frustration from the general public if they are under the impression that science should provide definite answers, and when scientific 'facts' change (as some inevitably will) this may be interpreted as evidence of science's failings, slowly corroding the public's trust in it. 2. Spread of doubt and confusion \u00b6 Another phenomenon which can erode the public's trust in science is documented in Naomi Oreskes and Eric M. Conways's book 'Merchants of Doubt: How a Handful of Scientists Obscured the Truth on Issues from Tobacco Smoke to Global Warming'.[@oreskes2011] The authors' show how a group of scientists in the United States used the inductive nature of science to discredit scientific findings that went against the financial interests of the industries that employed them. Two of the most-well known cases are those of the tobacco and oil industries. The tobacco industry famously denied the link between smoking and lung cancer long after science had provided robust evidence for it. The same can be said of the oil industry; first they sowed doubt on whether climate change was real, and when that became increasingly difficult to deny, they advanced the possibility that perhaps it was not man-made (again, going against what the evidence was consistently showing). In both cases, the scientists (who had ties to these industries) used the very nature of the scientific method as a way to undermine science's findings. As we know, because science is at its core inductive, nothing can ever be definitely and finally proven. There is always the possibility, however remote, that current scientific understanding is wrong. This does not mean of course, that there cannot be overall consensus in science. In the case of climate change for instance, Oreskes (2004, 2007)[@oreskes2004] - [@oreskes2007] documents that in a review of approximately 900 scientific articles on climate change, none of them refuted the idea \"global climate change is ocurring, and human activities are at least part of the reason why.\"[@almassi2012] However, she also notes that a 2006 ABC News poll in the US found that 64% of Americans perceived there to be a lot of disagreeemnt amongst scientists on the reality of global warming (ibid). This is the strategy of the 'merchants of doubt': spreading confusion among the public about the level of scientific consensus on certain topics in an effort to delay public criticism and therefore regulation. If, as we have noted, the public expects science to provide with a set of unchanging facts, this strategy will have a doubly negative effect: not only will it undermine trust in science as scientists are supposed to have the answers, but it will also create the false perception that there is no scientific consensus when there actually seems to be one. Clearly, this can increase distrust in science, especially if the strategy is used to hide harms to the public in an attempt to maintain or increase the profits of those funding the scientists (as is blatantly the case in the examples from the tobacco and oil industries). 3. Vested interests \u00b6 Related to the 'merchants of doubt' strategy is the general problem of scientists having incentives other than those motivated by the disinterested attitude of someone trying to understand how the world works. This is of course, always true: scientists can be driven by multiple motivations, such as ego or the quest for status, among many others. Yet when scientists' incentives are aligned with protecting the interests of the companies who employ them, things become particularly problematic. Cases where this happens are well-document in medicine, where the pharmaceutical industry has been known to use diverse strategies to advance their interests: from aggressive marketing directed at physicians to increase the prescription of their drugs,[@keefe2021] to funding scientists who are supposedly objective and neutral, but who in reality are under the pharmaceutical companies' payroll.[@ritchie2020] An example from the world of tech comes from Facebook (now Meta). In documents known as 'the Facebook files', the Wall Street Journal published various articles documenting the company's clear awareness of the many problems and failings of it products. However, it did surprisingly little to fix them, all the while minisiming the extent of the problem or feigning ignorance to the general public. In particular, leaked company documents show that internal research inside Meta concluded that Instagram was detrimental for teen mental health.[@wells2021] The documents not only detail the evidence Meta had amassed linking poor teen mental health outcomes to Instagram use, but they also documented how this information was relayed to Mark Zuckerberg, the company's CEO. Additionally, it shows that worries about users interacting less with the platform played into the company's decision to fix (or more accurately, to do nothing about) some of Instagram's problems (ibid). When research like this is exposed, it is not hard to see how it could erode people's trust in researchers. Although in this case we are looking at Meta employees payed to carry our research and not at independent researchers, 'the Facebook files' highlight the negative consequences for the public that occur when research is ultimately serving the interests' of private companies rather than the public good. 4. Fraud \u00b6 Even more extreme cases occur when scientists are caught in outright fraud, claiming to have achieved feats which are simply not true, or inventing data and publishing it as real. Many examples come from the world of medicine and technology. Take the case of Paolo Macchiarini, a surgeon who claimed to have solved the rejection problem in trachea transplants (whereby the body rejects the transplanted organ). Not only that, he managed to convince the scientific establishment of his successes and for a while became a rockstar in his field.[@ritchie2020] But the reality was far from a success story: Macchiarini had exaggerated or outright lied about the effectiveness of his treatments, and tragically, many of the patients to which the procedure was performed to died in the following months or years due to complications from the surgery. One of the most worrying parts of stories like this is how long it took to catch Macchiarini in his wrongdoings. It would be one thing if had he been exposed during the peer-review process or shortly after publishing, but this was not what happened. Macchiarini managed to publish his research in the most prestigious journals in his field, operate on multiple patients, and even land a job at the world-renowed Karolinksa Institute before being exposed. Fraud to the extent commited by people like Macchiarini serves to highlight just \"[...]how much science, despite its built-in organised scepticism, comes down to trust\" (ibid).[@ritchie2020] As a general rule, scientists operate with a certain level of trust towards other scientists: trust that researchers are telling the truth, that they actually conducted the experiments they claim they did, trust that the statistical analysis have been reported correctly, etc. Why do scientists lie? Clearly there can be many reasons. As we saw in the last section, scientists (and human beings in general) might have many different motivations such as the search for status or fame in their fields, among others. But the so-called \"publish or perish\" culture prevalent in academia today certainly does not help. Because it might feel like their whole career is at stake, scientists can feel extremely pressured to get something, anything, published (even if that means resorting to the worse means to make that possible). Of course, this is not to excuse fraud at any level, and there are plenty of other reasons scientists will resort to lying, but it does highlight a structural problem in the social functioning of the science today. We will delve deeper into the problem in the next section. Cheating and fraud will never be completely eradicated from any human endeavour, and cases where fraudsters are exposed show us that at least not all lies can stay buried in science. However, there remains the question of how much fraud is not being exposed. As Ritchie notes, the image of objectivity and honesty that the scientific community prides itself in, might be exactly what prevents it from spotting fraudsters like Macchiarini in a timely fashion. If the implicit trust level between scientists is 'too high', this might stop them from scrutinising the data and results in enough depth. Another extreme case is that of Elizabeth Holmes, founder of Theranos, the now-infamous company which claimed to have developed breakthrough health technology to automate and miniaturise blood tests. This technology supposedly enabled hundreds of tests to be done with just a single drop of blood. A few years after raising millions of dollars for Theranos, (which even earned Holmes the title of youngest self-made female billionaire), she was exposed as a complete fake. Her technology did not work at all. The company had given customers innacurate test results which in many cases compromised their health. Once again, the fact that it took years and millions of dollars in investements to realise the extent of her fraud, unsurprisingly may cast doubt on the public perception of science as honest and legitimate. 5. Bias, negligence, and hype \u00b6 Scientists do not have to commit outright fraud to skew their results in ways which can, in the long run, diminish public trust in science. There are other ways in which scientific results can be presented to make them seem more robust than they really are. As we saw, the incentives in the \"publish or perish\" culture are such that scientists are driven to put the possibility of publication over every other consideration, which can lead to biases in their research as well as sloppiness. 1 The drive is not only to publish, but to publish \"attention-grabbing, unequivocal, statistically significant results\" (ibid, emphasis added), and it makes for one of the biggest sources of bias and skewed results in science. 2 Researchers know that certain kinds of studies are very unlikely to get published. Studies which do not find any new and surprising effect, studies which (only) replicate previous findings, or studies with no statistically significant results have very slim chances of getting published in peer-reviewed journals regardless of how rigorous the methodology is. Obviously, this is a big problem. In a perfect world, studies would be published based almost solely on their methodological virtues, paying no attention to how new or surprising the effect found is (or to whether an effect is found at all). If a study is designed properly, its results should be of interest to the scientific community whether the result is positive, negative or null.[@ritchie2020] Instead what we get is scientists not publishing (and sometimes not even writing up) research which did not find any statistically significant results (sometimes refered to as the 'file-drawer effect'). If only studies with statistically significant effects are published while others which show a smaller effect or no effect at all never see the light of day, the whole literature in the area will overstate the effect(s). In fact, this is partly what is the driving the so-called replication crisis in academia, where famous studies which established relevant effects cannot be replicated by other researchers. The problem is widespread and well-documented, and it seems to be the most prevalent in social sciences like psychology as well as biological or medical sciences.[@ioannidis2005] In fact, in a study published in Nature in 2016, over 70% of the 1,500 researchers who filled out a questionnaire, declared that they had tried and failed to replicate other scientists experiments and over half of them failed to replicate their own experiments.[@baker2016] Not only that, but over 60% of respondents claimed that two factors, pressure to publish and selective reporting, were driving the problems in replicability. Does this mean the original scientists were lying? No, not at all. More likely, they just 'got lucky' and their data showed bigger effects than what the 'real' effect is (or what one would expect to get on average if one runs the experiment many times). In any case, if only the 'lucky' studies get published, the overall effect in question will most likely be inflated. But there are also other reasons which contribute to the inflation of research results. An all-too prevalent practice in academia, known as p-hacking is one of them. P-hacking refers to a set of practices where scientists slightly nudge (or hack) their p-values until something reaches the almost holy grail status of statistic significance. They can re-run almost identical versions of their regressions until they get a statistically significant result, drop certain data points, change the statistical tests used, or even take a data set with no particular hypothesis in question and just see what sorts of effects are statistically significant. The pressure to publish makes this kind of behaviour way too common in current scientific practice, to the point where many researchers might not even think they are doing anything wrong. One particularly revealing case is that of Professor Brian Wansink, for a long time one of the most important voices on food psychology (the famous studies which shows that people who use bigger plates tend to eat more food comes from his lab). Professor Wansinsk inadvertently outed himself when he wrote a blog post detailing how he adviced a student whose original hypothesis had \"failed\". Wansink encouraged his student to keep mining the data until something was salvaged. The blog post got other scientists revising Wansink's work, which led to the retraction of many of his studies, and with his resignation from Cornell University where he had been head of the Food and Brand Lab.[@ritchie2020] As bad as Wansinks case is, he is hardly alone. A 2012 poll of over 2,000 psychologists asked whether they had ever engaged in p-hacking[@john2012]: 65% admitted to collecting data on several different outcomes but reporting on only some of them, 40% claimed to have excluded particular datapoints after looking at the results, and 57% said they decided to collect further data after running their analyses.[@ritchie2020] Scientists might just also be negligent when checking their findings which may skew their overall results. As Ritchie rightfully points out, when researchers get 'good' results, that is, results which they think are likely to get published, they will probably feel excited (and perhaps relieved), and then move on. Conversely, if the results are 'bad' (unlikely to get published), they might scrutinise them in detail to make absolutely sure that such a dissapointing result is correct. If this kind of uneven behaviour is consistent, then all flawed null results will get corrected, but most statistically significant which are flawed will not, further inflating the statistically significant effects found.[@ritchie2020] Finally, there is also the over-hype of scientific results. There is no shortage of examples where scientists use words like 'unique', 'robust' and 'unprecedented' to describe their work, and the most prestigious journals pride themselves in publishing studies of \"exceptional importance\" (Proceedings of the National Academy of Sciences) and papers which have \"great potential impact\" in their fields[@ritchie2020]. As we saw, it is highly unlikely that the most methodologically rigorous studies will all find unique and unprecedented findings. However, there is pressure to present them this way since researchers might feel like it's language that appeals to readers and to reviewers and editors of prestigious journals. All of these issues end up heavily skewing data towards mostly positive results which many times are inflated due to a combination of the effects just described. The replication crisis in academia and the (correct) perception that there are relevant problems in the way the incentives are currently set up for scientits, provides us with good reasons to be at least somewhat weary of scientific findings. 6. Lack of control over the message \u00b6 No matter how much care researchers take when communicating and engaging with the public, the truth is, no one is entirely in control of the message they put out. Scientists might be misquoted or misinterpreted in the media by unscrupulous journalists or simply by errors of miscommunication. And scientific findings may be spun in ways which are designed to grab people's attention while not necessarily communicating in the most truthful way. Going back to pandemic examples, since the Covid-19 vaccine has been rolled out, there have been numerous claims that more vaccinated people are dying of Covid than unvaccinated. While technically this may have been true in some cases, (that is, the number of deaths of vaccinated may have been higher than that of the unvaccinated). However, these numbers failed to take into account that as the vaccine uptake increased, the proportion of the unvaccinated grew smaller and smaller. So, although the total number of deaths among the unvaccinated may have been small, if you take into account the total number of unvaccinated people, the proportion of deaths among the unvaccinated was a lot higher than that of the unvaccinated.[@spiegelhalter2021] Examples like this show the importance of science communication: it is not enough to get the numbers 'right' (in the sense that they are not adulterated or fabricated), one needs to be be able to read them properly. Even in cases when researchers are successfully able to convey their message across, it is impossible to control how this message might be then further distorted or changed in social media or otherwise. As we saw in sections 9 and 10, messages which cause strong emotions (such as anger or moral outrage) spread much more quickly through social media than unemotional or nuanced ones. Again, this is in great part due to the workings of the content-filtering algorithms which are programmed to show us content which is most likely to grab our attention and/or get users to share it. Click-bait headlines and misleading quotes are therefore easily propagated through social media and scientists engaging with the media and general public should be well aware of that. One cannot control headlines and quotes, but it is important that researchers are mindful of these issues, as well as be willing to engage with journalists when they feel they are being misquoted. It is important to remember that engagement does not end after giving an interview, it is important to follow up and clarify one's message when needed. 7. Mistreatment and discrimination of marginalised communities \u00b6 Science history is riddled with cases of sexism and racism being passed off as 'objective science', with some particularly gruesome episodes such as the Tuskegee studies in the United States during the 20th century. There is no shortage of examples where science was used to justify discriminatory practices and worldviews. It is unsurprising that the long history of science's racist and sexist practices can make people from historically oppresed communities suspicious or distrustful of science, evidenced for example in African American women's distrust of the birth control pill when it first emerged,[@grasswick2010] or in the higher rates of vaccine hesitancy of marginalised groups both for vaccines in general as well as and response to the Covid-19 vaccine.[@nguyen2022] Given its history, how does trust in science fare when in comes to data science and AI? When these technologies were first used, and as their use became widespread, it was thought that they would eradicate (or at least greatly diminish) biases and discrimination (such as racism or sexism) from scientific practice. The reason for this was that data and AI were broadly perceived as 'neutral and objetive'. It was humans, not algorithms, who were full of biases. We now know this way of thinking is grossly mistaken. If anything, algorithms can amplify the already existing human biases, regardless of whether these biases are conscious or not. Sadly, there are far too many examples in recent years. A 2019 paper published in Science showed how an algorithm used in US healthcare to predict patients' needs was producing racist results.[@obermeyer2019] The bias was introduced because the algorithm used past health costs as a proxy for health needs, which inadvertently favoured White patients. Less money was spent on Black patients with the same level of need as their White counterparts, and the algorithm thus falsely concluded that Black patients are healthier than equally sick White patients. Another famous example is Amazon's hiring algorithm which turned out to be biased against women. In an attempt to automate their hiring practices, Amazon developed an experimental hiring tool which used artificial intelligence to give job candidates scores ranging from one to five stars.[@dastin2022] The algorithm quickly taught itself to discriminate against women candidates, penalising resumes which included the word women, (in 'women's chess club' for instance), as well as downgrading resumes which came from all-women colleges (ibid). Although the algorithm is not used by the company, it was actually taken down precisely because of concerns about its sexism,[@dastin2018] it serves as a powerful example of how AI can perpetuate and amplify historical biases (such as learning from the fact that traditionally Amazon has not hired many women, and extrapolating that to mean that women are not good employees). Because discrimination is often embedded into technology, people from marginalised groups have a rational reason to distrust it, all the more if these technologies are untruthfully portayed as just the opposite: neutral and imparcial. Therefore, it is crucial to be aware of the potential biases of algorithms, reminding ourselves that no technology is ever truly neutral. 8. Misuse of data \u00b6 Often scientists and researchers can abuse their power and the trust members of the public have given them. The world of data and AI is full of opportunities to do so, especially if we take into account the huge asymmetry of information between researchers and the public in terms of how data is collected, used, and how the algorithms work. A famous example is the so-called Facebook emotional contagion study.[@kramer2014] A group of researchers at Facebook and Cornell University studied how emotional contagion spread across the social network. To do this, they manipulated the News Feed of Facebook users, either to reduce positive messages (thus amplifying negative ones), reduce negative messages (thus amplifying positive ones), or reduce messages at random (control condition). The level of emotional contagion was measured by the proportion of positive and negative messages the manipulated users themselves then posted. The study found that when positive expressions of emotion were reduced, people produced fewer positive posts and more negative posts; when negative expressions were reduced, the opposite pattern occurred. The article was published in the prestigious Proceedings of the National Academy of Sciences (PNAS). However, it was met with outrage from the public.[@meyer2014] - [@chambers2014] The company claimed it had not violated Facebook's terms and conditions (they explicitly invoked them to argue that users' acceptance of these T&C constituted informed consent for the research). However, the public did not agree. None of the users knew they were part of this research, much less what the research amounted to in terms of manipulation of their News Feed. The realisation of how easily Facebook could manipulate its users (for research or other purposes), angered, scared, and increased people's distrust in technology. It is interesting to note that one of the company's defenses actually had to do with how much and how often they already tweak and manipulate the algorithm.[@meyer2014] As we will see in the next section, this is no cause for celebration. This episode also highlights how unaware researchers might be of the public's concerns. These researchers decided to publish their research in one of the most prestigious journals in science where it would receive a lot of attention. Presumably, it did not occur to them that other researchers and the general public would be outraged at their privacy policies and would find the study was unethical. This reminds us of the importance of public deliberation, and having conversations which can allow trust to flourish between scientists and the public. 9. Online misinformation and disinformation \u00b6 We now turn to misinformation and disinformation. Misinformation refers to information which is incorrect or inaccurate whereas disinformation has been \"[...]used to denote a specific type of misinformation that is intentionally false\".[@scheufele2019] - 3 Unsurprisingly, they both can cause important damage to the relationship of trust between the public and the world of science and research. When exposed to misinformation people might become confused about what the scientists are saying and start to distrust scientists' motivations, which can then lead to overall distorted narratives about the state of scientific evidence for any given topic. Misinformation is certainly not a new phenomena, but it seems to have become increasingly more prevalent over the last years. It is now well-documented that fake news is more likely to be retweeted and spread online than real news,[@vosoughi2018] and the Internet and social media can sometimes seem to be infested by it. The link with algorithms and technology is direct (although possibly unintended as such). As we previously stated, no piece of technology is neutral. It is designed by humans with particular aims in mind and it can perpetuate and amplify human biases. In the case of social media, the algorithm which determines the users' News Feed is maximising for one thing: time spent on, and interacting with, the platform. Because of the business model of social media companies, they are competing for users' attention (sometimes referred to as 'the attention-economy').[@zuboff2019] - [@center] Therefore, content-filtering algorithms are designed to show us content which is most likely to grab our attention and thus keep us on the platform. Sadly, it seems that emotional content which angers or outrages us is an easy way to do so. Studies have shown that emotional, and particularly angry messages spread much faster in social media (one of them is the infamous Facebook emotional contagion study from the last section).[@kramer2014] - [@chen2017] - [@crockett2017] - [@brady2017] If the message being spread is fake or otherwise distorted, it is easier to make it as outrageous as required. Even though the algorithm was designed with the aim of maximising attention and engagement in mind, we can see how this can inadvertently also end up promoting fake news and misinformation. Widespread misinformation is detrimental to public trust in science for at least two reasons. First of all, if misinformation is rampant in social media, misinformation about science and scientists will not be an exception. The amount of misinformation and outright fake news about the Covid-19 pandemic and the response to it has sadly given us (too) many examples of this over the last two years. Additionally, an environment rife with misinformation promotes an overall worldview that maybe there is no 'real' information (sometimes known as post-truth), and that instead we are just confronted with people using supposed information to push their own interests forward in the public stage. As we will see in the next section, when this is combined with siloed communities which distrust anyone outside it, it can lead to very problematic epistemic outcomes. 10. Filter bubbles and echo chambers \u00b6 This leads us to epistemic filter bubbles and, even more epistemically pernicious, echo chambers. Again, these are not problems exclusive to social media platforms and the algorithms that fuel them, but the latter certainly play a role in making them more ubiquituous as well as amplifying their contents. C.Thi Nguyen (2020) proposes a useful disintion between epistemic bubbles and echo chambers. He defines an epistemic bubble as a \"social epistemic structure which has inadequate coverage through a process of exclusion by omission\".[@nguyen2020] That is, it is a filter bubble which omits certain views and positions. The key here is that this inadequate coverage occurs through omission . There is no need for ill-intent in the creation of epistemic bubbles, they can arise \"[..]through the ordinary processes of social selection and community formation\" (ibid). In fact, epistemic bubbles can be quite common in everyday life. We might find ourselves in one if we only buy newspapers of a certain political leaning, or only speak to friends who hold similar worldviews to us. Social media News Feeds can in many occasions become epistemic bubbles as people mostly interact with others who are similar to them. The good news about epistemic filter bubbles is that they can be burst through sufficient exposure to information from outside of the bubble.[@nguyen2020] In this case, someone's warped view of the world is mainly due to lack of exposure to a variety of views on certain issues. Therefore, the solution is relatively easy: in order to burst the bubble, people should be exposed to many varied worldviews and opinions. An echo chamber however, is another story. Unlike an epistemic bubble, a lack of diversity is not the main reason people become polarised and entrenched in their views. Instead, Nguyen defines an echo chamber as a \"social epistemic structure in which other relevant voices [those outside of it] have been discredited\" (ibid), which implies at least a certain level of intentionality in the discrediting of those not part of the echo chamber. In fact, the crucial element of an echo chamber as an epistemic community is that there is a \"[...]significant disparity in trust between members and non members\".[@nguyen2020] Members of the echo chambers are given almost infinite credence when they voice their opinions and views, while beliefs of those outside it are completely discredited. It is a process similar to cult indoctrination, and it is very easy to see why it is so pernicious. By preemptively dismissing the opinions of those who do not share the beliefs of those inside the echo chamber, it is easy to epistemically insulate oneself to the point that even evidence which contradicts your views and should give you reason to reevaluate, instead ends up confirming your original views even further. An echo chamber might be the most dangerous challenge to trust in science: once someone is inside one where scientists are considered outsiders and discredited, it is almost impossible to get them to reconsider their beliefs, especially when the suggestion to do so comes from those outside the echo chamber. Once again it is important to remember that although echo chambers are not completely online phenomena, the way algorithms are employed in social media do seem to aid in their formation. For a detailed explanation of these phenomena, see Stuart Ritchie's book, Science Fictions (2020). \u21a9 Even the crucial concept of 'statistically significant' has led to a lot of confusion, as the word significant seems to allude to an effect which is big and important in some way, where it actually means that the effect found is sufficiently different from what we would expect to see if there was no effect (Ritchie, 2020, 133).[@ritchie2020] The related concept of p-value has also lent itself for gross misunderstanding. In fact, a study found that 89% of Introduction to Psychology textbooks got the definition wrong.[@ritchie2020] - [@cassidy2019] \u21a9 In order to be concise, I will use the term misinformation to refer to both misinformation and disinformation unless explicitly stated. \u21a9","title":"Public Trust in Science and Technology"},{"location":"ped/chapter5/trust/#public-trust-in-science-and-technology","text":"Going back to the beginning of the course, establishing social trust in science is one of the goals of public engagement. Not only does a lack of trust diminish science's legitimacy in the public eye, it also makes the more practical goals of science much harder to achieve . If laypeople distrust science and are incredulous or suspicious of its claims, it will be quite difficult to put scientific results into wide use.[@grasswick2010] Vaccine hesitancy is a clear example of this. If a relevant portion of the public distrusts the science behind vaccine efficacy and safety, they will be less likely to get vaccinated no matter how readily available the vaccine is. Not only does this leave the unvaccinated with a higher risk of falling ill themselves, it also negatively impacts the overall level of immunity in the population. Another example from the Covid-19 pandemic comes from contact-tracing apps which were designed to track the spread of the virus. These apps enjoyed widespread support from healthcare organisations as they had the potential to develop and strenghten epidemiological research. However, the reception from the general population was mixed. In particular, a relevant portion of the public was concerned with whether their data was being handled responsibly, and this sometimes led to poor adoption rates from the general population, once again highlighting the difficulty of implementing scientific research and processes when people do not trust science. As important as citizen trust in science is, there are many challenges that can make science less trustworthy to individuals and communities. As scientists and researchers, it is important to become aware of them and reflect on how they might be mitigated.","title":"Public Trust in Science and Technology"},{"location":"ped/chapter5/trust/#10-challenges-to-public-trust-in-science","text":"Over the next sections we will look at different challenges or barriers to public trust in science and technology. Although we will go through each issue separately, in practice there is a lot of overlap and interrelations between them, and some of them feed into or amplify each other.","title":"10 challenges to public trust in science"},{"location":"ped/chapter5/trust/#1-understanding-of-science","text":"The first challenge to fostering public trust in science we will look at has to do with science education. In particular, with the general public's poor understanding of what science is and how it works, which can lead them to misinterpret the normal workings of science as its failings. On the first day we looked at the Deficit Model[@lewenstein2003] which assumed scientific literacy amounted to knowledge of scientific findings. As we saw, there are problems with this view since arguably members of the public have no reason to know the detailed workings of generative models in machine learning or complicated equations in theoretical physics. There is however, another way to think about scientific literacy; not in terms of how much scientific knowledge the public has, but instead on whether they grasp the nature of the scientific process. As Douglas (2012) argues, what should be at the core of scientific education is not science as a set of facts about areas of knowledge, but instead a thorough understanding of what science is as an epistemic endeavour .[@douglas2017] The most important thing to understand about the scientific process is that science is jointly critical and inductive in nature (ibid). Science seeks to build an empirical understanding of the world through proposing explanatory theories and then testing them in the best way possible (ibid). Therefore, science must always rely on induction to reach its conclusions. As such, there is no irrevocable proving of facts in science. Instead, one can only falsify (or fail to falsify) its theories. A classical example is that of the black swan. The phrase dates back to the Roman poet Juvenal who used it to describe a rare bird presumed to be non-existent. The phrase was popular in 16th century London to refer to impossible events or inexisting ojects. Because no one had ever seen a black swan (at least among Europeans), it was assumed that black swans did not exist. Such an analysis relies on induction: it goes from particular instances of (a lack) of sightings of black swans to the general conclusion that black swans do not exist. However, in 1697 Dutch explorers became the first Europeans to see a black swan in Western Australia and their non-existence was thus disproven. The black swan story helps illustrate the limits of induction: no matter how many sightings of (only) white swans, one cannot irrevocably prove that black swans do not exist. To do so, one would need to examine all swans in existence, which is not possible. And science always works like this: we can only generalise findings from particular instances. This means a definite and irrevocable proof is unattainable. Given its inductive nature, science must also be continually open to criticism: it must always allow for the testing of its claims under the light of new evidence. The very reason for the advancement of scientific research is the fact that the status quo is continually challenged and tested as opposed to being dogmatic and definitive in its conclusions. However, the argument goes, the problem is that many people do not understand science as an ever-evolving process which produces only provisional results. Instead they perceive science to be a set of facts about the world (the speed of light is 300,000,000 m/s, humans cells have 23 pairs of chromosomes, etc.). This leads to public confusion about how science works, and it can make people interpret the normal ( even crucial ) workings of science as evidence of scientific failure. A good example is expert disagremeent and debate. As we just saw, this is a key component of a healthy scientific community. Only if scientists are always open to criticism and willing to change their minds, can science continually improve its understanding of the world. As Douglas notes, \"[e]xperts changing their minds is also evidence of science functioning properly , not evidence of experts being fickle of weak-minded\".[@douglas2017] However, expert disagreement can be met with frustration from the general public if they are under the impression that science should provide definite answers, and when scientific 'facts' change (as some inevitably will) this may be interpreted as evidence of science's failings, slowly corroding the public's trust in it.","title":"1. Understanding of science"},{"location":"ped/chapter5/trust/#2-spread-of-doubt-and-confusion","text":"Another phenomenon which can erode the public's trust in science is documented in Naomi Oreskes and Eric M. Conways's book 'Merchants of Doubt: How a Handful of Scientists Obscured the Truth on Issues from Tobacco Smoke to Global Warming'.[@oreskes2011] The authors' show how a group of scientists in the United States used the inductive nature of science to discredit scientific findings that went against the financial interests of the industries that employed them. Two of the most-well known cases are those of the tobacco and oil industries. The tobacco industry famously denied the link between smoking and lung cancer long after science had provided robust evidence for it. The same can be said of the oil industry; first they sowed doubt on whether climate change was real, and when that became increasingly difficult to deny, they advanced the possibility that perhaps it was not man-made (again, going against what the evidence was consistently showing). In both cases, the scientists (who had ties to these industries) used the very nature of the scientific method as a way to undermine science's findings. As we know, because science is at its core inductive, nothing can ever be definitely and finally proven. There is always the possibility, however remote, that current scientific understanding is wrong. This does not mean of course, that there cannot be overall consensus in science. In the case of climate change for instance, Oreskes (2004, 2007)[@oreskes2004] - [@oreskes2007] documents that in a review of approximately 900 scientific articles on climate change, none of them refuted the idea \"global climate change is ocurring, and human activities are at least part of the reason why.\"[@almassi2012] However, she also notes that a 2006 ABC News poll in the US found that 64% of Americans perceived there to be a lot of disagreeemnt amongst scientists on the reality of global warming (ibid). This is the strategy of the 'merchants of doubt': spreading confusion among the public about the level of scientific consensus on certain topics in an effort to delay public criticism and therefore regulation. If, as we have noted, the public expects science to provide with a set of unchanging facts, this strategy will have a doubly negative effect: not only will it undermine trust in science as scientists are supposed to have the answers, but it will also create the false perception that there is no scientific consensus when there actually seems to be one. Clearly, this can increase distrust in science, especially if the strategy is used to hide harms to the public in an attempt to maintain or increase the profits of those funding the scientists (as is blatantly the case in the examples from the tobacco and oil industries).","title":"2. Spread of doubt and confusion"},{"location":"ped/chapter5/trust/#3-vested-interests","text":"Related to the 'merchants of doubt' strategy is the general problem of scientists having incentives other than those motivated by the disinterested attitude of someone trying to understand how the world works. This is of course, always true: scientists can be driven by multiple motivations, such as ego or the quest for status, among many others. Yet when scientists' incentives are aligned with protecting the interests of the companies who employ them, things become particularly problematic. Cases where this happens are well-document in medicine, where the pharmaceutical industry has been known to use diverse strategies to advance their interests: from aggressive marketing directed at physicians to increase the prescription of their drugs,[@keefe2021] to funding scientists who are supposedly objective and neutral, but who in reality are under the pharmaceutical companies' payroll.[@ritchie2020] An example from the world of tech comes from Facebook (now Meta). In documents known as 'the Facebook files', the Wall Street Journal published various articles documenting the company's clear awareness of the many problems and failings of it products. However, it did surprisingly little to fix them, all the while minisiming the extent of the problem or feigning ignorance to the general public. In particular, leaked company documents show that internal research inside Meta concluded that Instagram was detrimental for teen mental health.[@wells2021] The documents not only detail the evidence Meta had amassed linking poor teen mental health outcomes to Instagram use, but they also documented how this information was relayed to Mark Zuckerberg, the company's CEO. Additionally, it shows that worries about users interacting less with the platform played into the company's decision to fix (or more accurately, to do nothing about) some of Instagram's problems (ibid). When research like this is exposed, it is not hard to see how it could erode people's trust in researchers. Although in this case we are looking at Meta employees payed to carry our research and not at independent researchers, 'the Facebook files' highlight the negative consequences for the public that occur when research is ultimately serving the interests' of private companies rather than the public good.","title":"3. Vested interests"},{"location":"ped/chapter5/trust/#4-fraud","text":"Even more extreme cases occur when scientists are caught in outright fraud, claiming to have achieved feats which are simply not true, or inventing data and publishing it as real. Many examples come from the world of medicine and technology. Take the case of Paolo Macchiarini, a surgeon who claimed to have solved the rejection problem in trachea transplants (whereby the body rejects the transplanted organ). Not only that, he managed to convince the scientific establishment of his successes and for a while became a rockstar in his field.[@ritchie2020] But the reality was far from a success story: Macchiarini had exaggerated or outright lied about the effectiveness of his treatments, and tragically, many of the patients to which the procedure was performed to died in the following months or years due to complications from the surgery. One of the most worrying parts of stories like this is how long it took to catch Macchiarini in his wrongdoings. It would be one thing if had he been exposed during the peer-review process or shortly after publishing, but this was not what happened. Macchiarini managed to publish his research in the most prestigious journals in his field, operate on multiple patients, and even land a job at the world-renowed Karolinksa Institute before being exposed. Fraud to the extent commited by people like Macchiarini serves to highlight just \"[...]how much science, despite its built-in organised scepticism, comes down to trust\" (ibid).[@ritchie2020] As a general rule, scientists operate with a certain level of trust towards other scientists: trust that researchers are telling the truth, that they actually conducted the experiments they claim they did, trust that the statistical analysis have been reported correctly, etc. Why do scientists lie? Clearly there can be many reasons. As we saw in the last section, scientists (and human beings in general) might have many different motivations such as the search for status or fame in their fields, among others. But the so-called \"publish or perish\" culture prevalent in academia today certainly does not help. Because it might feel like their whole career is at stake, scientists can feel extremely pressured to get something, anything, published (even if that means resorting to the worse means to make that possible). Of course, this is not to excuse fraud at any level, and there are plenty of other reasons scientists will resort to lying, but it does highlight a structural problem in the social functioning of the science today. We will delve deeper into the problem in the next section. Cheating and fraud will never be completely eradicated from any human endeavour, and cases where fraudsters are exposed show us that at least not all lies can stay buried in science. However, there remains the question of how much fraud is not being exposed. As Ritchie notes, the image of objectivity and honesty that the scientific community prides itself in, might be exactly what prevents it from spotting fraudsters like Macchiarini in a timely fashion. If the implicit trust level between scientists is 'too high', this might stop them from scrutinising the data and results in enough depth. Another extreme case is that of Elizabeth Holmes, founder of Theranos, the now-infamous company which claimed to have developed breakthrough health technology to automate and miniaturise blood tests. This technology supposedly enabled hundreds of tests to be done with just a single drop of blood. A few years after raising millions of dollars for Theranos, (which even earned Holmes the title of youngest self-made female billionaire), she was exposed as a complete fake. Her technology did not work at all. The company had given customers innacurate test results which in many cases compromised their health. Once again, the fact that it took years and millions of dollars in investements to realise the extent of her fraud, unsurprisingly may cast doubt on the public perception of science as honest and legitimate.","title":"4. Fraud"},{"location":"ped/chapter5/trust/#5-bias-negligence-and-hype","text":"Scientists do not have to commit outright fraud to skew their results in ways which can, in the long run, diminish public trust in science. There are other ways in which scientific results can be presented to make them seem more robust than they really are. As we saw, the incentives in the \"publish or perish\" culture are such that scientists are driven to put the possibility of publication over every other consideration, which can lead to biases in their research as well as sloppiness. 1 The drive is not only to publish, but to publish \"attention-grabbing, unequivocal, statistically significant results\" (ibid, emphasis added), and it makes for one of the biggest sources of bias and skewed results in science. 2 Researchers know that certain kinds of studies are very unlikely to get published. Studies which do not find any new and surprising effect, studies which (only) replicate previous findings, or studies with no statistically significant results have very slim chances of getting published in peer-reviewed journals regardless of how rigorous the methodology is. Obviously, this is a big problem. In a perfect world, studies would be published based almost solely on their methodological virtues, paying no attention to how new or surprising the effect found is (or to whether an effect is found at all). If a study is designed properly, its results should be of interest to the scientific community whether the result is positive, negative or null.[@ritchie2020] Instead what we get is scientists not publishing (and sometimes not even writing up) research which did not find any statistically significant results (sometimes refered to as the 'file-drawer effect'). If only studies with statistically significant effects are published while others which show a smaller effect or no effect at all never see the light of day, the whole literature in the area will overstate the effect(s). In fact, this is partly what is the driving the so-called replication crisis in academia, where famous studies which established relevant effects cannot be replicated by other researchers. The problem is widespread and well-documented, and it seems to be the most prevalent in social sciences like psychology as well as biological or medical sciences.[@ioannidis2005] In fact, in a study published in Nature in 2016, over 70% of the 1,500 researchers who filled out a questionnaire, declared that they had tried and failed to replicate other scientists experiments and over half of them failed to replicate their own experiments.[@baker2016] Not only that, but over 60% of respondents claimed that two factors, pressure to publish and selective reporting, were driving the problems in replicability. Does this mean the original scientists were lying? No, not at all. More likely, they just 'got lucky' and their data showed bigger effects than what the 'real' effect is (or what one would expect to get on average if one runs the experiment many times). In any case, if only the 'lucky' studies get published, the overall effect in question will most likely be inflated. But there are also other reasons which contribute to the inflation of research results. An all-too prevalent practice in academia, known as p-hacking is one of them. P-hacking refers to a set of practices where scientists slightly nudge (or hack) their p-values until something reaches the almost holy grail status of statistic significance. They can re-run almost identical versions of their regressions until they get a statistically significant result, drop certain data points, change the statistical tests used, or even take a data set with no particular hypothesis in question and just see what sorts of effects are statistically significant. The pressure to publish makes this kind of behaviour way too common in current scientific practice, to the point where many researchers might not even think they are doing anything wrong. One particularly revealing case is that of Professor Brian Wansink, for a long time one of the most important voices on food psychology (the famous studies which shows that people who use bigger plates tend to eat more food comes from his lab). Professor Wansinsk inadvertently outed himself when he wrote a blog post detailing how he adviced a student whose original hypothesis had \"failed\". Wansink encouraged his student to keep mining the data until something was salvaged. The blog post got other scientists revising Wansink's work, which led to the retraction of many of his studies, and with his resignation from Cornell University where he had been head of the Food and Brand Lab.[@ritchie2020] As bad as Wansinks case is, he is hardly alone. A 2012 poll of over 2,000 psychologists asked whether they had ever engaged in p-hacking[@john2012]: 65% admitted to collecting data on several different outcomes but reporting on only some of them, 40% claimed to have excluded particular datapoints after looking at the results, and 57% said they decided to collect further data after running their analyses.[@ritchie2020] Scientists might just also be negligent when checking their findings which may skew their overall results. As Ritchie rightfully points out, when researchers get 'good' results, that is, results which they think are likely to get published, they will probably feel excited (and perhaps relieved), and then move on. Conversely, if the results are 'bad' (unlikely to get published), they might scrutinise them in detail to make absolutely sure that such a dissapointing result is correct. If this kind of uneven behaviour is consistent, then all flawed null results will get corrected, but most statistically significant which are flawed will not, further inflating the statistically significant effects found.[@ritchie2020] Finally, there is also the over-hype of scientific results. There is no shortage of examples where scientists use words like 'unique', 'robust' and 'unprecedented' to describe their work, and the most prestigious journals pride themselves in publishing studies of \"exceptional importance\" (Proceedings of the National Academy of Sciences) and papers which have \"great potential impact\" in their fields[@ritchie2020]. As we saw, it is highly unlikely that the most methodologically rigorous studies will all find unique and unprecedented findings. However, there is pressure to present them this way since researchers might feel like it's language that appeals to readers and to reviewers and editors of prestigious journals. All of these issues end up heavily skewing data towards mostly positive results which many times are inflated due to a combination of the effects just described. The replication crisis in academia and the (correct) perception that there are relevant problems in the way the incentives are currently set up for scientits, provides us with good reasons to be at least somewhat weary of scientific findings.","title":"5. Bias, negligence, and hype"},{"location":"ped/chapter5/trust/#6-lack-of-control-over-the-message","text":"No matter how much care researchers take when communicating and engaging with the public, the truth is, no one is entirely in control of the message they put out. Scientists might be misquoted or misinterpreted in the media by unscrupulous journalists or simply by errors of miscommunication. And scientific findings may be spun in ways which are designed to grab people's attention while not necessarily communicating in the most truthful way. Going back to pandemic examples, since the Covid-19 vaccine has been rolled out, there have been numerous claims that more vaccinated people are dying of Covid than unvaccinated. While technically this may have been true in some cases, (that is, the number of deaths of vaccinated may have been higher than that of the unvaccinated). However, these numbers failed to take into account that as the vaccine uptake increased, the proportion of the unvaccinated grew smaller and smaller. So, although the total number of deaths among the unvaccinated may have been small, if you take into account the total number of unvaccinated people, the proportion of deaths among the unvaccinated was a lot higher than that of the unvaccinated.[@spiegelhalter2021] Examples like this show the importance of science communication: it is not enough to get the numbers 'right' (in the sense that they are not adulterated or fabricated), one needs to be be able to read them properly. Even in cases when researchers are successfully able to convey their message across, it is impossible to control how this message might be then further distorted or changed in social media or otherwise. As we saw in sections 9 and 10, messages which cause strong emotions (such as anger or moral outrage) spread much more quickly through social media than unemotional or nuanced ones. Again, this is in great part due to the workings of the content-filtering algorithms which are programmed to show us content which is most likely to grab our attention and/or get users to share it. Click-bait headlines and misleading quotes are therefore easily propagated through social media and scientists engaging with the media and general public should be well aware of that. One cannot control headlines and quotes, but it is important that researchers are mindful of these issues, as well as be willing to engage with journalists when they feel they are being misquoted. It is important to remember that engagement does not end after giving an interview, it is important to follow up and clarify one's message when needed.","title":"6. Lack of control over the message"},{"location":"ped/chapter5/trust/#7-mistreatment-and-discrimination-of-marginalised-communities","text":"Science history is riddled with cases of sexism and racism being passed off as 'objective science', with some particularly gruesome episodes such as the Tuskegee studies in the United States during the 20th century. There is no shortage of examples where science was used to justify discriminatory practices and worldviews. It is unsurprising that the long history of science's racist and sexist practices can make people from historically oppresed communities suspicious or distrustful of science, evidenced for example in African American women's distrust of the birth control pill when it first emerged,[@grasswick2010] or in the higher rates of vaccine hesitancy of marginalised groups both for vaccines in general as well as and response to the Covid-19 vaccine.[@nguyen2022] Given its history, how does trust in science fare when in comes to data science and AI? When these technologies were first used, and as their use became widespread, it was thought that they would eradicate (or at least greatly diminish) biases and discrimination (such as racism or sexism) from scientific practice. The reason for this was that data and AI were broadly perceived as 'neutral and objetive'. It was humans, not algorithms, who were full of biases. We now know this way of thinking is grossly mistaken. If anything, algorithms can amplify the already existing human biases, regardless of whether these biases are conscious or not. Sadly, there are far too many examples in recent years. A 2019 paper published in Science showed how an algorithm used in US healthcare to predict patients' needs was producing racist results.[@obermeyer2019] The bias was introduced because the algorithm used past health costs as a proxy for health needs, which inadvertently favoured White patients. Less money was spent on Black patients with the same level of need as their White counterparts, and the algorithm thus falsely concluded that Black patients are healthier than equally sick White patients. Another famous example is Amazon's hiring algorithm which turned out to be biased against women. In an attempt to automate their hiring practices, Amazon developed an experimental hiring tool which used artificial intelligence to give job candidates scores ranging from one to five stars.[@dastin2022] The algorithm quickly taught itself to discriminate against women candidates, penalising resumes which included the word women, (in 'women's chess club' for instance), as well as downgrading resumes which came from all-women colleges (ibid). Although the algorithm is not used by the company, it was actually taken down precisely because of concerns about its sexism,[@dastin2018] it serves as a powerful example of how AI can perpetuate and amplify historical biases (such as learning from the fact that traditionally Amazon has not hired many women, and extrapolating that to mean that women are not good employees). Because discrimination is often embedded into technology, people from marginalised groups have a rational reason to distrust it, all the more if these technologies are untruthfully portayed as just the opposite: neutral and imparcial. Therefore, it is crucial to be aware of the potential biases of algorithms, reminding ourselves that no technology is ever truly neutral.","title":"7. Mistreatment and discrimination of marginalised communities"},{"location":"ped/chapter5/trust/#8-misuse-of-data","text":"Often scientists and researchers can abuse their power and the trust members of the public have given them. The world of data and AI is full of opportunities to do so, especially if we take into account the huge asymmetry of information between researchers and the public in terms of how data is collected, used, and how the algorithms work. A famous example is the so-called Facebook emotional contagion study.[@kramer2014] A group of researchers at Facebook and Cornell University studied how emotional contagion spread across the social network. To do this, they manipulated the News Feed of Facebook users, either to reduce positive messages (thus amplifying negative ones), reduce negative messages (thus amplifying positive ones), or reduce messages at random (control condition). The level of emotional contagion was measured by the proportion of positive and negative messages the manipulated users themselves then posted. The study found that when positive expressions of emotion were reduced, people produced fewer positive posts and more negative posts; when negative expressions were reduced, the opposite pattern occurred. The article was published in the prestigious Proceedings of the National Academy of Sciences (PNAS). However, it was met with outrage from the public.[@meyer2014] - [@chambers2014] The company claimed it had not violated Facebook's terms and conditions (they explicitly invoked them to argue that users' acceptance of these T&C constituted informed consent for the research). However, the public did not agree. None of the users knew they were part of this research, much less what the research amounted to in terms of manipulation of their News Feed. The realisation of how easily Facebook could manipulate its users (for research or other purposes), angered, scared, and increased people's distrust in technology. It is interesting to note that one of the company's defenses actually had to do with how much and how often they already tweak and manipulate the algorithm.[@meyer2014] As we will see in the next section, this is no cause for celebration. This episode also highlights how unaware researchers might be of the public's concerns. These researchers decided to publish their research in one of the most prestigious journals in science where it would receive a lot of attention. Presumably, it did not occur to them that other researchers and the general public would be outraged at their privacy policies and would find the study was unethical. This reminds us of the importance of public deliberation, and having conversations which can allow trust to flourish between scientists and the public.","title":"8. Misuse of data"},{"location":"ped/chapter5/trust/#9-online-misinformation-and-disinformation","text":"We now turn to misinformation and disinformation. Misinformation refers to information which is incorrect or inaccurate whereas disinformation has been \"[...]used to denote a specific type of misinformation that is intentionally false\".[@scheufele2019] - 3 Unsurprisingly, they both can cause important damage to the relationship of trust between the public and the world of science and research. When exposed to misinformation people might become confused about what the scientists are saying and start to distrust scientists' motivations, which can then lead to overall distorted narratives about the state of scientific evidence for any given topic. Misinformation is certainly not a new phenomena, but it seems to have become increasingly more prevalent over the last years. It is now well-documented that fake news is more likely to be retweeted and spread online than real news,[@vosoughi2018] and the Internet and social media can sometimes seem to be infested by it. The link with algorithms and technology is direct (although possibly unintended as such). As we previously stated, no piece of technology is neutral. It is designed by humans with particular aims in mind and it can perpetuate and amplify human biases. In the case of social media, the algorithm which determines the users' News Feed is maximising for one thing: time spent on, and interacting with, the platform. Because of the business model of social media companies, they are competing for users' attention (sometimes referred to as 'the attention-economy').[@zuboff2019] - [@center] Therefore, content-filtering algorithms are designed to show us content which is most likely to grab our attention and thus keep us on the platform. Sadly, it seems that emotional content which angers or outrages us is an easy way to do so. Studies have shown that emotional, and particularly angry messages spread much faster in social media (one of them is the infamous Facebook emotional contagion study from the last section).[@kramer2014] - [@chen2017] - [@crockett2017] - [@brady2017] If the message being spread is fake or otherwise distorted, it is easier to make it as outrageous as required. Even though the algorithm was designed with the aim of maximising attention and engagement in mind, we can see how this can inadvertently also end up promoting fake news and misinformation. Widespread misinformation is detrimental to public trust in science for at least two reasons. First of all, if misinformation is rampant in social media, misinformation about science and scientists will not be an exception. The amount of misinformation and outright fake news about the Covid-19 pandemic and the response to it has sadly given us (too) many examples of this over the last two years. Additionally, an environment rife with misinformation promotes an overall worldview that maybe there is no 'real' information (sometimes known as post-truth), and that instead we are just confronted with people using supposed information to push their own interests forward in the public stage. As we will see in the next section, when this is combined with siloed communities which distrust anyone outside it, it can lead to very problematic epistemic outcomes.","title":"9. Online misinformation and disinformation"},{"location":"ped/chapter5/trust/#10-filter-bubbles-and-echo-chambers","text":"This leads us to epistemic filter bubbles and, even more epistemically pernicious, echo chambers. Again, these are not problems exclusive to social media platforms and the algorithms that fuel them, but the latter certainly play a role in making them more ubiquituous as well as amplifying their contents. C.Thi Nguyen (2020) proposes a useful disintion between epistemic bubbles and echo chambers. He defines an epistemic bubble as a \"social epistemic structure which has inadequate coverage through a process of exclusion by omission\".[@nguyen2020] That is, it is a filter bubble which omits certain views and positions. The key here is that this inadequate coverage occurs through omission . There is no need for ill-intent in the creation of epistemic bubbles, they can arise \"[..]through the ordinary processes of social selection and community formation\" (ibid). In fact, epistemic bubbles can be quite common in everyday life. We might find ourselves in one if we only buy newspapers of a certain political leaning, or only speak to friends who hold similar worldviews to us. Social media News Feeds can in many occasions become epistemic bubbles as people mostly interact with others who are similar to them. The good news about epistemic filter bubbles is that they can be burst through sufficient exposure to information from outside of the bubble.[@nguyen2020] In this case, someone's warped view of the world is mainly due to lack of exposure to a variety of views on certain issues. Therefore, the solution is relatively easy: in order to burst the bubble, people should be exposed to many varied worldviews and opinions. An echo chamber however, is another story. Unlike an epistemic bubble, a lack of diversity is not the main reason people become polarised and entrenched in their views. Instead, Nguyen defines an echo chamber as a \"social epistemic structure in which other relevant voices [those outside of it] have been discredited\" (ibid), which implies at least a certain level of intentionality in the discrediting of those not part of the echo chamber. In fact, the crucial element of an echo chamber as an epistemic community is that there is a \"[...]significant disparity in trust between members and non members\".[@nguyen2020] Members of the echo chambers are given almost infinite credence when they voice their opinions and views, while beliefs of those outside it are completely discredited. It is a process similar to cult indoctrination, and it is very easy to see why it is so pernicious. By preemptively dismissing the opinions of those who do not share the beliefs of those inside the echo chamber, it is easy to epistemically insulate oneself to the point that even evidence which contradicts your views and should give you reason to reevaluate, instead ends up confirming your original views even further. An echo chamber might be the most dangerous challenge to trust in science: once someone is inside one where scientists are considered outsiders and discredited, it is almost impossible to get them to reconsider their beliefs, especially when the suggestion to do so comes from those outside the echo chamber. Once again it is important to remember that although echo chambers are not completely online phenomena, the way algorithms are employed in social media do seem to aid in their formation. For a detailed explanation of these phenomena, see Stuart Ritchie's book, Science Fictions (2020). \u21a9 Even the crucial concept of 'statistically significant' has led to a lot of confusion, as the word significant seems to allude to an effect which is big and important in some way, where it actually means that the effect found is sufficiently different from what we would expect to see if there was no effect (Ritchie, 2020, 133).[@ritchie2020] The related concept of p-value has also lent itself for gross misunderstanding. In fact, a study found that 89% of Introduction to Psychology textbooks got the definition wrong.[@ritchie2020] - [@cassidy2019] \u21a9 In order to be concise, I will use the term misinformation to refer to both misinformation and disinformation unless explicitly stated. \u21a9","title":"10. Filter bubbles and echo chambers"},{"location":"rri/","text":"About this Course \u00b6 Responsible scientific research and technological innovation (RRI) is a vital component of a flourishing and fair society. As an area of study and mode of enquiry, RRI plays a central role within academic, public, private, and third-sector organisations. For example, the UKRI\u2019s Engineering and Physical Sciences Research Council (EPSRC) is increasingly making a commitment to RRI necessary for research funding, and also embedding RRI training into its Centres for Doctoral Training. Furthermore, the UK Government has highlighted the importance of RRI in both of its national data and national AI strategies. Building on these commitments, this course will explore what it means to take (individual and collective) responsibility for (and over) the processes and outcomes of research and innovation in data science and AI. The notion of \u2018responsibility\u2019 employed throughout this course will be grounded in an understanding of the moral relationship between science, technology, and society, exploring both historical and contemporary examples of RRI practices. As well as looking at the theoretical basis of RRI this course will also take a hands-on approach by exploring a variety of tools and procedures that can help operationalise and implement a robust notion of responsibility within research and innovation practices. Who is this Guidebook For? \u00b6 Primarily, this guidebook is for researchers with an active interest in RRI. This doesn't mean you have to be a data scientist, or a researcher using R or Python to analyse data. You could be an ethicist, sociologist, or someone with an interest in law and policy. However, the guide is oriented towards research issues and related topics. In addition, while this course has practical, and sometimes hands-on activities, these activities are geared towards ethical reflection and deliberation. If you want to dive deeper into the specific day-to-day requirements of RRI for data science, we recommend heading on over to The Turing Way community, organised by our fantastic colleagues. Learning Objectives \u00b6 This guidebook has the following learning objectives: Understand what is meant by the term \u2018responsible research and innovation\u2019, including the motivation and historical context for its increasing relevance. Identify and evaluate the ethical issues associated with the key stages of a typical data science or AI project lifecycle: (project) design, (model) development, (system) deployment. Explore practical tools and mechanisms for operationalising the concept of \u2018responsibility\u2019 within the context of data science and AI research and innovation. Gain an appreciation of shared goals and values across scientific disciplines and research domains through dialogue with other participants. Table of Contents \u00b6 :octicons-beaker-16:{ .lg .middle } What is Responsible Research and Innovation? This chapter looks at foundational concepts and topics associated with responsible research and innovation (RRI). :octicons-arrow-right-24: Go to chapter :material-database-eye:{ .lg .middle } Responsible Data Science and AI This chapter applies the concepts and lessons of the previous chapter to the context of data science and AI. :octicons-arrow-right-24: Go to chapter :fontawesome-solid-arrows-spin:{ .lg .middle } The Project Lifecycle This chapter introduces the model and framework of the ML/AI project lifecycle, and explores the constituent stages. :octicons-arrow-right-24: Go to chapter :material-chat-processing:{ .lg .middle } Responsible Communication This chapter explores and critically examines what it means to act responsibly when communicating the processes by which a project is governed. :octicons-arrow-right-24: Go to chapter :material-forward:{ .lg .middle } Conclusion The concluding chapter looks forwards and points to further resources for those interested in embedding RRI into their own work. :octicons-arrow-right-24: Go to chapter","title":"(RRI) About this Course"},{"location":"rri/#about-this-course","text":"Responsible scientific research and technological innovation (RRI) is a vital component of a flourishing and fair society. As an area of study and mode of enquiry, RRI plays a central role within academic, public, private, and third-sector organisations. For example, the UKRI\u2019s Engineering and Physical Sciences Research Council (EPSRC) is increasingly making a commitment to RRI necessary for research funding, and also embedding RRI training into its Centres for Doctoral Training. Furthermore, the UK Government has highlighted the importance of RRI in both of its national data and national AI strategies. Building on these commitments, this course will explore what it means to take (individual and collective) responsibility for (and over) the processes and outcomes of research and innovation in data science and AI. The notion of \u2018responsibility\u2019 employed throughout this course will be grounded in an understanding of the moral relationship between science, technology, and society, exploring both historical and contemporary examples of RRI practices. As well as looking at the theoretical basis of RRI this course will also take a hands-on approach by exploring a variety of tools and procedures that can help operationalise and implement a robust notion of responsibility within research and innovation practices.","title":"About this Course"},{"location":"rri/#who-is-this-guidebook-for","text":"Primarily, this guidebook is for researchers with an active interest in RRI. This doesn't mean you have to be a data scientist, or a researcher using R or Python to analyse data. You could be an ethicist, sociologist, or someone with an interest in law and policy. However, the guide is oriented towards research issues and related topics. In addition, while this course has practical, and sometimes hands-on activities, these activities are geared towards ethical reflection and deliberation. If you want to dive deeper into the specific day-to-day requirements of RRI for data science, we recommend heading on over to The Turing Way community, organised by our fantastic colleagues.","title":"Who is this Guidebook For?"},{"location":"rri/#learning-objectives","text":"This guidebook has the following learning objectives: Understand what is meant by the term \u2018responsible research and innovation\u2019, including the motivation and historical context for its increasing relevance. Identify and evaluate the ethical issues associated with the key stages of a typical data science or AI project lifecycle: (project) design, (model) development, (system) deployment. Explore practical tools and mechanisms for operationalising the concept of \u2018responsibility\u2019 within the context of data science and AI research and innovation. Gain an appreciation of shared goals and values across scientific disciplines and research domains through dialogue with other participants.","title":"Learning Objectives"},{"location":"rri/#table-of-contents","text":":octicons-beaker-16:{ .lg .middle } What is Responsible Research and Innovation? This chapter looks at foundational concepts and topics associated with responsible research and innovation (RRI). :octicons-arrow-right-24: Go to chapter :material-database-eye:{ .lg .middle } Responsible Data Science and AI This chapter applies the concepts and lessons of the previous chapter to the context of data science and AI. :octicons-arrow-right-24: Go to chapter :fontawesome-solid-arrows-spin:{ .lg .middle } The Project Lifecycle This chapter introduces the model and framework of the ML/AI project lifecycle, and explores the constituent stages. :octicons-arrow-right-24: Go to chapter :material-chat-processing:{ .lg .middle } Responsible Communication This chapter explores and critically examines what it means to act responsibly when communicating the processes by which a project is governed. :octicons-arrow-right-24: Go to chapter :material-forward:{ .lg .middle } Conclusion The concluding chapter looks forwards and points to further resources for those interested in embedding RRI into their own work. :octicons-arrow-right-24: Go to chapter","title":"Table of Contents"},{"location":"rri/activities/case_studies/","text":"Case Studies \u00b6 The following case studies have been designed to work alongside a series of practical activities that cover the various stages of the project lifecycle. They offer only basic information to guide reflective and deliberative activities. If you find that you do not have sufficient information to address an issue that arises during deliberation, you should try to come up with something reasonable that fits the context of your chosen case study and then reflect on what this would mean for the project. For example, when you reach the model selection section, you will have to consider what the benefits and risks could be for choosing a particular model over another, given the type of technology being developed. Because the focus of this course is on normative issues associated with data science and AI, we will not be training any models, so the actual outcome of choosing one model over another will require some informed speculation and reflection. 1) Predicting Risk of Reoffending \u00b6 Summary You are a project team responsible for developing a predictive risk assessment tool that can support sentencing decisions by judges in criminal courts. The tool will take data about a defendant and feed this into an algorithm that predicts a risk score , between 1 and 5 that is presented to the judge alongside additional case evidence (e.g., witness testimony). This score will represent the likelihood of reoffending, and, therefore, inform the sentencing decision made by the judge. For those that are discharged, the system will also receive feedback about whether the defendant goes on to reoffend. Details \u00b6 Category Details Type of technology: Decision Support Tool Context of Use: Sentencing Decisions in Criminal Courts Outcome: A Risk Score Project Team: Data scientists working for the courts Data Types: Age , Gender , Crime Committed , Postcode , Housing Status , Level of education , Occupation , Past offences , Feedback from police and parole officers (i.e., whether the defendant goes on to reoffend) Data Source(s): Criminal Court Data 2) Recommending Courses \u00b6 Summary You work for an EdTech company and need to develop a recommender system that will be sold to schools to augment careers advice for students considering university courses. The system will ask each student to answer a series of questions, and will then provide an ordered list of recommended courses (linked to the respective university) that it \"believes\" are good options for the student. The system will also use satisfaction survey results and obtained degree results from those students who used the system previously as a way of calibrating and adjusting its recommendations (i.e., learning). Category Details Type of technology: Recommendation System Context of Use: Secondary Schools or Colleges Outcome: A ranking of possible degrees and career paths Project Team: Private EdTech Organisation Data Types: Courses currently being studied at school , Current grades , Gender , Age , Postcode , Extracurricular activities , Interests/Hobbies (from list of pre-selected options), Satisfaction survey results from previous users of system , University grades of previous users of system Data Source(s): Input by Student, Gathered from Partner Universities 3) Classifying Hate Speech \u00b6 Summary You are a team of social data scientists employed as consultants for a social media company. You have been tasked with reducing the levels of hate speech on the company's platform by developing a classifier that can flag potential instances of hate speech for review by human moderators. The tool will automatically review every post submitted to the platform, but will only flag those that are likely to represent an instance of hate speech, based on whether they exceed some likelihood threshold. In addition to the textual content contained within the post, your tool can also use a variety of other input sources to improve its decision-making, including feedback from the human moderators that help improve the accuracy of the model over time. Category Details Type of technology: Hate Speech Classifier Context of Use: Social Media Platform Outcome: Binary variable ('hate speech' or 'not hate speech') with confidence rating Project Team: Social Media Consultants and Platform Data Types: Text content of post , Links or URLs , Network or connections of user , Tags , Images , Use of emojis , Liked communities , Stored cookies , Moderator feedback Data Source(s): Social Media Company Data","title":"Case Studies"},{"location":"rri/activities/case_studies/#case-studies","text":"The following case studies have been designed to work alongside a series of practical activities that cover the various stages of the project lifecycle. They offer only basic information to guide reflective and deliberative activities. If you find that you do not have sufficient information to address an issue that arises during deliberation, you should try to come up with something reasonable that fits the context of your chosen case study and then reflect on what this would mean for the project. For example, when you reach the model selection section, you will have to consider what the benefits and risks could be for choosing a particular model over another, given the type of technology being developed. Because the focus of this course is on normative issues associated with data science and AI, we will not be training any models, so the actual outcome of choosing one model over another will require some informed speculation and reflection.","title":"Case Studies"},{"location":"rri/activities/case_studies/#1-predicting-risk-of-reoffending","text":"Summary You are a project team responsible for developing a predictive risk assessment tool that can support sentencing decisions by judges in criminal courts. The tool will take data about a defendant and feed this into an algorithm that predicts a risk score , between 1 and 5 that is presented to the judge alongside additional case evidence (e.g., witness testimony). This score will represent the likelihood of reoffending, and, therefore, inform the sentencing decision made by the judge. For those that are discharged, the system will also receive feedback about whether the defendant goes on to reoffend.","title":"1) Predicting Risk of Reoffending"},{"location":"rri/activities/case_studies/#details","text":"Category Details Type of technology: Decision Support Tool Context of Use: Sentencing Decisions in Criminal Courts Outcome: A Risk Score Project Team: Data scientists working for the courts Data Types: Age , Gender , Crime Committed , Postcode , Housing Status , Level of education , Occupation , Past offences , Feedback from police and parole officers (i.e., whether the defendant goes on to reoffend) Data Source(s): Criminal Court Data","title":"Details"},{"location":"rri/activities/case_studies/#2-recommending-courses","text":"Summary You work for an EdTech company and need to develop a recommender system that will be sold to schools to augment careers advice for students considering university courses. The system will ask each student to answer a series of questions, and will then provide an ordered list of recommended courses (linked to the respective university) that it \"believes\" are good options for the student. The system will also use satisfaction survey results and obtained degree results from those students who used the system previously as a way of calibrating and adjusting its recommendations (i.e., learning). Category Details Type of technology: Recommendation System Context of Use: Secondary Schools or Colleges Outcome: A ranking of possible degrees and career paths Project Team: Private EdTech Organisation Data Types: Courses currently being studied at school , Current grades , Gender , Age , Postcode , Extracurricular activities , Interests/Hobbies (from list of pre-selected options), Satisfaction survey results from previous users of system , University grades of previous users of system Data Source(s): Input by Student, Gathered from Partner Universities","title":"2) Recommending Courses"},{"location":"rri/activities/case_studies/#3-classifying-hate-speech","text":"Summary You are a team of social data scientists employed as consultants for a social media company. You have been tasked with reducing the levels of hate speech on the company's platform by developing a classifier that can flag potential instances of hate speech for review by human moderators. The tool will automatically review every post submitted to the platform, but will only flag those that are likely to represent an instance of hate speech, based on whether they exceed some likelihood threshold. In addition to the textual content contained within the post, your tool can also use a variety of other input sources to improve its decision-making, including feedback from the human moderators that help improve the accuracy of the model over time. Category Details Type of technology: Hate Speech Classifier Context of Use: Social Media Platform Outcome: Binary variable ('hate speech' or 'not hate speech') with confidence rating Project Team: Social Media Consultants and Platform Data Types: Text content of post , Links or URLs , Network or connections of user , Tags , Images , Use of emojis , Liked communities , Stored cookies , Moderator feedback Data Source(s): Social Media Company Data","title":"3) Classifying Hate Speech"},{"location":"rri/chapter1/","text":"What is Responsible Research and Innovation? \u00b6 Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk)) Chapter Outline \u00b6 What is Responsibility? A Short History of RRI Science, Technology, and Society Science and Technology Studies (A Timeline) Chapter Summary This section defines the term 'responsible', as it occurs in 'responsible research and innovation', and also explores a short history of RRI with reference to notable events and case studies that help illustrate central themes and issues. The chapter helps to set the foundation for the remainder of the course, which will rely on many of the core concepts that are introduced. However, the section also plays a motivating role by highlighting the interconnected nature of science, technology, and society. Learning Objectives In this chapter, you will: Learn what is meant by the term \u2018responsibility\u2019. Familiarise yourself with several principles that characterise RRI. Explore illustrative and historical case studies. Reflect on how the interactions between science, technology, and society give rise to many normative issues. Review pivotal moments in the history of science and technology studies (STS).","title":"Introduction"},{"location":"rri/chapter1/#what-is-responsible-research-and-innovation","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))","title":"What is Responsible Research and Innovation?"},{"location":"rri/chapter1/#chapter-outline","text":"What is Responsibility? A Short History of RRI Science, Technology, and Society Science and Technology Studies (A Timeline) Chapter Summary This section defines the term 'responsible', as it occurs in 'responsible research and innovation', and also explores a short history of RRI with reference to notable events and case studies that help illustrate central themes and issues. The chapter helps to set the foundation for the remainder of the course, which will rely on many of the core concepts that are introduced. However, the section also plays a motivating role by highlighting the interconnected nature of science, technology, and society. Learning Objectives In this chapter, you will: Learn what is meant by the term \u2018responsibility\u2019. Familiarise yourself with several principles that characterise RRI. Explore illustrative and historical case studies. Reflect on how the interactions between science, technology, and society give rise to many normative issues. Review pivotal moments in the history of science and technology studies (STS).","title":"Chapter Outline"},{"location":"rri/chapter1/history/","text":"Understanding RRI \u00b6 The term 'responsible research and innovation' is most strongly associated with the European Commission's Framework Programmes for Research and Technological Development\u2014a set of funding programmes that support research in the European Union. Beginning with the seventh framework programme in 2010, and continuing on through Horizon 2020 (FP8), the term 'responsible research and innovation' became increasingly important for the European Commission's policy. Since then, other national funding bodies have also shown a commitment to RRI. For example, UKRI's Engineering and Physical Sciences Research Council have developed the AREA framework, which sets out four principles for RRI: Anticipate, Reflect, Engage, and Act (AREA). In almost all cases, two significant and motivating drivers behind these policies and principles are (a) an awareness of the impact that science and technology can have on society, and (b) an appreciation of the need to include the public in a dialogue about how science and technology should shape society. The following three case studies help to provide illustrations of these points, while also serving as useful examples that will be returned to in subsequent discussions. Case Study 1: Tuskegee Syphilis Study \u00b6 Doctor drawing blood from a patient as part of the Tuskegee Syphilis Study (Reprinted from Wikimedia Commons\u2014https://commons.wikimedia.org/wiki/File:Tuskegee-syphilis-study_doctor-injecting-subject.jpg). Starting in 1932, the U.S. Public Health Service ran a study of \u201cuntreated syphilis in the male Negro\u201d, which affected almost 400 African-American men with the disease.[@reverby2001] As we know today, Syphilis can cause many symptoms including sores, blindness, hair loss, stroke, heart failure, and even death if left untreated. However, aside from the risk that these men were exposed to, a particularly abusive aspect of the study was that it was carried out on impoverished individuals , affected by the Great Depression, all while telling them they were being \u201ctreated\u201d for their \u201cbad blood\u201d.[@reverby2001] The study created a massive outcry but, nevertheless, continued for 40 years until 1972. In this time, syphilis became treatable as a result of the increased availability of penicillin, and funding for the study was withdrawn. However, as one of the participants states, Quote ''The thing that disturbs me now is that they found a cure,'' Shaw told the Baltimore Sun. ''They found penicillin. And they never gave it to us. It vexed me awfully sadly.''[@duffbrown2017] By the time the study ended, 128 participants had died from syphilis or related complications. Moreover, 40 of the participants\u2019 spouses had been infected, and 19 children were born with congenital syphilis. It should be obvious why this case study is infamous and well-rehearsed in courses on research ethics or biomedical ethics.[@beauchamp2013] However, there are many reasons for the study's continued infamy that have direct relevance to RRI, including the need to continually reflect on the structural biases that exist in society and create forms of racial discrimination that researchers should not ignore. One specific consideration is the fact that the study directly influenced a landmark event in the ethical importance of informed consent: the 1979 Belmont Report.[@nationalcommission1979] This report, produced by the US National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research, set out a variety of guidelines for clinical research, including the requirement for participants to have an understanding of the research being conducted, in order to provide informed consent. Quote Respect for persons requires that subjects, to the degree that they are capable, be given the opportunity to choose what shall or shall not happen to them. This opportunity is provided when adequate standards for informed consent are satisfied.[@nationalcommission1979] The conditions that need to be satisfied are, Disclosure of sufficient information Ensuring participant's comprehension Voluntarily providing consent Neither (1) nor (2) were satisfied in the Tuskegee study, and it would be difficult to argue that (3) was satisfied given the socioeconomic deprivation which affected the participants. Aside from the historical importance of this study, its inclusion here is also to help highlight why so much significance is placed on the responsible participation of stakeholder and affected users or individuals in present day research and activity. We will say more about this in the next section . Case Study 2: Human Genome Project \u00b6 DNA Double Helix The Human Genome Project was proposed in 1986 following a project feasibility workshop in Santa Fe, New Mexico. At the time, the US Department of Energy's Health and Environmental Research Advisory Committee urged the department Quote to commit to a large, long-term, multidisciplinary, technological undertaking to order and sequence the human genome.[@barnhart1989] Nearly 20 years and 2.7 billion dollars later, on April 14, 2003, the project was formally announced as complete. The project had been an international effort to identify all of our ~20,500 genes and determine the sequences of nearly 3 billion chemical base pairs make up our DNA. The scale of the project and the magnitude of the technological accomplishments should be praised in their own rights. But even more impressive is the continuing impact that this project has had on research projects, collaborative practices, and technological advancements.[@mcguire2020] Even from the very beginning of the project, it was clear that expanding our knowledge of genetics and genomics would have profound impacts on society\u2014not all of which would be positive. For example, concerns were voiced about whether the knowledge could be used to further discriminate against certain individuals or sub-groups of the human population, raising the spectre of past injustices caused by the practice of eugenics. 1 Therefore, in 1990, the National Human Genome Research Institute founded a program to specifically oversee and study the project's ethical, legal, and social implications, known as the ELSI program.[@nhgri2021] Writing several years after the completion of the project, the manager of the ELSI program, Michael S. Yesley, offered some remarks that are worth quoting in full. Quote The qualifications to do bioethics analysis are straightforward: familiarity with, and ability to analyze, the relevant facts and values . No discipline or profession has a monopoly on these skills or should dominate the process. Of the major disciplines engaged in bioethics, philosophy is useful in raising questions and providing rationale, but the actual resolution of bioethics issues \u2013 deciding which course of action to take or recommend \u2013 generally relies most heavily on factual analysis and seldom on philosophical insight alone. Law sometimes resolves bioethics issues but in most cases establishes only what is socially permissible , not what is most desirable, or merely imposes procedural requirements rather than a substantive result. The ethics traditions of medicine and science pervade bioethics and provide much guidance, but these professional perspectives have built-in conflicts that practitioners may not recognize when balancing the rights and interests of others . Social science is an obvious source of empirical information about both facts and values, but just as other fields play limited roles in bioethics, social science must be integrated in the broad policymaking process. To be useful in this process, social science must view bioethics policymaking as the goal, not the object, of its study.[@yesley2008] (emphasis added) Three things stand out about this quotation. First, Yesley is acutely aware of the complex relationship between both facts and values, but also between competing value perspectives when analysing and determining what is desirable from scientific research or technological innovation. Second, as a lawyer, he also has an appreciation of the normative limitations that can be derived from legal precedents, acknowledging that the law tends to establish that which is socially permissible but not what is most desirable . To paraphrase, the law can help create guardrails but is often unable on its own to set the direction of travel. And, finally, Yesley recognises that reflection on ethical, legal, and social implications counts for little without an ability to influence and shape policy. This final point is important, as Yesley continues by acknowledging how the model employed by ELSI reflected little consideration of whether it ''would be useful or appropriate to develop public policy that could potentially question the direction of the [Human Genome Project's] scientific research''. And, furthemore, that the project's scientist-administrators, ''established ELSI simply by earmarking funds from their science budget, and they controlled the content of ELSI by determining the boundaries of the funded research. No one would represent the public interest in the administration of ELSI, which would lack at its core an independent, representative entity to analyze the issues, determine research needs, analyze research results and develop well-supported policy recommendations''. At first glance, therefore, the 5% of the annual budget of the Human Genome Project that was earmarked for ELSI research seems impressive, and certainly unprecedented in terms of ethical funding. But Yesley's comments should give us pause to ask how the language of ethics\u2014and indeed, RRI\u2014can be co-opted by commercial, scientific, and political interests, rather than being used to cast a critical perspective on the most pressing questions that face society. This is all the more important when one recognises how many of the questions raised by the Human Genome Project are also brought up in the context of data ethics and AI ethics, such as the possibility of discriminatory outcomes, or concerns about \"ethics-washing\" by those with vested interests in the advancement of science and technology. As we will see in the next section, it is important to ask what social goal is being served by research and innovation, and whether such a goal is desirable as well as permissible. Case Study 3: Cambridge Analytica \u00b6 Cambridge Analytica Logo In 2013, three researchers at the University of Cambridge and Microsoft Research published a paper in the Proceedings of the National Academy of Sciences. The paper was titled, 'Private traits and attributes are predictable from digital records of human behavior', and it provided details of an application (MyPersonality) that allowed Facebook users to participate in a range of psychometric tests, including a personality test, an intelligence test, and a Satisfaction with Life survey. Following these tests, users were asked if they were happy for their social media profile data to be collected for research purposes. This included, where available, the various \u201cLikes\u201d of the users; their age, gender, sexual orientation, relationship status, political views, religion, and social network information (e.g. network density); details of the users\u2019 consumption of alcohol, drugs, and cigarettes, and whether their parents stayed together until the user was 21 years old; and also visual inspection of profile pictures, in order to assign ethnicity to a randomly selected subsample of users. The purpose of gathering this information was to see whether psychological traits could be predicted from social media data. In short, could the results from the user's psychometric tests be inferred from the data gathered from their social media profiles. The results were mixed and gave rise to many questions about validity, reliability, and generalisability.[@burr2019] However, the results or these related questions are not our present concern. Almost four years after the publication of their research paper, in 2017, Donald Trump was inaugurated as President of the United States and the United Kingdom gave formal notice of its intent to withdraw from the EU following the Brexit referendum. In an attempt to make sense of these surprising events, a large number of investigative journalists started contacting the three researchers enquiring about their links to a political consulting firm known as Cambridge Analytica. Despite having never worked with Cambridge Analytica, and refusing their requests, the method the researchers described in their 2013 paper led the firm to work with another Cambridge University researcher to develop their own app.[@weaver2018] It was later revealed that this app enabled Cambridge Analytica to access the data of up to 87 million Facebook users without their knowledge or permission due to poor data privacy and protection policies on the platform.[@kang2018] The firm subsequently used this data to develop and sell predictive analytics services to the Trump administration, influencing the outcomes of the US election, and also attempt to court the Leave.EU Brexit campaign.[@ball2020] The events that surround the Cambridge Analytica data scandal read as though they were plucked from a novel about espionage, information warfare, and PSYOPS, carefully woven together and exposed by the tireless efforts of investigative journalists.[@cadwalladr2017] Our interest in the case study is, unfortunately, less glamorous. It can be captured by a single question: Quote If you were one of the original researchers, back in 2013, investigating whether social media data could be used to infer otherwise private information about the psychological attitudes and beliefs of users, would you think you were behaving irresponsibly by publishing your research? In our first activity, we'll reflect on this question and others related to these three case studies. This concern became the centrepiece for the 1997 dystopian sci-fi, Gattaca, which takes its title from the four letters of the nucleobases of DNA. \u21a9","title":"A Short History of RRI"},{"location":"rri/chapter1/history/#understanding-rri","text":"The term 'responsible research and innovation' is most strongly associated with the European Commission's Framework Programmes for Research and Technological Development\u2014a set of funding programmes that support research in the European Union. Beginning with the seventh framework programme in 2010, and continuing on through Horizon 2020 (FP8), the term 'responsible research and innovation' became increasingly important for the European Commission's policy. Since then, other national funding bodies have also shown a commitment to RRI. For example, UKRI's Engineering and Physical Sciences Research Council have developed the AREA framework, which sets out four principles for RRI: Anticipate, Reflect, Engage, and Act (AREA). In almost all cases, two significant and motivating drivers behind these policies and principles are (a) an awareness of the impact that science and technology can have on society, and (b) an appreciation of the need to include the public in a dialogue about how science and technology should shape society. The following three case studies help to provide illustrations of these points, while also serving as useful examples that will be returned to in subsequent discussions.","title":"Understanding RRI"},{"location":"rri/chapter1/history/#case-study-1-tuskegee-syphilis-study","text":"Doctor drawing blood from a patient as part of the Tuskegee Syphilis Study (Reprinted from Wikimedia Commons\u2014https://commons.wikimedia.org/wiki/File:Tuskegee-syphilis-study_doctor-injecting-subject.jpg). Starting in 1932, the U.S. Public Health Service ran a study of \u201cuntreated syphilis in the male Negro\u201d, which affected almost 400 African-American men with the disease.[@reverby2001] As we know today, Syphilis can cause many symptoms including sores, blindness, hair loss, stroke, heart failure, and even death if left untreated. However, aside from the risk that these men were exposed to, a particularly abusive aspect of the study was that it was carried out on impoverished individuals , affected by the Great Depression, all while telling them they were being \u201ctreated\u201d for their \u201cbad blood\u201d.[@reverby2001] The study created a massive outcry but, nevertheless, continued for 40 years until 1972. In this time, syphilis became treatable as a result of the increased availability of penicillin, and funding for the study was withdrawn. However, as one of the participants states, Quote ''The thing that disturbs me now is that they found a cure,'' Shaw told the Baltimore Sun. ''They found penicillin. And they never gave it to us. It vexed me awfully sadly.''[@duffbrown2017] By the time the study ended, 128 participants had died from syphilis or related complications. Moreover, 40 of the participants\u2019 spouses had been infected, and 19 children were born with congenital syphilis. It should be obvious why this case study is infamous and well-rehearsed in courses on research ethics or biomedical ethics.[@beauchamp2013] However, there are many reasons for the study's continued infamy that have direct relevance to RRI, including the need to continually reflect on the structural biases that exist in society and create forms of racial discrimination that researchers should not ignore. One specific consideration is the fact that the study directly influenced a landmark event in the ethical importance of informed consent: the 1979 Belmont Report.[@nationalcommission1979] This report, produced by the US National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research, set out a variety of guidelines for clinical research, including the requirement for participants to have an understanding of the research being conducted, in order to provide informed consent. Quote Respect for persons requires that subjects, to the degree that they are capable, be given the opportunity to choose what shall or shall not happen to them. This opportunity is provided when adequate standards for informed consent are satisfied.[@nationalcommission1979] The conditions that need to be satisfied are, Disclosure of sufficient information Ensuring participant's comprehension Voluntarily providing consent Neither (1) nor (2) were satisfied in the Tuskegee study, and it would be difficult to argue that (3) was satisfied given the socioeconomic deprivation which affected the participants. Aside from the historical importance of this study, its inclusion here is also to help highlight why so much significance is placed on the responsible participation of stakeholder and affected users or individuals in present day research and activity. We will say more about this in the next section .","title":"Case Study 1: Tuskegee Syphilis Study"},{"location":"rri/chapter1/history/#case-study-2-human-genome-project","text":"DNA Double Helix The Human Genome Project was proposed in 1986 following a project feasibility workshop in Santa Fe, New Mexico. At the time, the US Department of Energy's Health and Environmental Research Advisory Committee urged the department Quote to commit to a large, long-term, multidisciplinary, technological undertaking to order and sequence the human genome.[@barnhart1989] Nearly 20 years and 2.7 billion dollars later, on April 14, 2003, the project was formally announced as complete. The project had been an international effort to identify all of our ~20,500 genes and determine the sequences of nearly 3 billion chemical base pairs make up our DNA. The scale of the project and the magnitude of the technological accomplishments should be praised in their own rights. But even more impressive is the continuing impact that this project has had on research projects, collaborative practices, and technological advancements.[@mcguire2020] Even from the very beginning of the project, it was clear that expanding our knowledge of genetics and genomics would have profound impacts on society\u2014not all of which would be positive. For example, concerns were voiced about whether the knowledge could be used to further discriminate against certain individuals or sub-groups of the human population, raising the spectre of past injustices caused by the practice of eugenics. 1 Therefore, in 1990, the National Human Genome Research Institute founded a program to specifically oversee and study the project's ethical, legal, and social implications, known as the ELSI program.[@nhgri2021] Writing several years after the completion of the project, the manager of the ELSI program, Michael S. Yesley, offered some remarks that are worth quoting in full. Quote The qualifications to do bioethics analysis are straightforward: familiarity with, and ability to analyze, the relevant facts and values . No discipline or profession has a monopoly on these skills or should dominate the process. Of the major disciplines engaged in bioethics, philosophy is useful in raising questions and providing rationale, but the actual resolution of bioethics issues \u2013 deciding which course of action to take or recommend \u2013 generally relies most heavily on factual analysis and seldom on philosophical insight alone. Law sometimes resolves bioethics issues but in most cases establishes only what is socially permissible , not what is most desirable, or merely imposes procedural requirements rather than a substantive result. The ethics traditions of medicine and science pervade bioethics and provide much guidance, but these professional perspectives have built-in conflicts that practitioners may not recognize when balancing the rights and interests of others . Social science is an obvious source of empirical information about both facts and values, but just as other fields play limited roles in bioethics, social science must be integrated in the broad policymaking process. To be useful in this process, social science must view bioethics policymaking as the goal, not the object, of its study.[@yesley2008] (emphasis added) Three things stand out about this quotation. First, Yesley is acutely aware of the complex relationship between both facts and values, but also between competing value perspectives when analysing and determining what is desirable from scientific research or technological innovation. Second, as a lawyer, he also has an appreciation of the normative limitations that can be derived from legal precedents, acknowledging that the law tends to establish that which is socially permissible but not what is most desirable . To paraphrase, the law can help create guardrails but is often unable on its own to set the direction of travel. And, finally, Yesley recognises that reflection on ethical, legal, and social implications counts for little without an ability to influence and shape policy. This final point is important, as Yesley continues by acknowledging how the model employed by ELSI reflected little consideration of whether it ''would be useful or appropriate to develop public policy that could potentially question the direction of the [Human Genome Project's] scientific research''. And, furthemore, that the project's scientist-administrators, ''established ELSI simply by earmarking funds from their science budget, and they controlled the content of ELSI by determining the boundaries of the funded research. No one would represent the public interest in the administration of ELSI, which would lack at its core an independent, representative entity to analyze the issues, determine research needs, analyze research results and develop well-supported policy recommendations''. At first glance, therefore, the 5% of the annual budget of the Human Genome Project that was earmarked for ELSI research seems impressive, and certainly unprecedented in terms of ethical funding. But Yesley's comments should give us pause to ask how the language of ethics\u2014and indeed, RRI\u2014can be co-opted by commercial, scientific, and political interests, rather than being used to cast a critical perspective on the most pressing questions that face society. This is all the more important when one recognises how many of the questions raised by the Human Genome Project are also brought up in the context of data ethics and AI ethics, such as the possibility of discriminatory outcomes, or concerns about \"ethics-washing\" by those with vested interests in the advancement of science and technology. As we will see in the next section, it is important to ask what social goal is being served by research and innovation, and whether such a goal is desirable as well as permissible.","title":"Case Study 2: Human Genome Project"},{"location":"rri/chapter1/history/#case-study-3-cambridge-analytica","text":"Cambridge Analytica Logo In 2013, three researchers at the University of Cambridge and Microsoft Research published a paper in the Proceedings of the National Academy of Sciences. The paper was titled, 'Private traits and attributes are predictable from digital records of human behavior', and it provided details of an application (MyPersonality) that allowed Facebook users to participate in a range of psychometric tests, including a personality test, an intelligence test, and a Satisfaction with Life survey. Following these tests, users were asked if they were happy for their social media profile data to be collected for research purposes. This included, where available, the various \u201cLikes\u201d of the users; their age, gender, sexual orientation, relationship status, political views, religion, and social network information (e.g. network density); details of the users\u2019 consumption of alcohol, drugs, and cigarettes, and whether their parents stayed together until the user was 21 years old; and also visual inspection of profile pictures, in order to assign ethnicity to a randomly selected subsample of users. The purpose of gathering this information was to see whether psychological traits could be predicted from social media data. In short, could the results from the user's psychometric tests be inferred from the data gathered from their social media profiles. The results were mixed and gave rise to many questions about validity, reliability, and generalisability.[@burr2019] However, the results or these related questions are not our present concern. Almost four years after the publication of their research paper, in 2017, Donald Trump was inaugurated as President of the United States and the United Kingdom gave formal notice of its intent to withdraw from the EU following the Brexit referendum. In an attempt to make sense of these surprising events, a large number of investigative journalists started contacting the three researchers enquiring about their links to a political consulting firm known as Cambridge Analytica. Despite having never worked with Cambridge Analytica, and refusing their requests, the method the researchers described in their 2013 paper led the firm to work with another Cambridge University researcher to develop their own app.[@weaver2018] It was later revealed that this app enabled Cambridge Analytica to access the data of up to 87 million Facebook users without their knowledge or permission due to poor data privacy and protection policies on the platform.[@kang2018] The firm subsequently used this data to develop and sell predictive analytics services to the Trump administration, influencing the outcomes of the US election, and also attempt to court the Leave.EU Brexit campaign.[@ball2020] The events that surround the Cambridge Analytica data scandal read as though they were plucked from a novel about espionage, information warfare, and PSYOPS, carefully woven together and exposed by the tireless efforts of investigative journalists.[@cadwalladr2017] Our interest in the case study is, unfortunately, less glamorous. It can be captured by a single question: Quote If you were one of the original researchers, back in 2013, investigating whether social media data could be used to infer otherwise private information about the psychological attitudes and beliefs of users, would you think you were behaving irresponsibly by publishing your research? In our first activity, we'll reflect on this question and others related to these three case studies. This concern became the centrepiece for the 1997 dystopian sci-fi, Gattaca, which takes its title from the four letters of the nucleobases of DNA. \u21a9","title":"Case Study 3: Cambridge Analytica"},{"location":"rri/chapter1/responsibility/","text":"What is 'Responsibility'? \u00b6 If we want a systematic and reliable method for evaluating research and innovation projects\u2014whether past, present, or future instances\u2014we need to have an operationalisable definition of 'responsibility'. Fortunately, we don't have to start from scratch. In an oft-cited article, Von Schomberg defines RRI as follows: Quote \u201cResponsible Research and Innovation is a transparent, interactive process by which societal actors and innovators become mutually responsive to each other with a view on the (ethical) acceptability, sustainability and societal desirability of the innovation process and its marketable products (in order to allow a proper embedding of scientific and technological advances in our society).\u201d[@vonschomberg2011] This is a helpful definition, but it can be challenging to grasp its meaning on first glance, so let's break it down. First, we have the phrase, \"transparent, interactive process by which societal actors and innovators become mutually responsive to each other\" . This is straightforward enough. Without transparency and interaction between innovators, researchers, and society, associated outcomes cannot be meaningfully scrutinised or challenged. As such, possible harms or unintended consequences may go unnoticed. For example, if a medical research team failed to interact with and explain to their study participants the risks of a novel drug they are testing, the lack of transparency may prevent the participants from giving their meaningful and informed consent. Second, we have the goal to which the transparent, interactive process is directed: \"with a view on the (ethical) acceptability, sustainability and societal desirability of the innovation process\". The process of (responsible) research and innovation is necessarily dialogical and inclusive because it acknowledges an inherent pluralism of values that are implicated within the research and innovation lifecycle. For instance, many would agree that technological advances in renewable energy are desirable because of their contribution to a more sustainable future. But there may be disagreement about specific projects, such as the development of a hydroelectric power plant that displaces downstream communities by disrupting the local ecology. Such a tension between values can only be observed and resolved through a transparent and interactive process with affected and impacted stakeholders. Finally, von Schomberg's definition draws our attention to how RRI facilitates a, \"proper embedding of scientific and technological advances in our society\". This is another way of saying that scientific research and technological innovation do not operate in a vacuum. It draws our attention to the fact that the practices and processes or research and innovation occur at a specific place and time, and also to an awareness of their consequences, which can vary in scope and impact. Obvious examples, such as the development of the atomic bomb, the discovery of penicillin, or the invention of the internet may spring to mind. But all research and innovation has the potential to reshape society and social norms or expectations. RRI takes this embedding as a starting point, taking seriously the moral duties and obligations that such an embedding inculcates. Von Schomberg's definition helps to make clear the importance of social context for identifying the scope of societal responsibility, and subsequently helps emphasise the need for public and stakeholder engagement in delineating this scope\u2014a point to which we will return . But other key characteristics and principles of RRI remain under the surface. To identify these characteristics and principles, there are several frameworks for RRI that we could turn to and explore to identify the remaining principles. However, this would give rise to misleading impression that RRI is a top-down approach, in which we start with pre-determined principles that exist independently of the interconnected practices of science, technology, and society. In contrast, RRI is a normative framework that has been constructed slowly and iteratively in response to tangible social harms and felt injustices. The principles that have emerged have been shaped by such events. Therefore, although we will eventually discuss a set of principles for RRI, it is important to first explore some notable historical case studies.","title":"What is Responsibility"},{"location":"rri/chapter1/responsibility/#what-is-responsibility","text":"If we want a systematic and reliable method for evaluating research and innovation projects\u2014whether past, present, or future instances\u2014we need to have an operationalisable definition of 'responsibility'. Fortunately, we don't have to start from scratch. In an oft-cited article, Von Schomberg defines RRI as follows: Quote \u201cResponsible Research and Innovation is a transparent, interactive process by which societal actors and innovators become mutually responsive to each other with a view on the (ethical) acceptability, sustainability and societal desirability of the innovation process and its marketable products (in order to allow a proper embedding of scientific and technological advances in our society).\u201d[@vonschomberg2011] This is a helpful definition, but it can be challenging to grasp its meaning on first glance, so let's break it down. First, we have the phrase, \"transparent, interactive process by which societal actors and innovators become mutually responsive to each other\" . This is straightforward enough. Without transparency and interaction between innovators, researchers, and society, associated outcomes cannot be meaningfully scrutinised or challenged. As such, possible harms or unintended consequences may go unnoticed. For example, if a medical research team failed to interact with and explain to their study participants the risks of a novel drug they are testing, the lack of transparency may prevent the participants from giving their meaningful and informed consent. Second, we have the goal to which the transparent, interactive process is directed: \"with a view on the (ethical) acceptability, sustainability and societal desirability of the innovation process\". The process of (responsible) research and innovation is necessarily dialogical and inclusive because it acknowledges an inherent pluralism of values that are implicated within the research and innovation lifecycle. For instance, many would agree that technological advances in renewable energy are desirable because of their contribution to a more sustainable future. But there may be disagreement about specific projects, such as the development of a hydroelectric power plant that displaces downstream communities by disrupting the local ecology. Such a tension between values can only be observed and resolved through a transparent and interactive process with affected and impacted stakeholders. Finally, von Schomberg's definition draws our attention to how RRI facilitates a, \"proper embedding of scientific and technological advances in our society\". This is another way of saying that scientific research and technological innovation do not operate in a vacuum. It draws our attention to the fact that the practices and processes or research and innovation occur at a specific place and time, and also to an awareness of their consequences, which can vary in scope and impact. Obvious examples, such as the development of the atomic bomb, the discovery of penicillin, or the invention of the internet may spring to mind. But all research and innovation has the potential to reshape society and social norms or expectations. RRI takes this embedding as a starting point, taking seriously the moral duties and obligations that such an embedding inculcates. Von Schomberg's definition helps to make clear the importance of social context for identifying the scope of societal responsibility, and subsequently helps emphasise the need for public and stakeholder engagement in delineating this scope\u2014a point to which we will return . But other key characteristics and principles of RRI remain under the surface. To identify these characteristics and principles, there are several frameworks for RRI that we could turn to and explore to identify the remaining principles. However, this would give rise to misleading impression that RRI is a top-down approach, in which we start with pre-determined principles that exist independently of the interconnected practices of science, technology, and society. In contrast, RRI is a normative framework that has been constructed slowly and iteratively in response to tangible social harms and felt injustices. The principles that have emerged have been shaped by such events. Therefore, although we will eventually discuss a set of principles for RRI, it is important to first explore some notable historical case studies.","title":"What is 'Responsibility'?"},{"location":"rri/chapter1/sts/","text":"Science , Technology, and Society \u00b6 In April 1945, Michael Polanyi\u2014a chemist and sociologist of science\u2014and Bertrand Russell\u2014a philosopher and logician\u2014were speaking on a radio programme about the practical implications of the famous formula, $E = mc^2$. They were asked whether the formula had any practical applications for society, but neither could provide an answer. Three months later the Manhattan project dropped the first of their three atomic bombs! A 3d rendering of an atomic bomb Bridgstock[@bridgstock1998] draws attention to this story because it was used originally by Polanyi, in his essay, 'The Republic of Science' [@polanyi1962], to suggest that the practical and societal outcomes of pure scientific research are often unforeseen and unintended. The problem with this suggestion is that it implies that scientists cannot be held accountable or exercise any real responsibility for the consequences of their research\u2014a troubling implication if true! However, the definitions we have already encountered suggest that science, technology, and society are closely interconnected, and that RRI requires reflection upon the myriad ways that science and technology can impact and shape social norms and practices. Responsibility arises out of this relationship. But if the impacts or consequences are unforeseeable and often unanticipated, then the principles of RRI may be too demanding. Fortunately, the implications of Polanyi's example are narrow in scope. The following thought experiment will help us illustrate why. The Careless CEO Imagine a CEO of a large manufacturing company is approached by one of her scientific advisors and informed that a project that she has proposed will require an environmental impact assessment before it can proceed. The CEO dismisses this and orders that the project continue without the assessment. Furthermore, she callously proclaims that she does not care what the environmental consequences of the project may be. All she is interested in is making as much profit as possible. As it turns out, the project ends up causing vast amounts of pollution that cause irreparable harm to the nearby flora and fauna, and also affects the health of a community living downstream of the manufacturing plant. The CEO is, rightfully, held accountable, both morally and legally, and is prosecuted when her dismissal of the impact assessment is uncovered. Few would take issue with these consequences for the CEO. She had a responsibility to ensure her company's processes operated in a safe and ethical manner, but chose to wilfully neglect this responsibility in spite of receiving advice from her staff. However, let's alter some of the details of this case while keeping the logical structure the same. This time, everything about the thought experiment remains the same except for the outcomes of the project . Now, the project causes no harm. In fact, the efficiency of the new project actually reduces the company's emissions and leads to more sustainable operations. There is no need to worry about accountability in this instance, as no harms occurred. But does the CEO deserve praise for her actions? This question is less likely to have a universal consensus among the answers. Those that believe the CEO deserves no praise are likely to point to the fact that she did not carry out her due diligence or reflect upon the possible consequences of her project. She chose to ignore the suggestion of undertaking an environmental impact assessment, and, therefore, did not act in a responsible manner. She was unable to anticipate any harms or benefits because she did not gather the appropriate evidence. Neither did she act with deliberate intention, but instead acted in a careless manner. In short, she was lucky that the outcomes were positive, and because of a lack of deliberate or intentional action some may argue that there are no legitimate grounds for praise. Others may disagree, and simply argue that the consequences are all that matter. We won't try to settle this debate here. Instead, it can be left for personal reflection. Returning to the issue raised by Polanyi's statement, we can of course acknowledge that he is right to suggest that the societal impacts or consequences of some pure scientific research are hard to anticipate. This is especially true for the more distant effects\u2014consider again the long-term impact of the Human Genome Project, which is arguably still affecting current research. But this is not the case for all research or innovation projects. Many consequences can in fact be more readily anticipated or predicted by carrying out careful activities of reflection and deliberation. There is no question about whether scientists, researchers, and developers have some responsibility for the applications of their research. This must be true for them to be praised for the positive outcomes, which they often are, and also to be held accountable and blamed for the negative consequences when they occur. The question is, rather, when they should receive praise and blame. To help us address this question, we will look at some of the practical ways that scientists, researchers, and developers can take responsibility for the social impacts of their research, in order to maximise the potential opportunities and minimise the possible harms associated with their work. This is a primary objective for the entire course, but we can introduce some of the practical mechanisms now. Risk and Impact Assessments \u00b6 At the start of this chapter we looked at a definition of RRI from.[@vonschomberg2011] Let's look at another one, this time from the European Commission: === \"European Commission\" Responsible research and innovation is an approach that anticipates and assesses potential implications and societal expectations with regard to research and innovation, with the aim to foster the design of inclusive and sustainable research and innovation.[@europeancommission2014] === \"Ren\u00e9 von Schomberg\" Responsible Research and Innovation is a transparent, interactive process by which societal actors and innovators become mutually responsive to each other with a view on the (ethical) acceptability, sustainability and societal desirability of the innovation process and its marketable products (in order to allow a proper embedding of scientific and technological advances in our society).[@vonschomberg2011] There are many similarities between the two definitions, but they also emphasise different aspects through their choice of terminology. For instance, the definition from the European Commission mentions anticipation and assessment , as a means for highlighting the importance of associated activities such as risk or impact assessments. There are many types of risk and impact assessments that can be carried out, such as safety and risk assessments, equality impact assessments, human rights impact assessments, and, of course, data protection impact assessments. The necessity of such assessments will be familiar to those who work in commercial or public sector organisations, but less so to those in academic institutions. Typically they are carried out for compliance reasons. However, the structured nature of such assessments can also support more ethical forms of reflection and anticipation. It is not necessary to present an overview of all the different impact assessments that could be useful within the context of RRI. 1 Instead, we will focus on a process that is central to almost all forms of risk or impact assessments: stakeholder participation. Inclusive and Deliberative Stakeholder Participation \u00b6 Question Why should stakeholders be included in a research or innovation project? There are at least two answers that can be given to this question? To identify and meet stakeholder or user needs To ensure that possible harms that could arise are identified and addressed The first of these answers is reminiscent of a typical stage in product design. For example, a design committee for a new product may reach out to possible consumers/users to identify what they think about a range of prototypes or to gather feedback about a possible feature. Such processes have what we can refer to as 'instrumental value'. That is, the purpose of including stakeholders or users is directed towards the benefit it brings to the project. Their participation serves an instrumental role in obtaining a goal, such as developing a product that is more likely to sell. A similar claim could also be made for the second answer. However, in this instance, the focus is on mitigating risks or harms, as opposed to realising benefits. Here, the stakeholders or users still play an instrumental role in securing a goal of the project team (e.g., to ensure no harm is caused by their research or project). However, stakeholder engagement has a further intrinsic value, which is more clearly exposed when science and technology are properly situated in a social context. Framed as a third answer that appropriates the language of the above two definitions: Quote Stakeholder participation is a necessary component of responsible design, development, and deployment. It recognises the need for stakeholders and innovators to become mutually responsive to each other in an inclusive and deliberative process that aims at realising sustainable research and innovation practices that promote the social good. This answer helps to expose a limitation of the instrumental perspective. That is, stakeholder engagement is about more than the identification of potential risks and benefits; just as RRI is about more than the avoidance of gross misconduct (e.g., plagiarism, fabrication/falsification of results, developing obviously dangerous technologies). It is also about recognising the right that all members of society have to participate in science and innovation, especially insofar as it relates to how science pursues goals that shape and alter social norms and expectations. And, to be clear, it is a right \u2014one which is captured in Article 27 of the Universal Declaration of Human Rights: Quote Everyone has the right freely to participate in the cultural life of the community, to enjoy the arts and to share in scientific advancement and its benefits.[@un48] RRI relies on both the organic and creative nature of the human mind as much as it does the meticulous standards of the scientific method. Involving diverse stakeholders in creative forms of participation can prevent a sort of cognitive staleness or group-think arising within teams when it comes to problem solving. But more importantly, participation in research and innovation can improve trust, buy-in, and promote scientific understanding and literacy. This goes beyond a narrow instrumental value to a broader social and ethical value. It is also, arguably, part of the motivation behind famous ladder of public engagement.[@arnstein1969] Sherry Arnstein's Ladder of Engagement (Reprinted from Wikimedia Commons\u2014https://commons.wikimedia.org/wiki/File:Ladder_of_citizen_participation,_Sherry_Arnstein.tiff). To summarise, responsible participation goes beyond informing or consultation (i.e., monological forms of engagement). It aims at dialogical engagement that is representative of meaningful partnerships and, hopefully, sustainable forms of devolution of power where members of society are equipped with the capabilities necessary to have legitimate control over the outcomes of scientific research or technological innovation. Our next activity will help us widen our sphere of consideration for ethical reflection and deliberation, in order to support these forms of participation and engagement. See the 'Further Resources' section for links to guides for each of these activities. \u21a9","title":"Science, Technology, and Society"},{"location":"rri/chapter1/sts/#science-technology-and-society","text":"In April 1945, Michael Polanyi\u2014a chemist and sociologist of science\u2014and Bertrand Russell\u2014a philosopher and logician\u2014were speaking on a radio programme about the practical implications of the famous formula, $E = mc^2$. They were asked whether the formula had any practical applications for society, but neither could provide an answer. Three months later the Manhattan project dropped the first of their three atomic bombs! A 3d rendering of an atomic bomb Bridgstock[@bridgstock1998] draws attention to this story because it was used originally by Polanyi, in his essay, 'The Republic of Science' [@polanyi1962], to suggest that the practical and societal outcomes of pure scientific research are often unforeseen and unintended. The problem with this suggestion is that it implies that scientists cannot be held accountable or exercise any real responsibility for the consequences of their research\u2014a troubling implication if true! However, the definitions we have already encountered suggest that science, technology, and society are closely interconnected, and that RRI requires reflection upon the myriad ways that science and technology can impact and shape social norms and practices. Responsibility arises out of this relationship. But if the impacts or consequences are unforeseeable and often unanticipated, then the principles of RRI may be too demanding. Fortunately, the implications of Polanyi's example are narrow in scope. The following thought experiment will help us illustrate why. The Careless CEO Imagine a CEO of a large manufacturing company is approached by one of her scientific advisors and informed that a project that she has proposed will require an environmental impact assessment before it can proceed. The CEO dismisses this and orders that the project continue without the assessment. Furthermore, she callously proclaims that she does not care what the environmental consequences of the project may be. All she is interested in is making as much profit as possible. As it turns out, the project ends up causing vast amounts of pollution that cause irreparable harm to the nearby flora and fauna, and also affects the health of a community living downstream of the manufacturing plant. The CEO is, rightfully, held accountable, both morally and legally, and is prosecuted when her dismissal of the impact assessment is uncovered. Few would take issue with these consequences for the CEO. She had a responsibility to ensure her company's processes operated in a safe and ethical manner, but chose to wilfully neglect this responsibility in spite of receiving advice from her staff. However, let's alter some of the details of this case while keeping the logical structure the same. This time, everything about the thought experiment remains the same except for the outcomes of the project . Now, the project causes no harm. In fact, the efficiency of the new project actually reduces the company's emissions and leads to more sustainable operations. There is no need to worry about accountability in this instance, as no harms occurred. But does the CEO deserve praise for her actions? This question is less likely to have a universal consensus among the answers. Those that believe the CEO deserves no praise are likely to point to the fact that she did not carry out her due diligence or reflect upon the possible consequences of her project. She chose to ignore the suggestion of undertaking an environmental impact assessment, and, therefore, did not act in a responsible manner. She was unable to anticipate any harms or benefits because she did not gather the appropriate evidence. Neither did she act with deliberate intention, but instead acted in a careless manner. In short, she was lucky that the outcomes were positive, and because of a lack of deliberate or intentional action some may argue that there are no legitimate grounds for praise. Others may disagree, and simply argue that the consequences are all that matter. We won't try to settle this debate here. Instead, it can be left for personal reflection. Returning to the issue raised by Polanyi's statement, we can of course acknowledge that he is right to suggest that the societal impacts or consequences of some pure scientific research are hard to anticipate. This is especially true for the more distant effects\u2014consider again the long-term impact of the Human Genome Project, which is arguably still affecting current research. But this is not the case for all research or innovation projects. Many consequences can in fact be more readily anticipated or predicted by carrying out careful activities of reflection and deliberation. There is no question about whether scientists, researchers, and developers have some responsibility for the applications of their research. This must be true for them to be praised for the positive outcomes, which they often are, and also to be held accountable and blamed for the negative consequences when they occur. The question is, rather, when they should receive praise and blame. To help us address this question, we will look at some of the practical ways that scientists, researchers, and developers can take responsibility for the social impacts of their research, in order to maximise the potential opportunities and minimise the possible harms associated with their work. This is a primary objective for the entire course, but we can introduce some of the practical mechanisms now.","title":"Science , Technology, and Society"},{"location":"rri/chapter1/sts/#risk-and-impact-assessments","text":"At the start of this chapter we looked at a definition of RRI from.[@vonschomberg2011] Let's look at another one, this time from the European Commission: === \"European Commission\" Responsible research and innovation is an approach that anticipates and assesses potential implications and societal expectations with regard to research and innovation, with the aim to foster the design of inclusive and sustainable research and innovation.[@europeancommission2014] === \"Ren\u00e9 von Schomberg\" Responsible Research and Innovation is a transparent, interactive process by which societal actors and innovators become mutually responsive to each other with a view on the (ethical) acceptability, sustainability and societal desirability of the innovation process and its marketable products (in order to allow a proper embedding of scientific and technological advances in our society).[@vonschomberg2011] There are many similarities between the two definitions, but they also emphasise different aspects through their choice of terminology. For instance, the definition from the European Commission mentions anticipation and assessment , as a means for highlighting the importance of associated activities such as risk or impact assessments. There are many types of risk and impact assessments that can be carried out, such as safety and risk assessments, equality impact assessments, human rights impact assessments, and, of course, data protection impact assessments. The necessity of such assessments will be familiar to those who work in commercial or public sector organisations, but less so to those in academic institutions. Typically they are carried out for compliance reasons. However, the structured nature of such assessments can also support more ethical forms of reflection and anticipation. It is not necessary to present an overview of all the different impact assessments that could be useful within the context of RRI. 1 Instead, we will focus on a process that is central to almost all forms of risk or impact assessments: stakeholder participation.","title":"Risk and Impact Assessments"},{"location":"rri/chapter1/sts/#inclusive-and-deliberative-stakeholder-participation","text":"Question Why should stakeholders be included in a research or innovation project? There are at least two answers that can be given to this question? To identify and meet stakeholder or user needs To ensure that possible harms that could arise are identified and addressed The first of these answers is reminiscent of a typical stage in product design. For example, a design committee for a new product may reach out to possible consumers/users to identify what they think about a range of prototypes or to gather feedback about a possible feature. Such processes have what we can refer to as 'instrumental value'. That is, the purpose of including stakeholders or users is directed towards the benefit it brings to the project. Their participation serves an instrumental role in obtaining a goal, such as developing a product that is more likely to sell. A similar claim could also be made for the second answer. However, in this instance, the focus is on mitigating risks or harms, as opposed to realising benefits. Here, the stakeholders or users still play an instrumental role in securing a goal of the project team (e.g., to ensure no harm is caused by their research or project). However, stakeholder engagement has a further intrinsic value, which is more clearly exposed when science and technology are properly situated in a social context. Framed as a third answer that appropriates the language of the above two definitions: Quote Stakeholder participation is a necessary component of responsible design, development, and deployment. It recognises the need for stakeholders and innovators to become mutually responsive to each other in an inclusive and deliberative process that aims at realising sustainable research and innovation practices that promote the social good. This answer helps to expose a limitation of the instrumental perspective. That is, stakeholder engagement is about more than the identification of potential risks and benefits; just as RRI is about more than the avoidance of gross misconduct (e.g., plagiarism, fabrication/falsification of results, developing obviously dangerous technologies). It is also about recognising the right that all members of society have to participate in science and innovation, especially insofar as it relates to how science pursues goals that shape and alter social norms and expectations. And, to be clear, it is a right \u2014one which is captured in Article 27 of the Universal Declaration of Human Rights: Quote Everyone has the right freely to participate in the cultural life of the community, to enjoy the arts and to share in scientific advancement and its benefits.[@un48] RRI relies on both the organic and creative nature of the human mind as much as it does the meticulous standards of the scientific method. Involving diverse stakeholders in creative forms of participation can prevent a sort of cognitive staleness or group-think arising within teams when it comes to problem solving. But more importantly, participation in research and innovation can improve trust, buy-in, and promote scientific understanding and literacy. This goes beyond a narrow instrumental value to a broader social and ethical value. It is also, arguably, part of the motivation behind famous ladder of public engagement.[@arnstein1969] Sherry Arnstein's Ladder of Engagement (Reprinted from Wikimedia Commons\u2014https://commons.wikimedia.org/wiki/File:Ladder_of_citizen_participation,_Sherry_Arnstein.tiff). To summarise, responsible participation goes beyond informing or consultation (i.e., monological forms of engagement). It aims at dialogical engagement that is representative of meaningful partnerships and, hopefully, sustainable forms of devolution of power where members of society are equipped with the capabilities necessary to have legitimate control over the outcomes of scientific research or technological innovation. Our next activity will help us widen our sphere of consideration for ethical reflection and deliberation, in order to support these forms of participation and engagement. See the 'Further Resources' section for links to guides for each of these activities. \u21a9","title":"Inclusive and Deliberative Stakeholder Participation"},{"location":"rri/chapter1/timeline/","text":"Science and Technology Studies \u00b6 Optional Section The following timeline for Science and Technology Studies (STS) can be treated as a reference and an optional resource for the RRI course. It is also a work-in-progress, and if you would like to add something to the timeline, please comment on this open issue . Science and technology studies (STS) is the interdisciplinary study of how science and technology shape, structure, affect and interact with society, politics, and cultural knowledge and values. Because of its inherent interdisciplinarity, it arose as the result of the convergence of different ideas and areas of interest, each of which have their own distinct histories. However, many of these intertwined ideas shared important themes, such as (a) viewing science as a social institution with a distinct normative structure (e.g., values of prioritising novel research; ethos and community of peers), (b)) treating science and technology as socially situated practices, and (c) the belief that technology and society co-constitute and mutually shape each other, including the way that scientific facts and technological artefacts are understood and conceptualised.[@merton1973] As Rohracher notes: Quote Facts and artifacts are but temporarily stable outcomes of heterogeneous activities of scientists and engineers and their entanglement in wider social and political relations.[@rohracher2015] Because of these themes, the concerns of STS overlap closely with the concerns of RRI. Therefore, the following potted (and incomplete) history of STS provides a reference to some notable publications and events that you may find interesting to explore. Timeline (In Progress) \u00b6","title":"Science and Technology Studies Timeline"},{"location":"rri/chapter1/timeline/#science-and-technology-studies","text":"Optional Section The following timeline for Science and Technology Studies (STS) can be treated as a reference and an optional resource for the RRI course. It is also a work-in-progress, and if you would like to add something to the timeline, please comment on this open issue . Science and technology studies (STS) is the interdisciplinary study of how science and technology shape, structure, affect and interact with society, politics, and cultural knowledge and values. Because of its inherent interdisciplinarity, it arose as the result of the convergence of different ideas and areas of interest, each of which have their own distinct histories. However, many of these intertwined ideas shared important themes, such as (a) viewing science as a social institution with a distinct normative structure (e.g., values of prioritising novel research; ethos and community of peers), (b)) treating science and technology as socially situated practices, and (c) the belief that technology and society co-constitute and mutually shape each other, including the way that scientific facts and technological artefacts are understood and conceptualised.[@merton1973] As Rohracher notes: Quote Facts and artifacts are but temporarily stable outcomes of heterogeneous activities of scientists and engineers and their entanglement in wider social and political relations.[@rohracher2015] Because of these themes, the concerns of STS overlap closely with the concerns of RRI. Therefore, the following potted (and incomplete) history of STS provides a reference to some notable publications and events that you may find interesting to explore.","title":"Science and Technology Studies"},{"location":"rri/chapter1/timeline/#timeline-in-progress","text":"","title":"Timeline (In Progress)"},{"location":"rri/chapter2/","text":"Responsible Data Science and AI \u00b6 Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk)) Chapter Outline \u00b6 Responsible Data Science and AI Introducing the Project Lifecycle (A Sociotechnical Approach) Roles and Responsibilities Understanding Bias) Chapter Summary This chapter builds on the content of the previous one by applying many of the central concepts to research and innovation in data science and AI. First, we will look at what being responsible means in the context of data science and AI, and explore several principles that can help get us started with operationalising the term 'responsibility'. Next, we explore a simple model that has been designed to help with reflection and deliberation throughout the project lifecycle, and also look at what this model means for individual roles within a project, as well as a broader notion of collective responsibility. Finally, we examine the concept of 'bias', which will play an important role in the subsequent chapters. Learning Objectives In this chapter, you will: Explore what differentiates responsible data science and AI from responsible research and innovation more generally. Examine a model of a typical project lifecycle to better appreciate why individual responsibility is often insufficient in the context of data science and AI. Understand the differences between social, statistical, and cognitive biases, and why they all matter for responsible data science and AI.","title":"Introduction"},{"location":"rri/chapter2/#responsible-data-science-and-ai","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))","title":"Responsible Data Science and AI"},{"location":"rri/chapter2/#chapter-outline","text":"Responsible Data Science and AI Introducing the Project Lifecycle (A Sociotechnical Approach) Roles and Responsibilities Understanding Bias) Chapter Summary This chapter builds on the content of the previous one by applying many of the central concepts to research and innovation in data science and AI. First, we will look at what being responsible means in the context of data science and AI, and explore several principles that can help get us started with operationalising the term 'responsibility'. Next, we explore a simple model that has been designed to help with reflection and deliberation throughout the project lifecycle, and also look at what this model means for individual roles within a project, as well as a broader notion of collective responsibility. Finally, we examine the concept of 'bias', which will play an important role in the subsequent chapters. Learning Objectives In this chapter, you will: Explore what differentiates responsible data science and AI from responsible research and innovation more generally. Examine a model of a typical project lifecycle to better appreciate why individual responsibility is often insufficient in the context of data science and AI. Understand the differences between social, statistical, and cognitive biases, and why they all matter for responsible data science and AI.","title":"Chapter Outline"},{"location":"rri/chapter2/project_lifecycle/","text":"Introducing the Project Lifecycle (A Sociotechnical Approach) \u00b6 There are many ways of carving up the lifecycle for a data science or AI project (hereafter, \u2018project lifecycle\u2019). 1 For instance,[@sweenor2020] break it into four stages: Build, Manage, Deploy & Integrate, Monitor. 2 Ashmore[@ashmore2019] also identify four stages, which have a more specific focus on data science: data management, model learning, model verification, and model deployment. The multiplicity of approaches is likely a product of the evolution of diverse methods in data mining/analytics, the significant impact of ML on research and innovation, and the specific practices and considerations inherent to each of the various domains where ML techniques are applied. While there are many benefits of existing frameworks, they do not tend to focus on the wider social or ethical aspects that interweave throughout the various stages of a ML lifecycle. Project-lifecycle , therefore, presents a model of a typical lifecycle for a project involving data science or the production of an ML/AI system. We have designed this model to support the ethical reflection and deliberation that is characteristic of responsible data science and AI, while remaining faithful to the technical details. However, it is important to note that the model is a heuristic for reflection and deliberation. Therefore, it is not intended to be perfectly capture of describe the processes for all data science or AI projects. The Project Lifecycle. The overarching stages of design, development, and deployment (for a typical data-driven project) can be split into indicative tasks and activities. In practice, both the stages and the tasks will overlap with their neighbours, and may be revisited where a particular task requires an iterative approach. The spiral indicates that this is a diachronic, macroscopic process that evolves and develops over time, and as the deployment stage finishes, a new iteration is likely to begin. To begin, the inner circle breaks the project lifecycle into three processes: (Project) Design (Model) Development (System) Deployment These terms are intended to be maximally inclusive. For example, the design stage encompasses any project task or decision-making process that scaffolds or sets constraints on later project stages (i.e. design system constraints). Importantly, this includes ethical, social, and legal constraints, which we will discuss later. Each of the stages shades into its neighbours, as there is no clear boundary that differentiates certain project design activities (e.g. data extraction and exploratory analysis) from model design activities (e.g. preprocessing and feature engineering, model selection). As such, the design stage overlaps with the development stage, but the latter extends to include the actual process of training, testing, and validating a ML model. Similarly, the process of productionalising a model within a system can be thought of as both a development and deployment activity. And, so, the deployment stage overlaps with the \u2018development\u2019 stage, and also overlaps with the \u2018design\u2019 stage because the deployment of a system should be thought of as an ongoing process (e.g. where new data are used to continuously train the ML model, or, the decision to de-provision a model may require the planning and design of a new model if the older (legacy) system becomes outdated). For these reasons, the project lifecycle is depicted as a spiral. However, despite the unidirectional nature of the arrows, we also acknowledge that ML/AI research and innovation is frequently an iterative process. Therefore, the singular direction is only present at a macroscopic level of abstraction (i.e., the overall direction of progress for a project), and allows for some inevitable back and forth between the stages at the microscopic level. The three higher-level stages can be thought of as a useful heuristic for approaching the project lifecycle. However, each higher-level stage subsumes a wide variety of tasks and activities that are likely to be carried out by different individuals, teams, and organisations, depending on their specific roles and responsibilities (e.g. procurement of data). Therefore, it is important to break each of the three higher-level stages into their (typical) constituent parts, which are likely to vary to some extent between specific projects or within particular organisations. In doing so, we expose a wide range of diverse tasks, each of which give rise to a variety of ethical, social, and legal challenges. Chapter 3 is dedicated to exploring each specific stage and activity in detail. The remainder of this chapter covers some topics that apply to the project lifecycle as a whole. The following text is adapted from a publication titled, 'Ethical Assurance: A practical approach to the responsible design, development, and deployment of data-driven technologies'.[@burr2021] \u21a9 These four stages are influenced by an \u2018ML OPs\u2019 perspective. The term \u2018MLOps\u2019 refers to the application of DevOps practices to ML pipelines. The term is often used in an inclusive manner to incorporate traditional statistical or data science practices that support the ML lifecycle, but are not themselves constitutive of machine learning (e.g. exploratory data analysis), as well as deployment practices that are important within business and operational contexts (e.g. monitoring key performance indicators). \u21a9","title":"Introducing the Project Lifecycle"},{"location":"rri/chapter2/project_lifecycle/#introducing-the-project-lifecycle-a-sociotechnical-approach","text":"There are many ways of carving up the lifecycle for a data science or AI project (hereafter, \u2018project lifecycle\u2019). 1 For instance,[@sweenor2020] break it into four stages: Build, Manage, Deploy & Integrate, Monitor. 2 Ashmore[@ashmore2019] also identify four stages, which have a more specific focus on data science: data management, model learning, model verification, and model deployment. The multiplicity of approaches is likely a product of the evolution of diverse methods in data mining/analytics, the significant impact of ML on research and innovation, and the specific practices and considerations inherent to each of the various domains where ML techniques are applied. While there are many benefits of existing frameworks, they do not tend to focus on the wider social or ethical aspects that interweave throughout the various stages of a ML lifecycle. Project-lifecycle , therefore, presents a model of a typical lifecycle for a project involving data science or the production of an ML/AI system. We have designed this model to support the ethical reflection and deliberation that is characteristic of responsible data science and AI, while remaining faithful to the technical details. However, it is important to note that the model is a heuristic for reflection and deliberation. Therefore, it is not intended to be perfectly capture of describe the processes for all data science or AI projects. The Project Lifecycle. The overarching stages of design, development, and deployment (for a typical data-driven project) can be split into indicative tasks and activities. In practice, both the stages and the tasks will overlap with their neighbours, and may be revisited where a particular task requires an iterative approach. The spiral indicates that this is a diachronic, macroscopic process that evolves and develops over time, and as the deployment stage finishes, a new iteration is likely to begin. To begin, the inner circle breaks the project lifecycle into three processes: (Project) Design (Model) Development (System) Deployment These terms are intended to be maximally inclusive. For example, the design stage encompasses any project task or decision-making process that scaffolds or sets constraints on later project stages (i.e. design system constraints). Importantly, this includes ethical, social, and legal constraints, which we will discuss later. Each of the stages shades into its neighbours, as there is no clear boundary that differentiates certain project design activities (e.g. data extraction and exploratory analysis) from model design activities (e.g. preprocessing and feature engineering, model selection). As such, the design stage overlaps with the development stage, but the latter extends to include the actual process of training, testing, and validating a ML model. Similarly, the process of productionalising a model within a system can be thought of as both a development and deployment activity. And, so, the deployment stage overlaps with the \u2018development\u2019 stage, and also overlaps with the \u2018design\u2019 stage because the deployment of a system should be thought of as an ongoing process (e.g. where new data are used to continuously train the ML model, or, the decision to de-provision a model may require the planning and design of a new model if the older (legacy) system becomes outdated). For these reasons, the project lifecycle is depicted as a spiral. However, despite the unidirectional nature of the arrows, we also acknowledge that ML/AI research and innovation is frequently an iterative process. Therefore, the singular direction is only present at a macroscopic level of abstraction (i.e., the overall direction of progress for a project), and allows for some inevitable back and forth between the stages at the microscopic level. The three higher-level stages can be thought of as a useful heuristic for approaching the project lifecycle. However, each higher-level stage subsumes a wide variety of tasks and activities that are likely to be carried out by different individuals, teams, and organisations, depending on their specific roles and responsibilities (e.g. procurement of data). Therefore, it is important to break each of the three higher-level stages into their (typical) constituent parts, which are likely to vary to some extent between specific projects or within particular organisations. In doing so, we expose a wide range of diverse tasks, each of which give rise to a variety of ethical, social, and legal challenges. Chapter 3 is dedicated to exploring each specific stage and activity in detail. The remainder of this chapter covers some topics that apply to the project lifecycle as a whole. The following text is adapted from a publication titled, 'Ethical Assurance: A practical approach to the responsible design, development, and deployment of data-driven technologies'.[@burr2021] \u21a9 These four stages are influenced by an \u2018ML OPs\u2019 perspective. The term \u2018MLOps\u2019 refers to the application of DevOps practices to ML pipelines. The term is often used in an inclusive manner to incorporate traditional statistical or data science practices that support the ML lifecycle, but are not themselves constitutive of machine learning (e.g. exploratory data analysis), as well as deployment practices that are important within business and operational contexts (e.g. monitoring key performance indicators). \u21a9","title":"Introducing the Project Lifecycle (A Sociotechnical Approach)"},{"location":"rri/chapter2/responsible_ds/","text":"Responsible Data Science and AI \u00b6 Quote What separates responsible data science and AI from responsible research and innovation more generally? We saw in the previous chapter how RRI can be defined with reference to concepts that emphasise the need for ethical reflection on possible social harms and benefits, supported by inclusive participation of affected stakeholders. Responsible data science and AI shares this emphasis, but can be further refined by considering more specific principles that are geared towards the particular harms and benefits associated with data science and AI. These principles can help us identify what is unique to responsible data science and AI. SAFE-D Principles \u00b6 According to Mittelstadt[@mittelstadt2019], in 2019 there were at least 84 statements that provided ''high-level principles, values and other tenets to guide the ethical development, deployment and governance of AI''. By now there are surely many, many more! In response to this proliferation of principles, some have attempted to distil and condense the myriad documents, in order to identify commonalities and extract a unified list of shared principles.[@jobin2019] - [@floridi2019] However, regardless of which set of principles we start with, one thing remains the same: good principles should support ongoing reflection and deliberation; they are not decision procedures in their own right. This point is sometimes lost in the ensuing debate about which set of principles should be used or adhered to, or which set is best. However, what matters is that the set of principles should a) be responsive to the actual harms and benefits that matter to the communities of affected individuals, b) be underwritten by a set of shared values, which support and motivate dialogue between stakeholders 1 , and c) serve as starting points in a wider process of reflection and deliberation. With these points in mind, we will make use of the following set of principles known as the 'SAFE-D principles': Sustainability Accountability Fairness Explainability Data Quality, Integrity, Protection and Privacy These principles are grounded in comprehensive research and understanding of human rights and data protection law, as well as applied ethics of data and AI.[@leslie2021a] You can click through the following illustrative examples to get an idea of some of the social harms associated with data-driven technologies: === \"Predicting Risk\" Avon and Somerset Police and Bristol City Council developed a sophisticated predictive risk tool that was used, among other things, to predict the risk of children suffering sexual abuse. But, the Bristol Cable reported that many children were falsely flagged as being at risk, and that the tool was developed using dozens of public sector databases, including schools, housing, NHS records, and even credit scores from Experian. [The Bristol Cable, 'How a police and council database is predicting if your child is at risk of harm'](https://thebristolcable.org/2021/07/how-a-police-and-council-database-is-predicting-if-your-child-is-at-risk-of-harm/) === \"Facebook Discriminatory Job Adverts\" The algorithmic system used by Facebook to automatically show job adverts to users it believes are most likely to engage with them was reported to perpetuate discriminatory gender norms. Thee BBC reported that !!! quote almost all Facebook users shown adverts for mechanics were men, while ads for nursery nurses were seen almost exclusively by women. [BBC News, 'Facebook accused of allowing sexist job advertising'](https://www.bbc.co.uk/news/technology-58487026) === \"Racist Photo Cropping Tool\" Twitter was forced to apologise after many users reported that the automated tool for cropping images on the social media platform showed a racial bias towards faces of white people over faces of black people. According to [Twitter](https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm), one source of the issue was the use of a ''saliency algorithm'' that was trained on human eye-tracking data. [The Guardian, 'Twitter apologises for 'racist' image-cropping algorithm'](https://www.theguardian.com/technology/2020/sep/21/twitter-apologises-for-racist-image-cropping-algorithm) === \"Lethal Autonomous Weapons\" Turkish company STM manufactures the [Kargu-2](https://www.stm.com.tr/uploads/docs/1628858259_tacticalminiuavsystems.pdf?)\u2014an attack drone that can operate autonomously by using machine learning and real-time image processing to identify targets. According to a UN security council report this drone was reported to have been used to \"remotely engage\" and \"hunt down\" logistics convoys and retreating forces in the Libyan civil war during 2019. [NPR, 'A Military Drone With A Mind Of Its Own Was Used In Combat, U.N. Says'](https://www.npr.org/2021/06/01/1002196245/a-u-n-report-suggests-libya-saw-the-first-battlefield-killing-by-an-autonomous-d) === \"#TravelingWhileTrans\" In their book, Design Justice, Sasha Costanza-Chock highlights how the design of sociotechnical systems reinforce and embed a variety of social norms and expectations that can be harmful to vulnerable or marginalised communities. For example, the impact of full-body scanners at airport security that require operators to select either 'male' or 'female', even when presented with non-binary or trans individuals whose bodies may not conform to the models embedded within the machine. [Design Justice, 'Introduction: #TravelingWhileTrans, Design Justice, and Escape from the Matrix of Domination'](https://design-justice.pubpub.org/pub/ap8rgw5e/release/1) === \"Any Others?\" Do you know any other examples of social harms associated with data-driven technologies? Each of the SAFE-D principles is either motivated by and captures a specific set of harms that have been uncovered and exposed, responds to a set or well-documented risks that arise in the context of data science and AI, or is oriented towards the sustainable, ethical, and responsible use of data-driven technologies. Let's look at each principle in turn. Sustainability \u00b6 Sustainability can mean a couple of things. From a technical perspective, sustainability requires the outputs of a project to be safe, secure, robust, and reliable. For example, if an organisation is developing an autonomous vehicle, it should operate safely in the intended context of use. However, in the context of responsible data science and AI, there is also a social sustainability component. This aspect of sustainability requires a project's practices to be informed by ongoing consideration of the risk of exposing individuals to harms even after the system has been deployed and the project completed\u2014a long-term (or sustainable) safety. Accountability \u00b6 Accountability can refer to transparency of processes and associated outcomes that enable people to understand how a project was conducted (e.g., project documentation), or why a specific decision was reached. But it can also refer to broader processes of responsible project governance that seek to establish clear roles of responsibility where full transparency may be inappropriate (e.g., confidential projects). Fairness \u00b6 Fairness is inseparably connected with legal conceptions of equality and justice, which may emphasize a variety of features such as non-discrimination, equitable outcomes, or procedural fairness through bias mitigation. However, these notions serve as a subset of broader normative considerations pertaining to social justice, socioeconomic capabilities, diversity and inclusivity. For this reason, the term 'fairness' can be confusing due to the wide variety of ways it is employed, and the large number of more specific concepts that fall within its scope. Explainability \u00b6 Explainability is a key condition for autonomous and informed decision-making in situations where data-driven systems interact with or influence human judgement and choice behaviour. Explainability goes beyond the ability to merely interpret specific aspects of a project (e.g., interpreting the parameters of a model); it also depends on the ability to provide an accessible and relevant information base about the processes behind the outcome. Data Quality, Integrity, Protection and Privacy \u00b6 Data quality, integrity, protection and privacy must all be established to be confident that a research or innovation project has been designed, developed, and deployed in a responsible manner. 'Data Quality' captures the static properties of data, such as whether they are a) relevant to and representative of the domain and use context, b) balanced and complete in terms of how well the dataset represents the underlying data generating process, and c) up-to-date and accurate as required by the project. \u2018Data Integrity' refers to more dynamic properties of data stewardship, such as how a dataset evolves over the course of a project lifecycle. In this manner, data integrity requires a) contemporaneous and attributable records from the start of a project (e.g., process logs; research statements), b) ensuring consistent and verifiable means of data analysis or processing during development, and c) taking steps to establish findable , accessible , interoperable , and reusable records towards the end of a project\u2019s lifecycle. 2 \u2018Data protection and privacy' reflect ongoing developments and priorities as set out in relevant legislation and regulation of data practices as they pertain to fundamental rights and freedoms, democracy, and the rule of law. For example, the right for data subjects to have inaccurate personal data rectified or erased.[@ico2021] Each of these principles can be treated as a goal to which responsible data science and AI ought to be directed. 3 But, on their own they are insufficient for establishing what specific actions or decisions should be taken in any given project. For instance, what does it mean to develop a fair diagnostic model in healthcare? Does it mean ensuring that all patients are exposed to the same level of risk with respect to the distribution of possible false negatives? What about false positives instead? What about the use of the decision support system in which thee model is implemented? Will it be used in all hospitals on all patients? Or, will only those wealthy enough to afford private healthcare receive this service? Should it instead be used for the most vulnerable and worse off in society? Questions such as these have no straightforward answer and are heavily context-dependant. Even if consensus were to be reached for a specific model used, say, in the diagnosis of lung cancer,[@svoboda2020] this would be no guarantee of a similar answer in a different area of healthcare (e.g., paediatrics, mental healthcare), or even for another diagnostic model in radiology (e.g., MRI instead of CT scans). Therefore, starting in the next section we will look at a model for helping us get a clear grasp of the situated and sociotechnical context under consideration in research and innovation projects. We won't say much about ethical values in this course. However, the course on AI Ethics & Governance focuses on them directly. \u21a9 These are known as the FAIR principles ( read more here ). \u21a9 In our guidebook on public engagement of data science and AI we formalise this notion of an ethical goal in relation to a method known as argument-based assurance. Here, the goals are supported by specific properties that must be established in a project, in order to provide justifiable assurance to stakeholders that the respective goal has been realised. \u21a9","title":"What is Responsible Data Science and AI"},{"location":"rri/chapter2/responsible_ds/#responsible-data-science-and-ai","text":"Quote What separates responsible data science and AI from responsible research and innovation more generally? We saw in the previous chapter how RRI can be defined with reference to concepts that emphasise the need for ethical reflection on possible social harms and benefits, supported by inclusive participation of affected stakeholders. Responsible data science and AI shares this emphasis, but can be further refined by considering more specific principles that are geared towards the particular harms and benefits associated with data science and AI. These principles can help us identify what is unique to responsible data science and AI.","title":"Responsible Data Science and AI"},{"location":"rri/chapter2/responsible_ds/#safe-d-principles","text":"According to Mittelstadt[@mittelstadt2019], in 2019 there were at least 84 statements that provided ''high-level principles, values and other tenets to guide the ethical development, deployment and governance of AI''. By now there are surely many, many more! In response to this proliferation of principles, some have attempted to distil and condense the myriad documents, in order to identify commonalities and extract a unified list of shared principles.[@jobin2019] - [@floridi2019] However, regardless of which set of principles we start with, one thing remains the same: good principles should support ongoing reflection and deliberation; they are not decision procedures in their own right. This point is sometimes lost in the ensuing debate about which set of principles should be used or adhered to, or which set is best. However, what matters is that the set of principles should a) be responsive to the actual harms and benefits that matter to the communities of affected individuals, b) be underwritten by a set of shared values, which support and motivate dialogue between stakeholders 1 , and c) serve as starting points in a wider process of reflection and deliberation. With these points in mind, we will make use of the following set of principles known as the 'SAFE-D principles': Sustainability Accountability Fairness Explainability Data Quality, Integrity, Protection and Privacy These principles are grounded in comprehensive research and understanding of human rights and data protection law, as well as applied ethics of data and AI.[@leslie2021a] You can click through the following illustrative examples to get an idea of some of the social harms associated with data-driven technologies: === \"Predicting Risk\" Avon and Somerset Police and Bristol City Council developed a sophisticated predictive risk tool that was used, among other things, to predict the risk of children suffering sexual abuse. But, the Bristol Cable reported that many children were falsely flagged as being at risk, and that the tool was developed using dozens of public sector databases, including schools, housing, NHS records, and even credit scores from Experian. [The Bristol Cable, 'How a police and council database is predicting if your child is at risk of harm'](https://thebristolcable.org/2021/07/how-a-police-and-council-database-is-predicting-if-your-child-is-at-risk-of-harm/) === \"Facebook Discriminatory Job Adverts\" The algorithmic system used by Facebook to automatically show job adverts to users it believes are most likely to engage with them was reported to perpetuate discriminatory gender norms. Thee BBC reported that !!! quote almost all Facebook users shown adverts for mechanics were men, while ads for nursery nurses were seen almost exclusively by women. [BBC News, 'Facebook accused of allowing sexist job advertising'](https://www.bbc.co.uk/news/technology-58487026) === \"Racist Photo Cropping Tool\" Twitter was forced to apologise after many users reported that the automated tool for cropping images on the social media platform showed a racial bias towards faces of white people over faces of black people. According to [Twitter](https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm), one source of the issue was the use of a ''saliency algorithm'' that was trained on human eye-tracking data. [The Guardian, 'Twitter apologises for 'racist' image-cropping algorithm'](https://www.theguardian.com/technology/2020/sep/21/twitter-apologises-for-racist-image-cropping-algorithm) === \"Lethal Autonomous Weapons\" Turkish company STM manufactures the [Kargu-2](https://www.stm.com.tr/uploads/docs/1628858259_tacticalminiuavsystems.pdf?)\u2014an attack drone that can operate autonomously by using machine learning and real-time image processing to identify targets. According to a UN security council report this drone was reported to have been used to \"remotely engage\" and \"hunt down\" logistics convoys and retreating forces in the Libyan civil war during 2019. [NPR, 'A Military Drone With A Mind Of Its Own Was Used In Combat, U.N. Says'](https://www.npr.org/2021/06/01/1002196245/a-u-n-report-suggests-libya-saw-the-first-battlefield-killing-by-an-autonomous-d) === \"#TravelingWhileTrans\" In their book, Design Justice, Sasha Costanza-Chock highlights how the design of sociotechnical systems reinforce and embed a variety of social norms and expectations that can be harmful to vulnerable or marginalised communities. For example, the impact of full-body scanners at airport security that require operators to select either 'male' or 'female', even when presented with non-binary or trans individuals whose bodies may not conform to the models embedded within the machine. [Design Justice, 'Introduction: #TravelingWhileTrans, Design Justice, and Escape from the Matrix of Domination'](https://design-justice.pubpub.org/pub/ap8rgw5e/release/1) === \"Any Others?\" Do you know any other examples of social harms associated with data-driven technologies? Each of the SAFE-D principles is either motivated by and captures a specific set of harms that have been uncovered and exposed, responds to a set or well-documented risks that arise in the context of data science and AI, or is oriented towards the sustainable, ethical, and responsible use of data-driven technologies. Let's look at each principle in turn.","title":"SAFE-D Principles"},{"location":"rri/chapter2/responsible_ds/#sustainability","text":"Sustainability can mean a couple of things. From a technical perspective, sustainability requires the outputs of a project to be safe, secure, robust, and reliable. For example, if an organisation is developing an autonomous vehicle, it should operate safely in the intended context of use. However, in the context of responsible data science and AI, there is also a social sustainability component. This aspect of sustainability requires a project's practices to be informed by ongoing consideration of the risk of exposing individuals to harms even after the system has been deployed and the project completed\u2014a long-term (or sustainable) safety.","title":"Sustainability"},{"location":"rri/chapter2/responsible_ds/#accountability","text":"Accountability can refer to transparency of processes and associated outcomes that enable people to understand how a project was conducted (e.g., project documentation), or why a specific decision was reached. But it can also refer to broader processes of responsible project governance that seek to establish clear roles of responsibility where full transparency may be inappropriate (e.g., confidential projects).","title":"Accountability"},{"location":"rri/chapter2/responsible_ds/#fairness","text":"Fairness is inseparably connected with legal conceptions of equality and justice, which may emphasize a variety of features such as non-discrimination, equitable outcomes, or procedural fairness through bias mitigation. However, these notions serve as a subset of broader normative considerations pertaining to social justice, socioeconomic capabilities, diversity and inclusivity. For this reason, the term 'fairness' can be confusing due to the wide variety of ways it is employed, and the large number of more specific concepts that fall within its scope.","title":"Fairness"},{"location":"rri/chapter2/responsible_ds/#explainability","text":"Explainability is a key condition for autonomous and informed decision-making in situations where data-driven systems interact with or influence human judgement and choice behaviour. Explainability goes beyond the ability to merely interpret specific aspects of a project (e.g., interpreting the parameters of a model); it also depends on the ability to provide an accessible and relevant information base about the processes behind the outcome.","title":"Explainability"},{"location":"rri/chapter2/responsible_ds/#data-quality-integrity-protection-and-privacy","text":"Data quality, integrity, protection and privacy must all be established to be confident that a research or innovation project has been designed, developed, and deployed in a responsible manner. 'Data Quality' captures the static properties of data, such as whether they are a) relevant to and representative of the domain and use context, b) balanced and complete in terms of how well the dataset represents the underlying data generating process, and c) up-to-date and accurate as required by the project. \u2018Data Integrity' refers to more dynamic properties of data stewardship, such as how a dataset evolves over the course of a project lifecycle. In this manner, data integrity requires a) contemporaneous and attributable records from the start of a project (e.g., process logs; research statements), b) ensuring consistent and verifiable means of data analysis or processing during development, and c) taking steps to establish findable , accessible , interoperable , and reusable records towards the end of a project\u2019s lifecycle. 2 \u2018Data protection and privacy' reflect ongoing developments and priorities as set out in relevant legislation and regulation of data practices as they pertain to fundamental rights and freedoms, democracy, and the rule of law. For example, the right for data subjects to have inaccurate personal data rectified or erased.[@ico2021] Each of these principles can be treated as a goal to which responsible data science and AI ought to be directed. 3 But, on their own they are insufficient for establishing what specific actions or decisions should be taken in any given project. For instance, what does it mean to develop a fair diagnostic model in healthcare? Does it mean ensuring that all patients are exposed to the same level of risk with respect to the distribution of possible false negatives? What about false positives instead? What about the use of the decision support system in which thee model is implemented? Will it be used in all hospitals on all patients? Or, will only those wealthy enough to afford private healthcare receive this service? Should it instead be used for the most vulnerable and worse off in society? Questions such as these have no straightforward answer and are heavily context-dependant. Even if consensus were to be reached for a specific model used, say, in the diagnosis of lung cancer,[@svoboda2020] this would be no guarantee of a similar answer in a different area of healthcare (e.g., paediatrics, mental healthcare), or even for another diagnostic model in radiology (e.g., MRI instead of CT scans). Therefore, starting in the next section we will look at a model for helping us get a clear grasp of the situated and sociotechnical context under consideration in research and innovation projects. We won't say much about ethical values in this course. However, the course on AI Ethics & Governance focuses on them directly. \u21a9 These are known as the FAIR principles ( read more here ). \u21a9 In our guidebook on public engagement of data science and AI we formalise this notion of an ethical goal in relation to a method known as argument-based assurance. Here, the goals are supported by specific properties that must be established in a project, in order to provide justifiable assurance to stakeholders that the respective goal has been realised. \u21a9","title":"Data Quality, Integrity, Protection and Privacy"},{"location":"rri/chapter2/roles_responsibility/","text":"Roles and Responsibilities \u00b6 One thing should be clear from the wide variety of activities associated with the project lifecycle... designing, developing, and deploying an AI system is not a one-person task! The activities and processes which comprise a typical AI project lifecycle involve a wide-ranging and far-reaching set of skills and capacities. Such skills are usually encapsulated within myriad roles and responsibilities: project commissioner, product manager, data protection officer, data scientist, system engineer, etc. However, the way such roles and responsibilities are often defined at an organisational or institutional level (e.g. in job specifications) tend to reflect the practical demands of organisational efficiency or HR management, rather than the normative demands of ethical and responsible governance. In reality, the individual roles and responsibilities, which are implicated in the design, development, and deployment of complex AI projects, are interwoven to such an extent that they form an inextricable Gordian Knot of collective responsibility. While there will always remain a pragmatic need to have individual roles and responsibilities for complex projects, which may be undertaken by a single person or a team, due to the inescapable burden of time constraints and finite cognitive resources, such a pragmatic consideration does not provide the individual project member with a morally defensible reason for excusing themselves from the shared and collective responsibility. What is needed, however, is a clear illustration of how this collective responsibility is instantiated throughout the lifecycle of an AI project\u2014or a project involving the development of a data-driven technology. The following offers a stylised illustration of the project lifecycle, introduced in the previous section , that we can build on. A key benefit of this illustration is that is helps us reflect on the way that individual roles and responsibilities within the project team interconnect. For instance, the developers or software engineers, in virtue of their expertise in implementing models and designing software tools may have a better understanding of how UX/UI elements will affect the decision-making of the end user, perhaps in virtue of their impact on cognitive biases.[@burton2020] As such, they may recognise the need to ensure that any output of an automated decision support system should be accompanied by a graphical representation that helps explain how an algorithm reached any particular decision. Therefore, reducing individual project members to their functional role devalues their creative and reflective potential. Taking our developer as an example, again, it is worth noting that bringing them more upstream into, say, the project design discussions with a product manager and stakeholders may allow them to hear things that the product manager would not otherwise know to ask them or tell them. {numref} pl-scriberia , therefore, serves as a starting point for reflecting on how a project team share a collective interest in realising a shared goal (i.e. the responsible design, development, and deployment of a data-driven technology), which requires all team members to reflect on how their own roles and responsibilities intersect or impact with the various stages of the project lifecycle beyond those that fall within their immediate remit of, say, a functional job description. The Dynamics of Project Teams \u00b6 There are, of course, a wide variety of ways that a project team can be established, and projects can come in all shapes and sizes. Some projects may be distributed across countries or continents. Some may be based within a single department in a university or research institute. Others may operate across multiple organisations, requiring complex management structures to ensure the project is governed effectively. Others may rely on the voluntary contribution of passionate individuals, who work as a decentralised cooperative, as is common in open source software. Each structure comes with its own benefits and drawbacks. A common drawback though is the existence of power imbalances that can lead to internal rifts or complications within a project team. For example, it is common within academic research projects for their to be a lead or principal investigator who is in charge of the project. Often, this individual is well-established in their academic profession, and (to some extent) protected from the precarious nature of academic employment. Although they will have a wide-range of faculty responsibilities to juggle, the security afforded by more senior positions, and the prestige and influence associated with them, creates many opportunities. This is not the case, however, for more junior members of the project, such as PhD students or research assistants. For these individuals, a core priority may be ensuring they obtain a key publication in a top-tier journal, prior to graduating and exploring post-doctoral positions or teaching roles within a university. Like their senior counterparts, these individuals have their own priorities and individual responsibilities to consider. Unfortunately, the resulting picture is not always a harmonious one, and when priorities clash the power imbalance often favours the more senior individual. A similar picture emerges in technology companies, as is illustrated nicely by Tanya Reilly in her excellent blog article, 'Being Glue' . Rather than summarising the article, I'd encourage you to set aside the time to read it. There are, of course, many more ways that project teams can lead to power imbalances or challenging dynamics. Varying levels of public and private funding between countries can create systematic forms of advantage for research teams at more prestigious institutions or citizens of wealthier countries. Language barriers (or disciplinary assumptions) can impede transparent and effective communication, both within and between research teams, and also between innovators and the public. Physical and Mental Disabilities, and socioeconomic disparities (e.g., educational background), contribute to an uneven (and often unjust) playing field. As do more general social or organisational norms or expectations, such as those associated with neurotypicality. All of these considerations, and more, shape the dynamics of project teams. It is important, therefore, to take the time to reflect on our own position and standing within our teams, and to identify if there is anything we can do to support each other. The more that individual's take this personal responsibility, the easier it becomes to share in a collective responsibility.","title":"Roles and Responsibilities"},{"location":"rri/chapter2/roles_responsibility/#roles-and-responsibilities","text":"One thing should be clear from the wide variety of activities associated with the project lifecycle... designing, developing, and deploying an AI system is not a one-person task! The activities and processes which comprise a typical AI project lifecycle involve a wide-ranging and far-reaching set of skills and capacities. Such skills are usually encapsulated within myriad roles and responsibilities: project commissioner, product manager, data protection officer, data scientist, system engineer, etc. However, the way such roles and responsibilities are often defined at an organisational or institutional level (e.g. in job specifications) tend to reflect the practical demands of organisational efficiency or HR management, rather than the normative demands of ethical and responsible governance. In reality, the individual roles and responsibilities, which are implicated in the design, development, and deployment of complex AI projects, are interwoven to such an extent that they form an inextricable Gordian Knot of collective responsibility. While there will always remain a pragmatic need to have individual roles and responsibilities for complex projects, which may be undertaken by a single person or a team, due to the inescapable burden of time constraints and finite cognitive resources, such a pragmatic consideration does not provide the individual project member with a morally defensible reason for excusing themselves from the shared and collective responsibility. What is needed, however, is a clear illustration of how this collective responsibility is instantiated throughout the lifecycle of an AI project\u2014or a project involving the development of a data-driven technology. The following offers a stylised illustration of the project lifecycle, introduced in the previous section , that we can build on. A key benefit of this illustration is that is helps us reflect on the way that individual roles and responsibilities within the project team interconnect. For instance, the developers or software engineers, in virtue of their expertise in implementing models and designing software tools may have a better understanding of how UX/UI elements will affect the decision-making of the end user, perhaps in virtue of their impact on cognitive biases.[@burton2020] As such, they may recognise the need to ensure that any output of an automated decision support system should be accompanied by a graphical representation that helps explain how an algorithm reached any particular decision. Therefore, reducing individual project members to their functional role devalues their creative and reflective potential. Taking our developer as an example, again, it is worth noting that bringing them more upstream into, say, the project design discussions with a product manager and stakeholders may allow them to hear things that the product manager would not otherwise know to ask them or tell them. {numref} pl-scriberia , therefore, serves as a starting point for reflecting on how a project team share a collective interest in realising a shared goal (i.e. the responsible design, development, and deployment of a data-driven technology), which requires all team members to reflect on how their own roles and responsibilities intersect or impact with the various stages of the project lifecycle beyond those that fall within their immediate remit of, say, a functional job description.","title":"Roles and Responsibilities"},{"location":"rri/chapter2/roles_responsibility/#the-dynamics-of-project-teams","text":"There are, of course, a wide variety of ways that a project team can be established, and projects can come in all shapes and sizes. Some projects may be distributed across countries or continents. Some may be based within a single department in a university or research institute. Others may operate across multiple organisations, requiring complex management structures to ensure the project is governed effectively. Others may rely on the voluntary contribution of passionate individuals, who work as a decentralised cooperative, as is common in open source software. Each structure comes with its own benefits and drawbacks. A common drawback though is the existence of power imbalances that can lead to internal rifts or complications within a project team. For example, it is common within academic research projects for their to be a lead or principal investigator who is in charge of the project. Often, this individual is well-established in their academic profession, and (to some extent) protected from the precarious nature of academic employment. Although they will have a wide-range of faculty responsibilities to juggle, the security afforded by more senior positions, and the prestige and influence associated with them, creates many opportunities. This is not the case, however, for more junior members of the project, such as PhD students or research assistants. For these individuals, a core priority may be ensuring they obtain a key publication in a top-tier journal, prior to graduating and exploring post-doctoral positions or teaching roles within a university. Like their senior counterparts, these individuals have their own priorities and individual responsibilities to consider. Unfortunately, the resulting picture is not always a harmonious one, and when priorities clash the power imbalance often favours the more senior individual. A similar picture emerges in technology companies, as is illustrated nicely by Tanya Reilly in her excellent blog article, 'Being Glue' . Rather than summarising the article, I'd encourage you to set aside the time to read it. There are, of course, many more ways that project teams can lead to power imbalances or challenging dynamics. Varying levels of public and private funding between countries can create systematic forms of advantage for research teams at more prestigious institutions or citizens of wealthier countries. Language barriers (or disciplinary assumptions) can impede transparent and effective communication, both within and between research teams, and also between innovators and the public. Physical and Mental Disabilities, and socioeconomic disparities (e.g., educational background), contribute to an uneven (and often unjust) playing field. As do more general social or organisational norms or expectations, such as those associated with neurotypicality. All of these considerations, and more, shape the dynamics of project teams. It is important, therefore, to take the time to reflect on our own position and standing within our teams, and to identify if there is anything we can do to support each other. The more that individual's take this personal responsibility, the easier it becomes to share in a collective responsibility.","title":"The Dynamics of Project Teams"},{"location":"rri/chapter2/understanding_bias/","text":"Understanding Bias \u00b6 An artistic representation of social, statistical, and cognitive biases Before we start exploring the project lifecycle and its associated activities there is a final topic that we need to explore. You will likely have some prior familiarity of the term 'bias', but your understanding of the concept may emphasise specific properties that reflect a specific focus of your research background. This section will present three common ways that 'bias' can be understood as it applies to and affects the research and innovation lifecycle. The three perspectives that we will look at are: social, statistical, and cognitive bias. Social Bias \u00b6 Outside of research and development communities, the term 'bias' is often associated with some form of prejudice or discrimination. For example, an inclination or disposition to treat an individual or organisation in a way that is considered to be unfair. This understanding of bias is necessary to draw attention to pre-existing or historical patterns of discrimination and social injustice that can be perpetuated, reinforced, or exacerbated through the development and deployment of data-driven technologies. There are numerous examples that illustrate this point. === \"Amazon's recruitment tool\" This [algorithmic system]((https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)) learned to perpetuate a bias to prefer male candidates to female candidates because this reflected past hiring decisions. === \"Predictive policing\" Predictive policing techniques are [increasingly being developed and deployed](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/), making use of geospatial data to try to learn associations between places, events, and historical crime rates. The attempt to predict where and when crimes are more likely to happen can create a positive feedback loop, which results in over-policing that may exacerbate tensions between communities and police. === \"Clinical decision support systems\" Clinical decision support systems can contribute to existing forms of [racial bias in access to healthcare](https://www.nature.com/articles/d41586-019-03228-6). A study conducted in the US found that an algorithm that used health costs as a proxy for health needs was ''less likely to refer black people than white people who were equally sick to programmes that aim to improve care for patients with complex medical needs''.[@obermeyer2019] A commonly heard response to such examples is the claim that the underlying problem is that the training data used to develop the algorithms or models were insufficiently representative. In other words,'' it was the data that were biased ''. The assumption behind this claim is that better data collection would solve the problem. Unfortunately, at best this response is only partially true, but at worst it belies a commitment to a form of 'technological solutionism' 1 that often ignores how technology affects social practices and norms. It is important to remember that most decisions that drive the project lifecycle are made by the project team. A choice to design a study in a particular way, or to deploy a system in a context that is characterised by patterns of historical discrimination, cannot simply be blamed on poor data. Statistical Bias \u00b6 If you are a data scientist, or use techniques from data science in your research or development, then it is likely that your understanding of bias is influenced by statistical concepts. Jeff Aronson explores the etymology of the term 'bias' in a series of interesting blog articles, which emphasise the statistical understanding of bias.[@aronson2018] He begins by tracing it back to the game of bowls, where the curved trajectory of the bowl as it ran along the green reflected the asymmetric shape of the bowl (i.e., its bias). However, according to Aronson, the term was not used in statistics until around the start of the 20th Century where it was used to refer to a systematic deviation from an expected statistical result that arises due to the influence of some additional factor. This understanding is common in observational studies where bias can arise in the process of sampling or measurement. On the basis of his historical review, Aronson identifies six features of definitions of bias that adopt a statistical perspective: Systematicity: bias arises from a systematic process, rather than a random or chance process. Truth: a realist assumption that the deviation is from a true state of the world Error: the bias reflects an error, perhaps due to sampling or measurement Deviation (or Distortion): a quantity in which the observed result is taken to differ from the actual result were there no bias. Affected elements: the study elements that may be affected by the bias include the conception, design, and conduct of the study, as well as the collection, analysis, interpretation, and representation of the data Direction: the deviation is directional, as it can be caused by both an under- or over-estimation Some of these features are specific to a statistical framing of 'bias', but some also apply to the other two perspectives For instance, 'systematicity' is arguably a necessary property for social biases (i.e., a bias that systematically leads to discriminatory outcomes). And, 'error' is sometimes a property of our next perspective: cognitive biases. Cognitive Bias \u00b6 Our modern understanding of cognitive biases has been most heavily influenced by research conducted by Daniel Kahneman and Amos Tversky. A lot of their work exposed a wide variety of psychological vulnerabilities, which impact our judgement and decision-making capabilities. In short, their experiments showed how individuals rely on an assortment of heuristics or biases, which speed up judgement and decision-making but also lead us astray. For example, consider the following example. The Linda Problem Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? 1) Linda is a bank teller. 2) 2) Linda is a bank teller and is active in the feminist movement. Try to answer this question yourself, before you reveal the answer. ??? note \"Reveal\" The correct answer is (1). Did you get it right? If you got it wrong, you have just committed what is known as the 'conjunction fallacy'. But don't worry you're in very good company! When Tvserky and Kahneman posed this question to a group of 88 undergraduate students, only $15%$ got the correct answer {cite}`tversky1983`. The reason it is (1) is because the probability of two events occurring *in conjunction*, such as Linda being both a 'bank teller' and 'active in the feminist movement' must be less than or equal to the probability of either event occurring on its own. Formally, for two events $A$ and $B$: $Pr(A \\wedge B) \u2264 Pr(A)$ and $Pr(A \\wedge B) \u2264 Pr(B)$ Or, to put it more simply, someone cannot belong to the set of $feminist bank tellers$ without also belonging to the set of $bank tellers$ \ud83d\udc47 ![venn diagram for linda problem](../../assets/images/graphics/venn.png) Tversky and Kahneman attributed this systematic error to what is known as the *representativeness heuristic*. In short, people don't think about the conjunction of events or consider probability theory when formulating an answer. Instead, their choice is based on which of the two options is *most representative* of the description of Linda. That is, they employ a mental shortcut (or, a heuristic) that in some instances lead to the right answer\u2014hence, their efficiency. However, in in other cases their use lead to mistakes or errors in judgement. A critical perspective on the view of judgement and decision-making put forward by Kahneman and Tversky would view it as an attempt to catalogue a variety of cognitive failures or irrationalities that stem from an individual\u2019s inability to perform rational calculations. However, those who adopt a view known as 'ecological rationality' argue that such a perspective judges human agents against a normative standard of rationality that is unsuitable for situated agents whose choice behaviour is constrained by myriad cognitive and environmental factors (e.g. temporal constraints that force decisions, limited information). This alternative account, made famous by Herbert Simon, and later developed by Gerd Gigerenzer reframes a lot of human judgement and decision-making as underpinned by \u201cfast and frugal\u201d heuristics, which are highly adaptive and support decision-making in complex and uncertain environments. It's not necessary to delve into this debate for the present purposes, but it is an interesting tangent for those interested in exploring how the choice to present statistical information in different ways (e.g., as probabilities versus frequencies) can affect comprehension and understanding. 2 When carrying out research and innovation in data science and AI, cognitive biases can impact the processes and outcome of the project lifecycle in myriad ways. There is, after all, a large list of cognitive biases to consider. No one is expected to memorise this list as a prerequisite for responsible action. However, there are some key cognitive biases that it can be helpful to consider. The next activity will explore a small handful of these biases, but we will look at others in more detail when we starting exploring the different stages of the project lifecycle in the next chapter. This way, we can anchor our understanding of biases\u2014social, statistical, and cognitive\u2014in the parts of the project lifecycle where we can hopefully mitigate their (potentially) negative impact. The term 'technological solutionism' is often used to refer to the belief (or, \"faith\") that technology can be used to solve a problem that was likely caused by technology in the first place.[@morozov2013] \u21a9 For those who want to reconstruct the debate between Kahneman, Tversky, and Gigerenzer, the following papers can be read in order: (1) Tsversky, (1974)[@tversky1974], (2) Gigerenzer, (1991)[@gigerenzer1991], (3) Kahneman, (1996)[@kahneman1996], (4) Gigerenzer, (1996)[@gigerenzer1996] \u21a9","title":"Understanding Bias"},{"location":"rri/chapter2/understanding_bias/#understanding-bias","text":"An artistic representation of social, statistical, and cognitive biases Before we start exploring the project lifecycle and its associated activities there is a final topic that we need to explore. You will likely have some prior familiarity of the term 'bias', but your understanding of the concept may emphasise specific properties that reflect a specific focus of your research background. This section will present three common ways that 'bias' can be understood as it applies to and affects the research and innovation lifecycle. The three perspectives that we will look at are: social, statistical, and cognitive bias.","title":"Understanding Bias"},{"location":"rri/chapter2/understanding_bias/#social-bias","text":"Outside of research and development communities, the term 'bias' is often associated with some form of prejudice or discrimination. For example, an inclination or disposition to treat an individual or organisation in a way that is considered to be unfair. This understanding of bias is necessary to draw attention to pre-existing or historical patterns of discrimination and social injustice that can be perpetuated, reinforced, or exacerbated through the development and deployment of data-driven technologies. There are numerous examples that illustrate this point. === \"Amazon's recruitment tool\" This [algorithmic system]((https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)) learned to perpetuate a bias to prefer male candidates to female candidates because this reflected past hiring decisions. === \"Predictive policing\" Predictive policing techniques are [increasingly being developed and deployed](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/), making use of geospatial data to try to learn associations between places, events, and historical crime rates. The attempt to predict where and when crimes are more likely to happen can create a positive feedback loop, which results in over-policing that may exacerbate tensions between communities and police. === \"Clinical decision support systems\" Clinical decision support systems can contribute to existing forms of [racial bias in access to healthcare](https://www.nature.com/articles/d41586-019-03228-6). A study conducted in the US found that an algorithm that used health costs as a proxy for health needs was ''less likely to refer black people than white people who were equally sick to programmes that aim to improve care for patients with complex medical needs''.[@obermeyer2019] A commonly heard response to such examples is the claim that the underlying problem is that the training data used to develop the algorithms or models were insufficiently representative. In other words,'' it was the data that were biased ''. The assumption behind this claim is that better data collection would solve the problem. Unfortunately, at best this response is only partially true, but at worst it belies a commitment to a form of 'technological solutionism' 1 that often ignores how technology affects social practices and norms. It is important to remember that most decisions that drive the project lifecycle are made by the project team. A choice to design a study in a particular way, or to deploy a system in a context that is characterised by patterns of historical discrimination, cannot simply be blamed on poor data.","title":"Social Bias"},{"location":"rri/chapter2/understanding_bias/#statistical-bias","text":"If you are a data scientist, or use techniques from data science in your research or development, then it is likely that your understanding of bias is influenced by statistical concepts. Jeff Aronson explores the etymology of the term 'bias' in a series of interesting blog articles, which emphasise the statistical understanding of bias.[@aronson2018] He begins by tracing it back to the game of bowls, where the curved trajectory of the bowl as it ran along the green reflected the asymmetric shape of the bowl (i.e., its bias). However, according to Aronson, the term was not used in statistics until around the start of the 20th Century where it was used to refer to a systematic deviation from an expected statistical result that arises due to the influence of some additional factor. This understanding is common in observational studies where bias can arise in the process of sampling or measurement. On the basis of his historical review, Aronson identifies six features of definitions of bias that adopt a statistical perspective: Systematicity: bias arises from a systematic process, rather than a random or chance process. Truth: a realist assumption that the deviation is from a true state of the world Error: the bias reflects an error, perhaps due to sampling or measurement Deviation (or Distortion): a quantity in which the observed result is taken to differ from the actual result were there no bias. Affected elements: the study elements that may be affected by the bias include the conception, design, and conduct of the study, as well as the collection, analysis, interpretation, and representation of the data Direction: the deviation is directional, as it can be caused by both an under- or over-estimation Some of these features are specific to a statistical framing of 'bias', but some also apply to the other two perspectives For instance, 'systematicity' is arguably a necessary property for social biases (i.e., a bias that systematically leads to discriminatory outcomes). And, 'error' is sometimes a property of our next perspective: cognitive biases.","title":"Statistical Bias"},{"location":"rri/chapter2/understanding_bias/#cognitive-bias","text":"Our modern understanding of cognitive biases has been most heavily influenced by research conducted by Daniel Kahneman and Amos Tversky. A lot of their work exposed a wide variety of psychological vulnerabilities, which impact our judgement and decision-making capabilities. In short, their experiments showed how individuals rely on an assortment of heuristics or biases, which speed up judgement and decision-making but also lead us astray. For example, consider the following example. The Linda Problem Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? 1) Linda is a bank teller. 2) 2) Linda is a bank teller and is active in the feminist movement. Try to answer this question yourself, before you reveal the answer. ??? note \"Reveal\" The correct answer is (1). Did you get it right? If you got it wrong, you have just committed what is known as the 'conjunction fallacy'. But don't worry you're in very good company! When Tvserky and Kahneman posed this question to a group of 88 undergraduate students, only $15%$ got the correct answer {cite}`tversky1983`. The reason it is (1) is because the probability of two events occurring *in conjunction*, such as Linda being both a 'bank teller' and 'active in the feminist movement' must be less than or equal to the probability of either event occurring on its own. Formally, for two events $A$ and $B$: $Pr(A \\wedge B) \u2264 Pr(A)$ and $Pr(A \\wedge B) \u2264 Pr(B)$ Or, to put it more simply, someone cannot belong to the set of $feminist bank tellers$ without also belonging to the set of $bank tellers$ \ud83d\udc47 ![venn diagram for linda problem](../../assets/images/graphics/venn.png) Tversky and Kahneman attributed this systematic error to what is known as the *representativeness heuristic*. In short, people don't think about the conjunction of events or consider probability theory when formulating an answer. Instead, their choice is based on which of the two options is *most representative* of the description of Linda. That is, they employ a mental shortcut (or, a heuristic) that in some instances lead to the right answer\u2014hence, their efficiency. However, in in other cases their use lead to mistakes or errors in judgement. A critical perspective on the view of judgement and decision-making put forward by Kahneman and Tversky would view it as an attempt to catalogue a variety of cognitive failures or irrationalities that stem from an individual\u2019s inability to perform rational calculations. However, those who adopt a view known as 'ecological rationality' argue that such a perspective judges human agents against a normative standard of rationality that is unsuitable for situated agents whose choice behaviour is constrained by myriad cognitive and environmental factors (e.g. temporal constraints that force decisions, limited information). This alternative account, made famous by Herbert Simon, and later developed by Gerd Gigerenzer reframes a lot of human judgement and decision-making as underpinned by \u201cfast and frugal\u201d heuristics, which are highly adaptive and support decision-making in complex and uncertain environments. It's not necessary to delve into this debate for the present purposes, but it is an interesting tangent for those interested in exploring how the choice to present statistical information in different ways (e.g., as probabilities versus frequencies) can affect comprehension and understanding. 2 When carrying out research and innovation in data science and AI, cognitive biases can impact the processes and outcome of the project lifecycle in myriad ways. There is, after all, a large list of cognitive biases to consider. No one is expected to memorise this list as a prerequisite for responsible action. However, there are some key cognitive biases that it can be helpful to consider. The next activity will explore a small handful of these biases, but we will look at others in more detail when we starting exploring the different stages of the project lifecycle in the next chapter. This way, we can anchor our understanding of biases\u2014social, statistical, and cognitive\u2014in the parts of the project lifecycle where we can hopefully mitigate their (potentially) negative impact. The term 'technological solutionism' is often used to refer to the belief (or, \"faith\") that technology can be used to solve a problem that was likely caused by technology in the first place.[@morozov2013] \u21a9 For those who want to reconstruct the debate between Kahneman, Tversky, and Gigerenzer, the following papers can be read in order: (1) Tsversky, (1974)[@tversky1974], (2) Gigerenzer, (1991)[@gigerenzer1991], (3) Kahneman, (1996)[@kahneman1996], (4) Gigerenzer, (1996)[@gigerenzer1996] \u21a9","title":"Cognitive Bias"},{"location":"rri/chapter3/","text":"The Project Lifecycle \u00b6 Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk)) Chapter Outline \u00b6 Case Studies (Project) Design Project Planning Problem Formulation Data Extraction or Procurement Data Analysis (Model) Development Preprocessing and Feature Engineering Model Selection Model Training, Testing and Validation Model Reporting (System) Deployment Model Productionalisation User Training System Use and Monitoring Model Updating or Deprovisioning Next Steps Chapter Summary In this chapter we will work our way through the key stages of the project lifecycle: (project) design, (model) development, and (system) deployment. For each stage, we will describe the accompanying activities, and highlight some of the salient ethical, social, and legal issues. However, this is presented at a relatively high-level of abstraction in the guide, because the material relies heavily on the use of case studies and accompanying activities to flesh out some of the context-specific issues. Therefore, if you are reading this chapter as part of an individual, self-directed study, you may have to adapt the activities a little. Learning Objectives In this chapter, you will: Gain a high-level understanding of the central stages of the project lifecycle: (project) design, (model) development, and (system) deployment. Explore the activities that are associated with each of the three stages, focusing on salient ethical, social, and legal issues. If you are following this material as part of a live course, you will also engage in practical discussions and activities, using several illustrative case studies to help you better understand how to conduct responsible data science and AI.","title":"Introduction"},{"location":"rri/chapter3/#the-project-lifecycle","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))","title":"The Project Lifecycle"},{"location":"rri/chapter3/#chapter-outline","text":"Case Studies (Project) Design Project Planning Problem Formulation Data Extraction or Procurement Data Analysis (Model) Development Preprocessing and Feature Engineering Model Selection Model Training, Testing and Validation Model Reporting (System) Deployment Model Productionalisation User Training System Use and Monitoring Model Updating or Deprovisioning Next Steps Chapter Summary In this chapter we will work our way through the key stages of the project lifecycle: (project) design, (model) development, and (system) deployment. For each stage, we will describe the accompanying activities, and highlight some of the salient ethical, social, and legal issues. However, this is presented at a relatively high-level of abstraction in the guide, because the material relies heavily on the use of case studies and accompanying activities to flesh out some of the context-specific issues. Therefore, if you are reading this chapter as part of an individual, self-directed study, you may have to adapt the activities a little. Learning Objectives In this chapter, you will: Gain a high-level understanding of the central stages of the project lifecycle: (project) design, (model) development, and (system) deployment. Explore the activities that are associated with each of the three stages, focusing on salient ethical, social, and legal issues. If you are following this material as part of a live course, you will also engage in practical discussions and activities, using several illustrative case studies to help you better understand how to conduct responsible data science and AI.","title":"Chapter Outline"},{"location":"rri/chapter3/summary/","text":"Next Steps \u00b6 The overview and summary of the project lifecycle presented in this chapter, by necessity, skips over a lot of detail\u2014both practical and theoretical. However, the framework has been designed with 'sustainability' in mind, and so it should be able to accommodate subsequent additions. For instance, perhaps there is a method of bias mitigation or explainability that has not been considered. Hopefully, the framework as it stands should be able to integrate this into the unifying reflective and deliberative procedure without needing to alter any of the overarching considerations. This remains to be seen. For the time being though, the project lifecycle framework provides a useful anchor for subsequent discussion, and serves to motivate the following question: How do you responsibly communicate the actions and decisions undertaken throughout the project lifecycle stages to an audience with diverse needs and expectations? This can be challenging, because there may be a range of ethical values that are implicated in your project, which in turn affect a plurality of goals and needs for the various stakeholder groups, including ensuring that the system being deployed is safe, secure, fair, trustworthy, explainable, sustainable, or respectful of human agency and autonomy. How do you provide assurance that the interconnected project processes and activities individually and collectively support the relevant goals? This is the topic of our final chapter.","title":"Next Steps"},{"location":"rri/chapter3/summary/#next-steps","text":"The overview and summary of the project lifecycle presented in this chapter, by necessity, skips over a lot of detail\u2014both practical and theoretical. However, the framework has been designed with 'sustainability' in mind, and so it should be able to accommodate subsequent additions. For instance, perhaps there is a method of bias mitigation or explainability that has not been considered. Hopefully, the framework as it stands should be able to integrate this into the unifying reflective and deliberative procedure without needing to alter any of the overarching considerations. This remains to be seen. For the time being though, the project lifecycle framework provides a useful anchor for subsequent discussion, and serves to motivate the following question: How do you responsibly communicate the actions and decisions undertaken throughout the project lifecycle stages to an audience with diverse needs and expectations? This can be challenging, because there may be a range of ethical values that are implicated in your project, which in turn affect a plurality of goals and needs for the various stakeholder groups, including ensuring that the system being deployed is safe, secure, fair, trustworthy, explainable, sustainable, or respectful of human agency and autonomy. How do you provide assurance that the interconnected project processes and activities individually and collectively support the relevant goals? This is the topic of our final chapter.","title":"Next Steps"},{"location":"rri/chapter3/model_development/model_reporting/","text":"Model Reporting \u00b6 Over the course of the previous activities, your project team will have created many diverse artefacts and forms of documentation. For example, during project planning you may have created a series of risk and impact assessments, or during exploratory data analysis you will have developed a set of notebooks explaining how you imported, cleaned, and analysed your data. In some cases, the artefacts will be by-products (e.g., system logs). In other cases, they will be the specific goal of the associated activity. Model reporting is an example of this latter set. In short, model reporting is a process of developing and integrating documentation and evidence about the processes by which your model was designed and developed (e.g., trained, tested, and evaluated), as well as how it ought to be used or deployed. What information you need to include is going to vary based on the type of project you are undertaking, and the intended use context. For example, a model developed by a private company that is sold to the NHS in England for the purpose of supporting radiologists when carrying out diagnosis in a hospital will need to be more thoroughly documented than a NLP model used as in a simple chatbot UI to support customer queries on an e-commerce website. In general, however, a model report is likely to include information about the data (e.g., size, source, method of collection, any sensitive attributes), the datasets used to train, test, and validate the model (e.g. training-testing distributions), the performance measures selected for evaluating the model (e.g. decision thresholds for classifiers, accuracy metrics), the intended use of the model, and any legal or ethical considerations associated with the model's use (e.g. fairness constraints, use of politically sensitive demographic features). There are several templates (and tools) that exist to help researchers and developers identify what information ought to be documented, but there are typically advantages and disadvantages associated with each of them. To give just a few examples: Datasheets for Datasets TRIPOD statement Model Cards for Model Reporting Factsheets When evaluating whether to use or repurposes tools such as these, it is important to consider the context in which you are developing. A model produced within the context of a publicly-funded research project may have certain disclosure obligations that are not mandatory for models developed in a commercial context. Alternatively, it may just be best practice to support reproducibility by considering how your model report adheres to principles such as the FAIR guidelines. In the next activity, we will reflect on the scope and content of some hypothetical model reports and grapple with ethical questions, such as the trade-off between transparency and privacy.","title":"Model Reporting"},{"location":"rri/chapter3/model_development/model_reporting/#model-reporting","text":"Over the course of the previous activities, your project team will have created many diverse artefacts and forms of documentation. For example, during project planning you may have created a series of risk and impact assessments, or during exploratory data analysis you will have developed a set of notebooks explaining how you imported, cleaned, and analysed your data. In some cases, the artefacts will be by-products (e.g., system logs). In other cases, they will be the specific goal of the associated activity. Model reporting is an example of this latter set. In short, model reporting is a process of developing and integrating documentation and evidence about the processes by which your model was designed and developed (e.g., trained, tested, and evaluated), as well as how it ought to be used or deployed. What information you need to include is going to vary based on the type of project you are undertaking, and the intended use context. For example, a model developed by a private company that is sold to the NHS in England for the purpose of supporting radiologists when carrying out diagnosis in a hospital will need to be more thoroughly documented than a NLP model used as in a simple chatbot UI to support customer queries on an e-commerce website. In general, however, a model report is likely to include information about the data (e.g., size, source, method of collection, any sensitive attributes), the datasets used to train, test, and validate the model (e.g. training-testing distributions), the performance measures selected for evaluating the model (e.g. decision thresholds for classifiers, accuracy metrics), the intended use of the model, and any legal or ethical considerations associated with the model's use (e.g. fairness constraints, use of politically sensitive demographic features). There are several templates (and tools) that exist to help researchers and developers identify what information ought to be documented, but there are typically advantages and disadvantages associated with each of them. To give just a few examples: Datasheets for Datasets TRIPOD statement Model Cards for Model Reporting Factsheets When evaluating whether to use or repurposes tools such as these, it is important to consider the context in which you are developing. A model produced within the context of a publicly-funded research project may have certain disclosure obligations that are not mandatory for models developed in a commercial context. Alternatively, it may just be best practice to support reproducibility by considering how your model report adheres to principles such as the FAIR guidelines. In the next activity, we will reflect on the scope and content of some hypothetical model reports and grapple with ethical questions, such as the trade-off between transparency and privacy.","title":"Model Reporting"},{"location":"rri/chapter3/model_development/model_selection/","text":"Model Selection \u00b6 This stage determines the model type and structure that will be produced in the next stages. In some projects, model selection will result in multiple models for the purpose of comparison based on some performance metric (e.g. accuracy). In other projects, there may be a need to first of all implement a pre-existing set of formal models into code. The class of relevant learning algorithms used to train the model is likely to have been highly constrained by many of the previous stages (e.g. available resources and skills, problem formulation). For example, where the problem demands a supervised learning algorithm, instead of an unsupervised learning algorithm, to help develop a model that can predict likely values for future instances not contained within the original dataset. Interpretability \u00b6 While accuracy or predictive power may be typical goals that motivate the selection of specific learning algorithms and models (see below ), there are also additional factors that can influence decision-making. One notable factor is the inherent interpretability of the model. Certain learning algorithms produce models that are inherently less interpretable than others. For instance, a linear regression model is easy to understand because of the straightforward connection between input variables and the learned weights that alter how much influence the individual variables have on the outputs. However, a neural network, at the other end of the extreme is often described as a ''black box model\" because the relationship between the input features and the output is often too difficult to interpret without the use of ad hoc methods[@ico2020]. The trade-off for this lower interpretability can be greater performance in terms of accuracy or predictive power. A schematic showing the trade-off between model interpretability and model performance. Reprinted from (Diop, 2019) This trade-off has important normative considerations though. For instance, consider the decision to deploy an algorithmic decision support system in criminal courts to help a judge decide on a sentence. A more accurate model could reduce the number of unfair decisions (e.g., someone being given a prison sentence rather than community service), but the judge may not be able to understand why a particular decision is recommend by the model and thus be unable to explain their decision to the defendant. As transparency and accessibility are vital parts of judicial decision-making and the rule of law, the use of a black-box model, in spite of the greater accuracy, may be deemed unlawful and unjust.[@bingham2011] The trade-off is in almost all non-trivial cases, unavoidable. Therefore, such a decision is inescapably value-laden and inherently about exercising ethical reflection and responsible deliberation\u2014likely in conjunction with affected stakeholders . Some Common Algorithms \u00b6 Supervised Learning \u00b6 An illustration of a neural network classifying an image of a dog correctly. Supervised learning involves training a model using a set of examples, which are pairs of input data and corresponding labels. For example, learning to classify labelled images as pictures of 'cats' or 'dogs', in order to then classify new (unlabelled) instances. Formally, the algorithm takes a set of $n$ pairs, ${(x_1,y_1),...,(x_n,y_n)}$, where $x_i$ is the feature vector of the $i$-th example and $y_i$ is its label. The task of the learning algorithm is to learn a function that maps members of the input space $X$ onto the output space $Y$. Supervised learning algorithms can be used to perform classification or regression tasks. Commonly used learning algorithms include: Linear Regression Logistic Regression Naive Bayes Decision Trees Support-Vector Machines Neural Networks Unsupervised Learning \u00b6 In contrast, unsupervised learning algorithms try to find patterns in unlabelled data, typically by clustering the data into similar groups or reducing the dimensionality of the variables (or, features) under consideration. For example, an algorithm may try to cluster shoppers into groups based on their purchasing habits. A human would then need to interpret the meaning behind this clustering. Commonly used unsupervised learning algorithms include: K-Means Clustering Hierarchical Clustering Support Vector Clustering Principal Component Analysis Reinforcement Learning \u00b6 Finally, we have reinforcement learning (RL). RL algorithms try to learn an optimal policy that has the goal of maximising some value function when interacting within a particular environment. For example, an intelligent agent that has the goal of scoring the highest number of points in a video game by learning what actions to perform in response to visual feedback from a screen. RL algorithms can be split into model-free or model-based methods, where the latter tries to build a model of its environment on which to choose the optimal policy.[@ai2019]","title":"Model Selection and Training"},{"location":"rri/chapter3/model_development/model_selection/#model-selection","text":"This stage determines the model type and structure that will be produced in the next stages. In some projects, model selection will result in multiple models for the purpose of comparison based on some performance metric (e.g. accuracy). In other projects, there may be a need to first of all implement a pre-existing set of formal models into code. The class of relevant learning algorithms used to train the model is likely to have been highly constrained by many of the previous stages (e.g. available resources and skills, problem formulation). For example, where the problem demands a supervised learning algorithm, instead of an unsupervised learning algorithm, to help develop a model that can predict likely values for future instances not contained within the original dataset.","title":"Model Selection"},{"location":"rri/chapter3/model_development/model_selection/#interpretability","text":"While accuracy or predictive power may be typical goals that motivate the selection of specific learning algorithms and models (see below ), there are also additional factors that can influence decision-making. One notable factor is the inherent interpretability of the model. Certain learning algorithms produce models that are inherently less interpretable than others. For instance, a linear regression model is easy to understand because of the straightforward connection between input variables and the learned weights that alter how much influence the individual variables have on the outputs. However, a neural network, at the other end of the extreme is often described as a ''black box model\" because the relationship between the input features and the output is often too difficult to interpret without the use of ad hoc methods[@ico2020]. The trade-off for this lower interpretability can be greater performance in terms of accuracy or predictive power. A schematic showing the trade-off between model interpretability and model performance. Reprinted from (Diop, 2019) This trade-off has important normative considerations though. For instance, consider the decision to deploy an algorithmic decision support system in criminal courts to help a judge decide on a sentence. A more accurate model could reduce the number of unfair decisions (e.g., someone being given a prison sentence rather than community service), but the judge may not be able to understand why a particular decision is recommend by the model and thus be unable to explain their decision to the defendant. As transparency and accessibility are vital parts of judicial decision-making and the rule of law, the use of a black-box model, in spite of the greater accuracy, may be deemed unlawful and unjust.[@bingham2011] The trade-off is in almost all non-trivial cases, unavoidable. Therefore, such a decision is inescapably value-laden and inherently about exercising ethical reflection and responsible deliberation\u2014likely in conjunction with affected stakeholders .","title":"Interpretability"},{"location":"rri/chapter3/model_development/model_selection/#some-common-algorithms","text":"","title":"Some Common Algorithms"},{"location":"rri/chapter3/model_development/model_selection/#supervised-learning","text":"An illustration of a neural network classifying an image of a dog correctly. Supervised learning involves training a model using a set of examples, which are pairs of input data and corresponding labels. For example, learning to classify labelled images as pictures of 'cats' or 'dogs', in order to then classify new (unlabelled) instances. Formally, the algorithm takes a set of $n$ pairs, ${(x_1,y_1),...,(x_n,y_n)}$, where $x_i$ is the feature vector of the $i$-th example and $y_i$ is its label. The task of the learning algorithm is to learn a function that maps members of the input space $X$ onto the output space $Y$. Supervised learning algorithms can be used to perform classification or regression tasks. Commonly used learning algorithms include: Linear Regression Logistic Regression Naive Bayes Decision Trees Support-Vector Machines Neural Networks","title":"Supervised Learning"},{"location":"rri/chapter3/model_development/model_selection/#unsupervised-learning","text":"In contrast, unsupervised learning algorithms try to find patterns in unlabelled data, typically by clustering the data into similar groups or reducing the dimensionality of the variables (or, features) under consideration. For example, an algorithm may try to cluster shoppers into groups based on their purchasing habits. A human would then need to interpret the meaning behind this clustering. Commonly used unsupervised learning algorithms include: K-Means Clustering Hierarchical Clustering Support Vector Clustering Principal Component Analysis","title":"Unsupervised Learning"},{"location":"rri/chapter3/model_development/model_selection/#reinforcement-learning","text":"Finally, we have reinforcement learning (RL). RL algorithms try to learn an optimal policy that has the goal of maximising some value function when interacting within a particular environment. For example, an intelligent agent that has the goal of scoring the highest number of points in a video game by learning what actions to perform in response to visual feedback from a screen. RL algorithms can be split into model-free or model-based methods, where the latter tries to build a model of its environment on which to choose the optimal policy.[@ai2019]","title":"Reinforcement Learning"},{"location":"rri/chapter3/model_development/model_testing/","text":"Model Training, Testing and Validation \u00b6 Before training a model, the data need to be split into 'training' and 'testing' sets to avoid model overfitting. 1 An example of overfitting, underfitting and the appropriate balance. Reprinted from (Bronshtein, 2020) The training set is the one used, as the name suggests, to train the model, whereas the testing set is a hold-out sample that is used to evaluate the fit of the ML model to the underlying data distribution. The testing set is kept separate while training the model to provide an less biased evaluation of the model once it has been fit to the training dataset. The human input at this stage is about deciding on the training-testing split and about how this shapes desiderata for model validation \u2014a subsequent process where the model is validated either internally or in wholly new environments (i.e. external validation). As such, the decision can be very consequential for the trustworthiness and reasonableness of the development phase of an ML/AI system. For instance, what if the training/testing split is not random? What if, hypothetically, the training data contain only examples from one class of objects and the testing data contain instances of an entirely different class? We would be unlikely to get a very useful model. There are various methods to help reduce the chance of this issue occurring, which are widely available in popular package libraries (e.g. the scikit-learn library for the Python programming language). However, a common method is to use something known as 'cross validation'. One of the most popular forms of cross validation is K-Folds Cross Validation. Here, the dataset is first split into training and testing sets, and then the training set is further split into k different subsets (e.g. 10 subsets). The model training process then occurs k times, using a different subset as the validation set on each round. At the end, an average is taken from the k models and this is tested against the original hold out testing set. The following graphic should help you visualise this. A simple representation of the K-Folds Cross Validation Process This type of validation is also known as 'internal validation', to distinguish it from external validation, and, in a similar way to choices made about the original training-testing split, the manner in which it is approached can have critical consequences for how the performance of a system is measured against the real-world conditions that it will face when operating \u201cin the wild.\u201d Therefore, external validation can also be performed, either using entirely novel datasets\u2014perhaps from different sites or using different collection methods. Or, separate external teams may even be able to externally validate a research model by attempting to reproduce similar results. To support this it is vital to ensure that the steps taken during these stages are properly documented and reported, as we will see in the next section. In short, overfitting occurs when a model is fit too closely to a specific set of data, likely leading to unnecessary complexity (e.g., too many features or parameters when compared to the number of observations). The model may perform very well on the training data, but perform poorly when presented with new observations. \u21a9","title":"Model Testing and Validation"},{"location":"rri/chapter3/model_development/model_testing/#model-training-testing-and-validation","text":"Before training a model, the data need to be split into 'training' and 'testing' sets to avoid model overfitting. 1 An example of overfitting, underfitting and the appropriate balance. Reprinted from (Bronshtein, 2020) The training set is the one used, as the name suggests, to train the model, whereas the testing set is a hold-out sample that is used to evaluate the fit of the ML model to the underlying data distribution. The testing set is kept separate while training the model to provide an less biased evaluation of the model once it has been fit to the training dataset. The human input at this stage is about deciding on the training-testing split and about how this shapes desiderata for model validation \u2014a subsequent process where the model is validated either internally or in wholly new environments (i.e. external validation). As such, the decision can be very consequential for the trustworthiness and reasonableness of the development phase of an ML/AI system. For instance, what if the training/testing split is not random? What if, hypothetically, the training data contain only examples from one class of objects and the testing data contain instances of an entirely different class? We would be unlikely to get a very useful model. There are various methods to help reduce the chance of this issue occurring, which are widely available in popular package libraries (e.g. the scikit-learn library for the Python programming language). However, a common method is to use something known as 'cross validation'. One of the most popular forms of cross validation is K-Folds Cross Validation. Here, the dataset is first split into training and testing sets, and then the training set is further split into k different subsets (e.g. 10 subsets). The model training process then occurs k times, using a different subset as the validation set on each round. At the end, an average is taken from the k models and this is tested against the original hold out testing set. The following graphic should help you visualise this. A simple representation of the K-Folds Cross Validation Process This type of validation is also known as 'internal validation', to distinguish it from external validation, and, in a similar way to choices made about the original training-testing split, the manner in which it is approached can have critical consequences for how the performance of a system is measured against the real-world conditions that it will face when operating \u201cin the wild.\u201d Therefore, external validation can also be performed, either using entirely novel datasets\u2014perhaps from different sites or using different collection methods. Or, separate external teams may even be able to externally validate a research model by attempting to reproduce similar results. To support this it is vital to ensure that the steps taken during these stages are properly documented and reported, as we will see in the next section. In short, overfitting occurs when a model is fit too closely to a specific set of data, likely leading to unnecessary complexity (e.g., too many features or parameters when compared to the number of observations). The model may perform very well on the training data, but perform poorly when presented with new observations. \u21a9","title":"Model Training, Testing and Validation"},{"location":"rri/chapter3/model_development/preprocessing/","text":"Preprocessing and Feature Engineering \u00b6 Pre-processing and feature engineering is a vital but often lengthy process, which overlaps with the design tasks in the previous section\u2014most notably the data analysis activities. It also shares with them the potential for human choices to introduce biases and discriminatory patterns into the ML/AI workflow. Tasks at this stage include (additional) data cleaning, data wrangling or normalisation, and data reduction or augmentation. Whereas data analysis is oriented towards a provisional understanding the dataset (e.g., analysing possible relationships between variables), preprocessing is directed towards the model development tasks. It is well understood that the methods employed for each of these tasks can have a significant impact on the model's performance (e.g. removing rows or columns can affect the predictive power of the model). As Ashmore et al.[@ashmore2019] note, there are also various desiderata that motivate the tasks, such as the need to ensure the dataset that will feed into the subsequent stages is relevant, complete, balanced, and accurate. Furthermore, at this stage, human decisions about how to group or disaggregate input features (e.g. how to carve up categories of gender or ethnic groups) or about which input features to exclude altogether (e.g. leaving out deprivation indicators in a predictive model for clinical diagnostics) can have significant downstream influences on the fairness and equity of an ML/AI system.","title":"Preprocessing and Feature Engineering"},{"location":"rri/chapter3/model_development/preprocessing/#preprocessing-and-feature-engineering","text":"Pre-processing and feature engineering is a vital but often lengthy process, which overlaps with the design tasks in the previous section\u2014most notably the data analysis activities. It also shares with them the potential for human choices to introduce biases and discriminatory patterns into the ML/AI workflow. Tasks at this stage include (additional) data cleaning, data wrangling or normalisation, and data reduction or augmentation. Whereas data analysis is oriented towards a provisional understanding the dataset (e.g., analysing possible relationships between variables), preprocessing is directed towards the model development tasks. It is well understood that the methods employed for each of these tasks can have a significant impact on the model's performance (e.g. removing rows or columns can affect the predictive power of the model). As Ashmore et al.[@ashmore2019] note, there are also various desiderata that motivate the tasks, such as the need to ensure the dataset that will feed into the subsequent stages is relevant, complete, balanced, and accurate. Furthermore, at this stage, human decisions about how to group or disaggregate input features (e.g. how to carve up categories of gender or ethnic groups) or about which input features to exclude altogether (e.g. leaving out deprivation indicators in a predictive model for clinical diagnostics) can have significant downstream influences on the fairness and equity of an ML/AI system.","title":"Preprocessing and Feature Engineering"},{"location":"rri/chapter3/project_design/data_extraction/","text":"Data Extraction and Procurement \u00b6 Ideally, the project team should have a clear idea in mind (from the planning and problem formulation stages) of what data are needed prior to collection, extraction, or procurement. This can help mitigate risks associated with over-collection of data (e.g. increased privacy or security concerns) and help align the project with values such as data minimisation .[@ico2020] Of course, this stage may need to be revisited after carrying out subsequent tasks (e.g. data analysis, preprocessing, model testing) if it is clear that insufficient or imbalanced data were collected to achieve the project's goals. Where data is procured, questions about provenance arise (e.g. legal issues, concerns about informed consent of human data subjects). For instance, what information about the dataset is necessary to provide sufficient assurance to the team that they are procuring data that has been reliably collected by another party. Or, will they be able to reuse and repurpose the data for their intended project. The procured data will need to be sufficiently representative of the intended target domain of the project if it is to be useful. Generally, addressing these issues and ensuring responsible data extraction and procurement requires the incorporation of myriad forms of expertise into decision-making. This can include, legal expertise (e.g., data protection officer) who is able to inform the project team of the relevant lawful bases for data collection and help; ethical expertise (e.g., whether the various rights and freedoms of data subjects have been properly respected); domain expertise (e.g., whether the method of original data collection, the expected quantity of data, and the variety of features, will be sufficient based on the problem being addressed, as formulated in the previous stage); and personal expertise (e.g., whether the data subjects are likely to be willing to provide access to all the data being requested) The FAIR Principles \u00b6 At this stage in a research project, it is also important to address the long-term sustainability of your work. One component of research sustainability is the active support of reproducibility of research. To this end, the FAIR guiding principles for scientific data management and stewardship were developed, as a means to improve the F indability, A ccessibility, I nteroperability, and R euse of research data and digital assets. In summary, data should be, Findable \u00b6 The first step in (re)using data is to find them. Metadata and data should be easy to find for both humans and computers. Machine-readable metadata are essential for automatic discovery of datasets and services. Accessible \u00b6 Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation. Interoperable \u00b6 The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing. Reusable \u00b6 The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings. We won't delve into these principles in any further detail, as it is beyond the scope of this course. However, there is more information on the Go-FAIR website and a great guide over on the Turing Way .[@community2019]","title":"Data Extraction or Procurement"},{"location":"rri/chapter3/project_design/data_extraction/#data-extraction-and-procurement","text":"Ideally, the project team should have a clear idea in mind (from the planning and problem formulation stages) of what data are needed prior to collection, extraction, or procurement. This can help mitigate risks associated with over-collection of data (e.g. increased privacy or security concerns) and help align the project with values such as data minimisation .[@ico2020] Of course, this stage may need to be revisited after carrying out subsequent tasks (e.g. data analysis, preprocessing, model testing) if it is clear that insufficient or imbalanced data were collected to achieve the project's goals. Where data is procured, questions about provenance arise (e.g. legal issues, concerns about informed consent of human data subjects). For instance, what information about the dataset is necessary to provide sufficient assurance to the team that they are procuring data that has been reliably collected by another party. Or, will they be able to reuse and repurpose the data for their intended project. The procured data will need to be sufficiently representative of the intended target domain of the project if it is to be useful. Generally, addressing these issues and ensuring responsible data extraction and procurement requires the incorporation of myriad forms of expertise into decision-making. This can include, legal expertise (e.g., data protection officer) who is able to inform the project team of the relevant lawful bases for data collection and help; ethical expertise (e.g., whether the various rights and freedoms of data subjects have been properly respected); domain expertise (e.g., whether the method of original data collection, the expected quantity of data, and the variety of features, will be sufficient based on the problem being addressed, as formulated in the previous stage); and personal expertise (e.g., whether the data subjects are likely to be willing to provide access to all the data being requested)","title":"Data Extraction and Procurement"},{"location":"rri/chapter3/project_design/data_extraction/#the-fair-principles","text":"At this stage in a research project, it is also important to address the long-term sustainability of your work. One component of research sustainability is the active support of reproducibility of research. To this end, the FAIR guiding principles for scientific data management and stewardship were developed, as a means to improve the F indability, A ccessibility, I nteroperability, and R euse of research data and digital assets. In summary, data should be,","title":"The FAIR Principles"},{"location":"rri/chapter3/project_design/data_extraction/#findable","text":"The first step in (re)using data is to find them. Metadata and data should be easy to find for both humans and computers. Machine-readable metadata are essential for automatic discovery of datasets and services.","title":"Findable"},{"location":"rri/chapter3/project_design/data_extraction/#accessible","text":"Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation.","title":"Accessible"},{"location":"rri/chapter3/project_design/data_extraction/#interoperable","text":"The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.","title":"Interoperable"},{"location":"rri/chapter3/project_design/data_extraction/#reusable","text":"The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings. We won't delve into these principles in any further detail, as it is beyond the scope of this course. However, there is more information on the Go-FAIR website and a great guide over on the Turing Way .[@community2019]","title":"Reusable"},{"location":"rri/chapter3/project_design/planning/","text":"Project Planning \u00b6 In October 2021, the Financial Times reported that facial recognition cameras were being used in UK schools to scan the faces of ''thousands of British pupils in school canteens'' to automate the process of taking payment for lunches. The managing director of CRB Cunninghams\u2014the company that developed the system sold to schools\u2014claimed that ''In a secondary school you have around about a 25-minute period to serve potentially 1,000 pupils. So we need fast throughput at the point of sale.'' Question Does this seem like a plausible justification for the design, development, and deployment of an automated facial recognition system? Or, does the use of controversial and possibly biased technology run the risk of normalising invasive surveillance practices? Addressing questions such as the one above should be one of the first activities in a responsible project lifecycle. Unfortunately, vested interests often prevent them from being discussed in an open and responsible manner. The result can be the treatment of data-driven technologies as a ''hammer'' with which to go looking for nails! To prevent this, it is best to have a clear idea in mind of what the project's goals are at the outset, and what problem is being addressed. This can help to avoid a myopic focus on a narrow class of technology-based \"solutions\", and also helps create space for a diversity of approaches\u2014some of which may not require data-driven technology at all. Project planning, therefore, can comprise a wide variety of tasks, including, but not limited to: an assessment of whether developing or using data-driven technology is the right approach given available resources and data, existing technologies and processes already in place, the complexity of the use-contexts involved, and the nature of the policy or social problem that needs to be solved[@leslie2021a]; a discussion with an ethics committee or internal review board to help evaluate the ethical credentials of the project; an analysis of user needs in relation to the prospective model and whether a solution involving the latter provides appropriate affordances in keeping with user needs and related functional desiderata; identification and mapping of key stages in the project to support project governance and business tasks (e.g. scenario planning); an assessment of resources and capabilities within a team, which is necessary for identifying any skills gaps ; a contextual assessment of the target domain and of the expectations, norms, and requirements that derive therefrom; a stakeholder impact assessment , supported by affected people and communities, to identify and evaluate possible harms and benefits associated with the project (e.g. socioeconomic inequalities that may be exacerbated as a result of carrying out the project), to gain social license and public trust, and also feed into the process of problem formulation in the next stage; an analysis of team positionality to determine the appropriate level and scope of community engagement activities (Leslie et al 2021b); wider impact assessments \u2014both where required by statute and done voluntarily for transparency and best practice (e.g. equality impact assessments, data protection impact assessments, human rights impact assessment, bias assessment). This can be a lot to undertake at the outset of a project, but the upfront cost can help offset the large technical and ethical debt that may otherwise accumulate from a failure to anticipate or foresee possible challenges. Many of the above activities have been tried and tested in a wide variety of domains, and there are many templates or frameworks that can be used to help speed up the process (see Further Resources ).","title":"Project Planning"},{"location":"rri/chapter3/project_design/planning/#project-planning","text":"In October 2021, the Financial Times reported that facial recognition cameras were being used in UK schools to scan the faces of ''thousands of British pupils in school canteens'' to automate the process of taking payment for lunches. The managing director of CRB Cunninghams\u2014the company that developed the system sold to schools\u2014claimed that ''In a secondary school you have around about a 25-minute period to serve potentially 1,000 pupils. So we need fast throughput at the point of sale.'' Question Does this seem like a plausible justification for the design, development, and deployment of an automated facial recognition system? Or, does the use of controversial and possibly biased technology run the risk of normalising invasive surveillance practices? Addressing questions such as the one above should be one of the first activities in a responsible project lifecycle. Unfortunately, vested interests often prevent them from being discussed in an open and responsible manner. The result can be the treatment of data-driven technologies as a ''hammer'' with which to go looking for nails! To prevent this, it is best to have a clear idea in mind of what the project's goals are at the outset, and what problem is being addressed. This can help to avoid a myopic focus on a narrow class of technology-based \"solutions\", and also helps create space for a diversity of approaches\u2014some of which may not require data-driven technology at all. Project planning, therefore, can comprise a wide variety of tasks, including, but not limited to: an assessment of whether developing or using data-driven technology is the right approach given available resources and data, existing technologies and processes already in place, the complexity of the use-contexts involved, and the nature of the policy or social problem that needs to be solved[@leslie2021a]; a discussion with an ethics committee or internal review board to help evaluate the ethical credentials of the project; an analysis of user needs in relation to the prospective model and whether a solution involving the latter provides appropriate affordances in keeping with user needs and related functional desiderata; identification and mapping of key stages in the project to support project governance and business tasks (e.g. scenario planning); an assessment of resources and capabilities within a team, which is necessary for identifying any skills gaps ; a contextual assessment of the target domain and of the expectations, norms, and requirements that derive therefrom; a stakeholder impact assessment , supported by affected people and communities, to identify and evaluate possible harms and benefits associated with the project (e.g. socioeconomic inequalities that may be exacerbated as a result of carrying out the project), to gain social license and public trust, and also feed into the process of problem formulation in the next stage; an analysis of team positionality to determine the appropriate level and scope of community engagement activities (Leslie et al 2021b); wider impact assessments \u2014both where required by statute and done voluntarily for transparency and best practice (e.g. equality impact assessments, data protection impact assessments, human rights impact assessment, bias assessment). This can be a lot to undertake at the outset of a project, but the upfront cost can help offset the large technical and ethical debt that may otherwise accumulate from a failure to anticipate or foresee possible challenges. Many of the above activities have been tried and tested in a wide variety of domains, and there are many templates or frameworks that can be used to help speed up the process (see Further Resources ).","title":"Project Planning"},{"location":"rri/chapter3/project_design/problem/","text":"Problem Formulation \u00b6 In the previous section we acknowledge that it is important to clearly define and delineate the problem that your project is intended to address. Here, \u2018problem\u2019 is used in two interrelated ways: To refer to a well-defined computational process (or a higher-level abstraction of the process) that is carried out by the algorithm to map inputs to outputs. To refer to the wider practical, social, or policy issue that will be addressed through the translation of that issue into the aforementioned mathematical or computational framing. On the computational side, we can think of how a convolutional neural network carries out a series of successive transformations by taking (as input) an image, encoded as an array, in order to produce (as output) a decision about whether some object is present in the image. On the practical, social, and policy side, there will be a need to define the computational ''problem'' being solved in terms of the algorithmic system\u2019s embeddedness in the social environment and to explain how it contributes to (or affects) the wider sociotechnical issue being considered. In the convolutional neural network example, the system being produced may be a facial recognition technology that responds to a perceived need for the biometric identification of criminal suspects by matching facial images in a police database. The social issue of wanting to identify suspects is, in this case, translated into the computational mechanism of the computer vision system. But, beyond this, diligent consideration of the practical, social, or policy issue being addressed by the system will also trigger, inter alia , reflection on the complex intersection of potential algorithmic bias, the cascading effects of sociohistorical patterns of racism and discrimination, wider societal and community impacts, and the potential effects of the use of the model on the actors in the criminal justice systems who will become implementers and subjects of the technology. Sociotechnical considerations are also important for determining and evaluating the choice of target variables used by the algorithm, which may ultimately be implemented within a larger automated decision-making system (e.g. in a risk prediction system). The task of formulating the problem allows the project team to get clear on what input data will be needed, for what purpose, and whether there exists any representational issues in, for example, how the target variables are defined. It also allows for a project team (and impacted stakeholders) to reflect on the reasonableness of the measurable proxy that is used as a mathematical expression of the target variable, for instance, whether moving a patient into an intensive care unit (ICU) is a reasonable action to take after being classified as ''at risk'' by a model that uses a set of demographic and physiological input variables. The fact that formulating problems and defining target variables in ML/AI innovation lifecycles can often be a biased and contested process is why stakeholder engagement, which helps bring a diversity of perspectives to project design, is so vital. It is also why this stage is so closely connected with the interpretive burdens of the project planning activities seen in the previous section (e.g. discussion about legal and ethical concerns regarding permissible uses of personal or sensitive information).","title":"Problem Formulation"},{"location":"rri/chapter3/project_design/problem/#problem-formulation","text":"In the previous section we acknowledge that it is important to clearly define and delineate the problem that your project is intended to address. Here, \u2018problem\u2019 is used in two interrelated ways: To refer to a well-defined computational process (or a higher-level abstraction of the process) that is carried out by the algorithm to map inputs to outputs. To refer to the wider practical, social, or policy issue that will be addressed through the translation of that issue into the aforementioned mathematical or computational framing. On the computational side, we can think of how a convolutional neural network carries out a series of successive transformations by taking (as input) an image, encoded as an array, in order to produce (as output) a decision about whether some object is present in the image. On the practical, social, and policy side, there will be a need to define the computational ''problem'' being solved in terms of the algorithmic system\u2019s embeddedness in the social environment and to explain how it contributes to (or affects) the wider sociotechnical issue being considered. In the convolutional neural network example, the system being produced may be a facial recognition technology that responds to a perceived need for the biometric identification of criminal suspects by matching facial images in a police database. The social issue of wanting to identify suspects is, in this case, translated into the computational mechanism of the computer vision system. But, beyond this, diligent consideration of the practical, social, or policy issue being addressed by the system will also trigger, inter alia , reflection on the complex intersection of potential algorithmic bias, the cascading effects of sociohistorical patterns of racism and discrimination, wider societal and community impacts, and the potential effects of the use of the model on the actors in the criminal justice systems who will become implementers and subjects of the technology. Sociotechnical considerations are also important for determining and evaluating the choice of target variables used by the algorithm, which may ultimately be implemented within a larger automated decision-making system (e.g. in a risk prediction system). The task of formulating the problem allows the project team to get clear on what input data will be needed, for what purpose, and whether there exists any representational issues in, for example, how the target variables are defined. It also allows for a project team (and impacted stakeholders) to reflect on the reasonableness of the measurable proxy that is used as a mathematical expression of the target variable, for instance, whether moving a patient into an intensive care unit (ICU) is a reasonable action to take after being classified as ''at risk'' by a model that uses a set of demographic and physiological input variables. The fact that formulating problems and defining target variables in ML/AI innovation lifecycles can often be a biased and contested process is why stakeholder engagement, which helps bring a diversity of perspectives to project design, is so vital. It is also why this stage is so closely connected with the interpretive burdens of the project planning activities seen in the previous section (e.g. discussion about legal and ethical concerns regarding permissible uses of personal or sensitive information).","title":"Problem Formulation"},{"location":"rri/chapter3/system_deployment/model_productionalisation/","text":"Model Productionalisation \u00b6 Unless the end result of the project is the model itself, which is perhaps more common in scientific research, it is likely that the model will need to be implemented within a larger system. This process, sometimes known as 'model operationalisation', requires understanding (a) how the model is intended to function in the proximate system (e.g. within an agricultural decision support system used to predict crop yield and quality) and (b) how the model will impact\u2014and be impacted by\u2014the functioning of the wider sociotechnical environment that the tool is embedded within (e.g. a decision support tool used in healthcare for patient triaging that may exacerbate existing health inequalities within the wider community). Ensuring the model works within the proximate system can be a complex programming and software engineering task, especially if it is expected that the model will be updated continuously in its runtime environment. But, more importantly, understanding how to ensure the model\u2019s sustainability given its embeddedness in complex and changing sociotechnical environments requires active and contextually-informed monitoring, situational awareness, and vigilant responsiveness. As noted, this stage of the product lifecycle involves often complex forms of software engineering, which in many cases are just about ensuring appropriate software dependencies and packages are installed that allow users of the system to access or interface with the model as intended (e.g. building and coding an API). This is not to deny that there are important ethical issues involved with model productionalisation, especially with how an API either enables inhibits access to the benefits of a model. However, we can consider these issues elsewhere (e.g. during ' project planning ' or ' user training ' and in a way that doesn't presuppose familiarity with technical concepts. Therefore, we will keep this section as a brief note.","title":"Model Productionalisation"},{"location":"rri/chapter3/system_deployment/model_productionalisation/#model-productionalisation","text":"Unless the end result of the project is the model itself, which is perhaps more common in scientific research, it is likely that the model will need to be implemented within a larger system. This process, sometimes known as 'model operationalisation', requires understanding (a) how the model is intended to function in the proximate system (e.g. within an agricultural decision support system used to predict crop yield and quality) and (b) how the model will impact\u2014and be impacted by\u2014the functioning of the wider sociotechnical environment that the tool is embedded within (e.g. a decision support tool used in healthcare for patient triaging that may exacerbate existing health inequalities within the wider community). Ensuring the model works within the proximate system can be a complex programming and software engineering task, especially if it is expected that the model will be updated continuously in its runtime environment. But, more importantly, understanding how to ensure the model\u2019s sustainability given its embeddedness in complex and changing sociotechnical environments requires active and contextually-informed monitoring, situational awareness, and vigilant responsiveness. As noted, this stage of the product lifecycle involves often complex forms of software engineering, which in many cases are just about ensuring appropriate software dependencies and packages are installed that allow users of the system to access or interface with the model as intended (e.g. building and coding an API). This is not to deny that there are important ethical issues involved with model productionalisation, especially with how an API either enables inhibits access to the benefits of a model. However, we can consider these issues elsewhere (e.g. during ' project planning ' or ' user training ' and in a way that doesn't presuppose familiarity with technical concepts. Therefore, we will keep this section as a brief note.","title":"Model Productionalisation"},{"location":"rri/chapter3/system_deployment/model_updating/","text":"Model Updating or Deprovisioning \u00b6 We come now to the end of the project lifecycle! In the previous section we saw that model drift can affect the accuracy and overall performance of a model. One way to address this is to update the model by retraining on more timely and up-to-date data. Model updating can occur continuously if the architecture of the system and context of its use allows for it. This can help prevent model drift from occurring in the first place. However, this type of online learning is a challenging task, and is not without its limitations. For instance, it introduces a further source of uncertainty into how the model and system will perform, given their close coupling with the environment. Alternatively, model updating can occur periodically . Perhaps, there is known seasonal variation in the performance of a system (e.g. a recommender system for an e-commerce site that has to adjust for varied shopping patterns). As such, the re-training of the model may be planned around these times to help maintain a high-level of performance. These types of updating can use the original model as a starting point, in order to just retune the model's parameters or maybe drop certain features that are no longer predictive. However, there is also the option of entirely de-provisioning (i.e., stopping use) the model and system if performance simply drops too low to be addressed by mere re-training. Full Circle \u00b6 As you may recall, the project lifecycle image showed this final stage connected to the first stage of ' Project Planning ' in a circular manner. The Project Lifecycle.] The reason for this should now be clearer. Depending on the choices made at this stage, it may be necessary to start planning for a new project. For instance, a project team may not be able to simply remove a system that serves a business critical function. Instead, an existing project may serve as a foundational input or constraint into the planning stages of a new project\u2014starting the cycle once more.","title":"Model Updating or Deprovisioning"},{"location":"rri/chapter3/system_deployment/model_updating/#model-updating-or-deprovisioning","text":"We come now to the end of the project lifecycle! In the previous section we saw that model drift can affect the accuracy and overall performance of a model. One way to address this is to update the model by retraining on more timely and up-to-date data. Model updating can occur continuously if the architecture of the system and context of its use allows for it. This can help prevent model drift from occurring in the first place. However, this type of online learning is a challenging task, and is not without its limitations. For instance, it introduces a further source of uncertainty into how the model and system will perform, given their close coupling with the environment. Alternatively, model updating can occur periodically . Perhaps, there is known seasonal variation in the performance of a system (e.g. a recommender system for an e-commerce site that has to adjust for varied shopping patterns). As such, the re-training of the model may be planned around these times to help maintain a high-level of performance. These types of updating can use the original model as a starting point, in order to just retune the model's parameters or maybe drop certain features that are no longer predictive. However, there is also the option of entirely de-provisioning (i.e., stopping use) the model and system if performance simply drops too low to be addressed by mere re-training.","title":"Model Updating or Deprovisioning"},{"location":"rri/chapter3/system_deployment/model_updating/#full-circle","text":"As you may recall, the project lifecycle image showed this final stage connected to the first stage of ' Project Planning ' in a circular manner. The Project Lifecycle.] The reason for this should now be clearer. Depending on the choices made at this stage, it may be necessary to start planning for a new project. For instance, a project team may not be able to simply remove a system that serves a business critical function. Instead, an existing project may serve as a foundational input or constraint into the planning stages of a new project\u2014starting the cycle once more.","title":"Full Circle"},{"location":"rri/chapter3/system_deployment/user_training/","text":"User Training \u00b6 Consider the following scenario: Security System You are in charge of deploying an automated facial recognition system that is used to verify that people who are attempting to enter a secure building are using the appropriate identity card. Upon swiping their identity card, the system scans the face of the user and matches it to the expected image from a database of authorised people. If the person matches their card and is also allowed access to the building, they are automatically granted access. After a week or so of deploying the system, you find out that the security guard in charge of the building has been overriding the system. You go to speak with the security guard, and he claims that the system has been refusing entry to people who clearly match their identity badge. When you investigate the issue, you find out that the system is functioning as expected and that no errors with the automated facial recognition system have been logged. However, every one of the attempts that the security guard has overridden are for people with expired identity cards. Although they match their cards, they should not have access to the building. This scenario highlights an important, but sometimes overlooked part of system deployment and evaluation: human factors . Human factors is a field in which researchers and practitioners are interested in both understanding the interaction of people and technology and in making that interaction more efficient, safer, and pleasant.[@durso2014] Research into human factors considers both perceptual, cognitive, social, and physical characteristics of the human operator, as well as how a technological system has been designed. This is important, because if there is an issue, such as the one in our above scenario, the sociotechnical system can be improved by changing either a) the human or b) the technology. For instance, in our scenario, you could provide training for the security guard that shows him how to identify the cause of an automated rejection. Or, you could design a simple prompt or notification that explains why an individual is being denied entry to avoid the possibility of the security guard overriding the system unless there is a legitimate false negative (i.e. an error with the facial recognition system for a valid identity card and matched person). There is a huge amount of quantitative and qualitative research related to human factors in general (see Durso, 2014[@durso2014]), and a growing source of research for human interaction with algorithmic systems more specifically. For present purposes, it is sufficient to note that although the performance of a model is evaluated in earlier stages, the model's impact cannot be fully evaluated without consideration of the human factors that affect its performance in real-world settings. For instance, the impact of human cognitive biases, such as algorithmic aversion must also be considered, as these biases can lead to over- and under-reliance on the model (or system), in turn negating any potential benefits that may arise from its use. Understanding the social and environmental context is also vital, as sociocultural norms may contribute to how training is received, and how the system itself is evaluated (see {Burton, 2020[@burton2020]). In the context of RRI, user training related to how an algorithmic system should be operated may include: a) conveying basic knowledge about the nature of machine learning (e.g. probabilistic results or outcomes), b) explaining the limitations of the system, c) educating users about the risks of AI-related biases, such as decision-automation bias or automation-distrust bias, and d) encouraging users to view the benefits and risks of deploying these systems in terms of their role in helping humans to come to judgements, rather than replacing that judgement.","title":"User Training"},{"location":"rri/chapter3/system_deployment/user_training/#user-training","text":"Consider the following scenario: Security System You are in charge of deploying an automated facial recognition system that is used to verify that people who are attempting to enter a secure building are using the appropriate identity card. Upon swiping their identity card, the system scans the face of the user and matches it to the expected image from a database of authorised people. If the person matches their card and is also allowed access to the building, they are automatically granted access. After a week or so of deploying the system, you find out that the security guard in charge of the building has been overriding the system. You go to speak with the security guard, and he claims that the system has been refusing entry to people who clearly match their identity badge. When you investigate the issue, you find out that the system is functioning as expected and that no errors with the automated facial recognition system have been logged. However, every one of the attempts that the security guard has overridden are for people with expired identity cards. Although they match their cards, they should not have access to the building. This scenario highlights an important, but sometimes overlooked part of system deployment and evaluation: human factors . Human factors is a field in which researchers and practitioners are interested in both understanding the interaction of people and technology and in making that interaction more efficient, safer, and pleasant.[@durso2014] Research into human factors considers both perceptual, cognitive, social, and physical characteristics of the human operator, as well as how a technological system has been designed. This is important, because if there is an issue, such as the one in our above scenario, the sociotechnical system can be improved by changing either a) the human or b) the technology. For instance, in our scenario, you could provide training for the security guard that shows him how to identify the cause of an automated rejection. Or, you could design a simple prompt or notification that explains why an individual is being denied entry to avoid the possibility of the security guard overriding the system unless there is a legitimate false negative (i.e. an error with the facial recognition system for a valid identity card and matched person). There is a huge amount of quantitative and qualitative research related to human factors in general (see Durso, 2014[@durso2014]), and a growing source of research for human interaction with algorithmic systems more specifically. For present purposes, it is sufficient to note that although the performance of a model is evaluated in earlier stages, the model's impact cannot be fully evaluated without consideration of the human factors that affect its performance in real-world settings. For instance, the impact of human cognitive biases, such as algorithmic aversion must also be considered, as these biases can lead to over- and under-reliance on the model (or system), in turn negating any potential benefits that may arise from its use. Understanding the social and environmental context is also vital, as sociocultural norms may contribute to how training is received, and how the system itself is evaluated (see {Burton, 2020[@burton2020]). In the context of RRI, user training related to how an algorithmic system should be operated may include: a) conveying basic knowledge about the nature of machine learning (e.g. probabilistic results or outcomes), b) explaining the limitations of the system, c) educating users about the risks of AI-related biases, such as decision-automation bias or automation-distrust bias, and d) encouraging users to view the benefits and risks of deploying these systems in terms of their role in helping humans to come to judgements, rather than replacing that judgement.","title":"User Training"},{"location":"rri/chapter4/","text":"Responsible Communication \u00b6 Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk)) Chapter Outline \u00b6 What is Argument-Based Assurance? Assurance and Responsible Communication Goals, Properties, and Evidence Chapter Summary In this chapter we will look at what happens when a project reaches the stage where it is necessary to communicate research findings or present the output of the innovation lifecycle to a broader audience. We will consider a method known as argument-based assurance, which has been designed to help developers and project members engage their audience in a trustworthy and transparent manner. Learning Objectives In this chapter, you will: Consider the basics of the argument-based assurance methodology Understand when and how it can be used to facilitate responsible communication Use the method to identify broader normative goals that may not have been covered in this course, and determine which properties need to be assured to help demonstrate that the respective goal has been obtained.","title":"Introduction"},{"location":"rri/chapter4/#responsible-communication","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk))","title":"Responsible Communication"},{"location":"rri/chapter4/#chapter-outline","text":"What is Argument-Based Assurance? Assurance and Responsible Communication Goals, Properties, and Evidence Chapter Summary In this chapter we will look at what happens when a project reaches the stage where it is necessary to communicate research findings or present the output of the innovation lifecycle to a broader audience. We will consider a method known as argument-based assurance, which has been designed to help developers and project members engage their audience in a trustworthy and transparent manner. Learning Objectives In this chapter, you will: Consider the basics of the argument-based assurance methodology Understand when and how it can be used to facilitate responsible communication Use the method to identify broader normative goals that may not have been covered in this course, and determine which properties need to be assured to help demonstrate that the respective goal has been obtained.","title":"Chapter Outline"},{"location":"rri/chapter4/assurance/","text":"What is Argument-Based Assurance? \u00b6 Reference This section is based on Burr[@burr2021], which provides a more thorough account of the argument-based assurance methodology and how it applies to responsible research and innovation in data science and AI. The method we will explore that serves the role of facilitating responsible communication is known as argument-based assurance (ABA). We can define ABA as: a process of using structured argumentation to provide assurance to another party (or parties) that a particular claim (or set of related claims) about a property of a system is warranted given the available evidence. As a structured method for communication, ABA is already widely used in safety-critical domains or industries where manufacturing and development processes are required to comply with strict regulatory standards and support industry-recognised best practices.[@hawkins2021] However, ABA is useful for more than just demonstrating regulatory compliance. It can also assist internal reflection and deliberation by providing a systematic and structured means for evaluating how the development of systems or products can fulfil certain normative goals (e.g. safety or robustness), according to certain well-defined properties (e.g. software hazards identified) and criteria (e.g. risk reduction thresholds met); provide a deliberate means for the anticipation and pre-emption of potential risks and adverse impacts through mechanisms of end-to-end assessment and redress; facilitate transparent communication between developers and affected stakeholders; support mechanisms and processes of documentation (or, reporting) to ensure accountability (e.g. audits, compliance); and build trust and confidence by promoting the adoption of best practices (e.g. standards for warranted evidence) and by conveying the integration of these into design, development, and deployment lifecycles to impacted stakeholders. For these reasons, it is a useful foundation upon which to build a framework for responsible communication. Assurance Cases \u00b6 When a barrister stands in a court room, in front of the judge, jury, and defendant, they are tasked with presenting a case. If they are part of the prosecution, their role is to convince the jury, beyond all reasonable doubt, that the defendant is guilty of committing the crimes of which they stand accused, based on the admissible evidence agreed upon by all parties. The case they present, therefore, is an argument that attempts to justify their position or standpoint. Although the format and goals may be different, the purpose of argument-based assurance is also to develop and present a case. This is not an argument in the antagonistic sense of the term, but rather a structured and justifiable case supported by evidence. The scope and content of what we can call an 'assurance case' is determined by the relevant details of the project in question, and what the project team need to provide assurance for. For example, if a project team needs to communicate the processes by which they have ensured the interpretability of their model, they may need to develop an interpretability case , which could look something like the following: Here, the assurance case is represented in a graphical format. The top-level claim is a goal that is supported by the lower level claims, which in turn further specify what it means to say \"The {ML model} is sufficiently {interpretable} in the intended {context}\". At the lowest level is the evidence that supports and establishes the relevant basis for making the specific claims. Overall, the case is a structured argument that is oriented towards the top-level goal. This is achieved by first reflecting on what the goal claim means. For instance, what are the parameters of sufficiently interpretable ? Or, what is the intended context ? Next, the project team consider the actions they have taken that can be referred to as supporting evidence . This evidence provides the inferential support for the higher-level claims. Finally, once all the pieces are together in a structured manner, the entire case is used as the basis for justifying the validity of the top-level goal. Elements of an Assurance Case \u00b6 The interpretability example helps us identify a minimal set of elements that need to be established in an assurance case: A top-level normative goal Claims about the project or system Supporting evidence The top-level goal orients and delineates the case by setting a direction and helping to establish the scope of what claims need to be included. For instance, a particular claim about the privacy or security of a project's data management policy many be important but unnecessary to include in an assurance case that justifies why a model does not cause any discriminatory harm. In addition to any clarificatory claims that pertain to the goal (e.g., what type of model is being assured; the context of use for the system), the lower-level claims should further specify the goal by a) addressing the specific activities that have been carried out during the project, or b) identifying properties of the system that help ensure the goal claim is legitimate. We can, therefore, separate the type of lower-level claim being established as either a project or system property claim. Unless the claim is self-evidential, there will need to be a final element that points to some supporting evidence. This evidence establishes the foundation upon which the justifiability of the overall argument depends. The following two figures show the relationship between these elements and also provide an example of a partial assurance case that focuses on a goal of safety. === \"Figure A\" ![A minimal set of elements for an assurance case.](../../assets/images/graphics/gce.png) A minimal set of elements for an assurance case. === \"Figure B\" ![An example of a goal, claims, and evidence using the minimal set of elements identified previously.](../../assets/images/graphics/gce-example.png) An example of a goal, claims, and evidence using the minimal set of elements identified previously. Who is the Target Audience? \u00b6 As you can probably imagine, all of these elements help play a vital role in the effective communication of an assurance case. A clear goal helps signal to stakeholders what values underwrite and motivate your project, as well as providing the means for more critical evaluation and engagement (i.e. an assessment by the stakeholders of whether your goal has been obtained, conditional on the evidence provided). The set of claims collectively establishes the scope and content of your argument, enabling stakeholders to identify whether there are any gaps (i.e. whether your argument is complete). And, the evidential base allows stakeholders to determine whether there is a legitimate reason for accepting your argument. However, the justifiability and acceptability of an assurance case, in part, depends on the target audience. Sticking with the interpretability example, we can note that what is interpretable to an expert in statistical learning theory may be completely uninterpretable for a policy-maker tasked with evaluating whether a particular model is suitable to deploy in their own project. Therefore, prior to building an assurance case, it is important to identify the target audience and understand their needs. In some cases, this may be determined on the behalf of the project team (e.g. where an external auditor requests assurance for compliance objectives). In other cases, identifying relevant stakeholders may have been performed through stakeholder engagement processes carried out as part of the ' Project Planning ' activities. Reflect, Act, Justify \u00b6 We now have sufficient background information to present an intuitive (high-level) procedure for developing and communicating an assurance case. The procedure is broken down into three steps, which also complement the goals of the project lifecycle : Reflect Act Justify Reflection is an anticipatory and deliberative process in which questions such as the following are asked of the project and its governance: What are the goals of your system? How are these goals defined? Which stakeholders have participated in the identification and defining of these goals? What properties need to be implemented in the project or system to ensure that these goals are achieved? Which actions ought to be taken to establish these properties within the project or system? These are inherently normative and value-laden questions, which is one reason why diverse and inclusive stakeholder engagement is so crucial. Action occurs throughout all of the stages of the project lifecycle, and the output of many of these actions are likely to serve as the evidence for the claims of the assurance case. These actions and evidential artefacts can also help you identify what claims may be relevant in your argument. As such, the following questions serve only to provide some additional supporting structure to this process: What actions have been undertaken during (project) design that have generated salient evidence for your goals and claims? What actions have been undertaken during (model) development that have generated salient evidence for your goals and claims? What actions have been undertaken during (system) deployment that have generated salient evidence for your goals and claims? Using the project lifecycle model as a scaffold or guide is, therefore, a useful tool for both a) reflectively planning the necessary activities that ought to be undertaken, prior to the project's commencement, and b) evaluating and assessing whether there are any gaps as a project evolves. The final step is to justify that your evidence base is sufficient to warrant the claims that are being made about the properties of your project or system. This does not mean that the assurance case is the final activity that needs to be done at the very end of a project. Rather, it's development should be seen as iterative and ongoing as the project evolves. Identifying the relevant evidence and determining whether the evidence base is sufficient and complete can be challenging. To help this process, deliberative prompts such as the following can be instructive: Which stakeholders, identified in your stakeholder engagement plan, can support the evaluation of your evidence and overall case? Is any evidence missing from your case? Are the collection of property claims jointly sufficient to support your top-level goal? However, in general stakeholder engagement\u2014especially with domain experts\u2014is essential. In the next section, we will discuss some possible orienting goals and principles, to help you identify the sorts of properties that may comprise an ethical and responsible assurance case.","title":"What is Argument-Based Assurance"},{"location":"rri/chapter4/assurance/#what-is-argument-based-assurance","text":"Reference This section is based on Burr[@burr2021], which provides a more thorough account of the argument-based assurance methodology and how it applies to responsible research and innovation in data science and AI. The method we will explore that serves the role of facilitating responsible communication is known as argument-based assurance (ABA). We can define ABA as: a process of using structured argumentation to provide assurance to another party (or parties) that a particular claim (or set of related claims) about a property of a system is warranted given the available evidence. As a structured method for communication, ABA is already widely used in safety-critical domains or industries where manufacturing and development processes are required to comply with strict regulatory standards and support industry-recognised best practices.[@hawkins2021] However, ABA is useful for more than just demonstrating regulatory compliance. It can also assist internal reflection and deliberation by providing a systematic and structured means for evaluating how the development of systems or products can fulfil certain normative goals (e.g. safety or robustness), according to certain well-defined properties (e.g. software hazards identified) and criteria (e.g. risk reduction thresholds met); provide a deliberate means for the anticipation and pre-emption of potential risks and adverse impacts through mechanisms of end-to-end assessment and redress; facilitate transparent communication between developers and affected stakeholders; support mechanisms and processes of documentation (or, reporting) to ensure accountability (e.g. audits, compliance); and build trust and confidence by promoting the adoption of best practices (e.g. standards for warranted evidence) and by conveying the integration of these into design, development, and deployment lifecycles to impacted stakeholders. For these reasons, it is a useful foundation upon which to build a framework for responsible communication.","title":"What is Argument-Based Assurance?"},{"location":"rri/chapter4/assurance/#assurance-cases","text":"When a barrister stands in a court room, in front of the judge, jury, and defendant, they are tasked with presenting a case. If they are part of the prosecution, their role is to convince the jury, beyond all reasonable doubt, that the defendant is guilty of committing the crimes of which they stand accused, based on the admissible evidence agreed upon by all parties. The case they present, therefore, is an argument that attempts to justify their position or standpoint. Although the format and goals may be different, the purpose of argument-based assurance is also to develop and present a case. This is not an argument in the antagonistic sense of the term, but rather a structured and justifiable case supported by evidence. The scope and content of what we can call an 'assurance case' is determined by the relevant details of the project in question, and what the project team need to provide assurance for. For example, if a project team needs to communicate the processes by which they have ensured the interpretability of their model, they may need to develop an interpretability case , which could look something like the following: Here, the assurance case is represented in a graphical format. The top-level claim is a goal that is supported by the lower level claims, which in turn further specify what it means to say \"The {ML model} is sufficiently {interpretable} in the intended {context}\". At the lowest level is the evidence that supports and establishes the relevant basis for making the specific claims. Overall, the case is a structured argument that is oriented towards the top-level goal. This is achieved by first reflecting on what the goal claim means. For instance, what are the parameters of sufficiently interpretable ? Or, what is the intended context ? Next, the project team consider the actions they have taken that can be referred to as supporting evidence . This evidence provides the inferential support for the higher-level claims. Finally, once all the pieces are together in a structured manner, the entire case is used as the basis for justifying the validity of the top-level goal.","title":"Assurance Cases"},{"location":"rri/chapter4/assurance/#elements-of-an-assurance-case","text":"The interpretability example helps us identify a minimal set of elements that need to be established in an assurance case: A top-level normative goal Claims about the project or system Supporting evidence The top-level goal orients and delineates the case by setting a direction and helping to establish the scope of what claims need to be included. For instance, a particular claim about the privacy or security of a project's data management policy many be important but unnecessary to include in an assurance case that justifies why a model does not cause any discriminatory harm. In addition to any clarificatory claims that pertain to the goal (e.g., what type of model is being assured; the context of use for the system), the lower-level claims should further specify the goal by a) addressing the specific activities that have been carried out during the project, or b) identifying properties of the system that help ensure the goal claim is legitimate. We can, therefore, separate the type of lower-level claim being established as either a project or system property claim. Unless the claim is self-evidential, there will need to be a final element that points to some supporting evidence. This evidence establishes the foundation upon which the justifiability of the overall argument depends. The following two figures show the relationship between these elements and also provide an example of a partial assurance case that focuses on a goal of safety. === \"Figure A\" ![A minimal set of elements for an assurance case.](../../assets/images/graphics/gce.png) A minimal set of elements for an assurance case. === \"Figure B\" ![An example of a goal, claims, and evidence using the minimal set of elements identified previously.](../../assets/images/graphics/gce-example.png) An example of a goal, claims, and evidence using the minimal set of elements identified previously.","title":"Elements of an Assurance Case"},{"location":"rri/chapter4/assurance/#who-is-the-target-audience","text":"As you can probably imagine, all of these elements help play a vital role in the effective communication of an assurance case. A clear goal helps signal to stakeholders what values underwrite and motivate your project, as well as providing the means for more critical evaluation and engagement (i.e. an assessment by the stakeholders of whether your goal has been obtained, conditional on the evidence provided). The set of claims collectively establishes the scope and content of your argument, enabling stakeholders to identify whether there are any gaps (i.e. whether your argument is complete). And, the evidential base allows stakeholders to determine whether there is a legitimate reason for accepting your argument. However, the justifiability and acceptability of an assurance case, in part, depends on the target audience. Sticking with the interpretability example, we can note that what is interpretable to an expert in statistical learning theory may be completely uninterpretable for a policy-maker tasked with evaluating whether a particular model is suitable to deploy in their own project. Therefore, prior to building an assurance case, it is important to identify the target audience and understand their needs. In some cases, this may be determined on the behalf of the project team (e.g. where an external auditor requests assurance for compliance objectives). In other cases, identifying relevant stakeholders may have been performed through stakeholder engagement processes carried out as part of the ' Project Planning ' activities.","title":"Who is the Target Audience?"},{"location":"rri/chapter4/assurance/#reflect-act-justify","text":"We now have sufficient background information to present an intuitive (high-level) procedure for developing and communicating an assurance case. The procedure is broken down into three steps, which also complement the goals of the project lifecycle : Reflect Act Justify Reflection is an anticipatory and deliberative process in which questions such as the following are asked of the project and its governance: What are the goals of your system? How are these goals defined? Which stakeholders have participated in the identification and defining of these goals? What properties need to be implemented in the project or system to ensure that these goals are achieved? Which actions ought to be taken to establish these properties within the project or system? These are inherently normative and value-laden questions, which is one reason why diverse and inclusive stakeholder engagement is so crucial. Action occurs throughout all of the stages of the project lifecycle, and the output of many of these actions are likely to serve as the evidence for the claims of the assurance case. These actions and evidential artefacts can also help you identify what claims may be relevant in your argument. As such, the following questions serve only to provide some additional supporting structure to this process: What actions have been undertaken during (project) design that have generated salient evidence for your goals and claims? What actions have been undertaken during (model) development that have generated salient evidence for your goals and claims? What actions have been undertaken during (system) deployment that have generated salient evidence for your goals and claims? Using the project lifecycle model as a scaffold or guide is, therefore, a useful tool for both a) reflectively planning the necessary activities that ought to be undertaken, prior to the project's commencement, and b) evaluating and assessing whether there are any gaps as a project evolves. The final step is to justify that your evidence base is sufficient to warrant the claims that are being made about the properties of your project or system. This does not mean that the assurance case is the final activity that needs to be done at the very end of a project. Rather, it's development should be seen as iterative and ongoing as the project evolves. Identifying the relevant evidence and determining whether the evidence base is sufficient and complete can be challenging. To help this process, deliberative prompts such as the following can be instructive: Which stakeholders, identified in your stakeholder engagement plan, can support the evaluation of your evidence and overall case? Is any evidence missing from your case? Are the collection of property claims jointly sufficient to support your top-level goal? However, in general stakeholder engagement\u2014especially with domain experts\u2014is essential. In the next section, we will discuss some possible orienting goals and principles, to help you identify the sorts of properties that may comprise an ethical and responsible assurance case.","title":"Reflect, Act, Justify"},{"location":"rri/chapter4/communication/","text":"Engaging, Communicating, Assuring \u00b6 In this final chapter, we will consider what it means to communicate the processes and outputs of your research or innovation project in a responsible manner. Although we have already looked at several relevant mechanisms for communication, such as stakeholder engagement or model reporting , these mechanisms are only two pieces within a larger puzzle. They play a significant role in supporting the early parts of a project's lifecycle, but some aspects of responsible communication go beyond any single stage or even the overall lifespan of a project. Take a typical scientific research project. It is quite common for a research project to publish findings and then receive ongoing public interest for some time after the scope of the project itself. This may come in the form of engagement with policy-makers, journalists, other researchers, commerical organisations looking to apply the findings, or perhaps even citizen interest groups. Each of these groups will have a different objective or rationale that motivates their interest and engagement with a project's research process or findings. It is important to take these factors into consideration, as they can have a big impact on how a research or innovation project will be perceived. There is, of course, a limit on how much responsibility (and accountability) a project team can take for the use (or misuse) of their research by others. For example, if a commercial organisation ignores a project team's clear advice and thorough documentation about the applicability of a model to a novel context, the project team should not be blamed. But this depends on whether the original communication and engagement was exercised in a responsible manner. The commercial organisation, for instance, should not be entirely blamed if the project team gave misleading or incomplete details about their work. It can be difficult to identify when sufficient responsibility has been exercised though. After all, there is a big space between, on the one hand, publishing scientific findings in the public sphere and then assuming an entirely laissez faire approach to how the research is used and interpreted, and on the other hand, attempting to monitor every use or application of the findings to ensure that no harms arise. In this chapter, however, we will seek to provide an intuitive (albeit partial) method (or, procedure) to help address this challenge of delineating or demarcating the scope and substance of a project team's responsibility for ongoing communication and engagement. Additional Guides Before we dicuss the method, however, we should note that there is significant overlap here with the content of our two other courses: Public Engagement and Communication of Science (PES) AI Ethics and Governance (AEG) The first of these two courses (PES) goes into more detail about the scientific and ethical rationale for public engagement, and also looks at how novel technologies are shaping the communication of science. The second course explores the normative foundations of public discourse, drawing upon work in discourse ethics and jurisprudence and extending their central themes to help address challenges of governing AI technologies that operate in a global and intercultural context. The topics and themes explored in these courses develop the notion of 'responsible communication' in important ways. And, arguably, an understanding of 'responsible communication' would be incomplete without reflecting on some of the conceptual issues that the above two courses consider. However, there is a limit to how much can be covered in any single course, so we won't delve into these issues here. We encourage you to look into these courses if you're interested. We will end this guide, therefore, by exploring a unifying framework that helps draw together the themes and activities we have already explored in this course, and which also provides a foundation for responsible communication. Unlike the topics and scope of the other courses, our present concern has a more (direct) pragmatic goal: the development of an assurance case that helps communicate and justify that you have exercised responsibility throughout a research or innovation project.","title":"Assurance and Responsible Communication"},{"location":"rri/chapter4/communication/#engaging-communicating-assuring","text":"In this final chapter, we will consider what it means to communicate the processes and outputs of your research or innovation project in a responsible manner. Although we have already looked at several relevant mechanisms for communication, such as stakeholder engagement or model reporting , these mechanisms are only two pieces within a larger puzzle. They play a significant role in supporting the early parts of a project's lifecycle, but some aspects of responsible communication go beyond any single stage or even the overall lifespan of a project. Take a typical scientific research project. It is quite common for a research project to publish findings and then receive ongoing public interest for some time after the scope of the project itself. This may come in the form of engagement with policy-makers, journalists, other researchers, commerical organisations looking to apply the findings, or perhaps even citizen interest groups. Each of these groups will have a different objective or rationale that motivates their interest and engagement with a project's research process or findings. It is important to take these factors into consideration, as they can have a big impact on how a research or innovation project will be perceived. There is, of course, a limit on how much responsibility (and accountability) a project team can take for the use (or misuse) of their research by others. For example, if a commercial organisation ignores a project team's clear advice and thorough documentation about the applicability of a model to a novel context, the project team should not be blamed. But this depends on whether the original communication and engagement was exercised in a responsible manner. The commercial organisation, for instance, should not be entirely blamed if the project team gave misleading or incomplete details about their work. It can be difficult to identify when sufficient responsibility has been exercised though. After all, there is a big space between, on the one hand, publishing scientific findings in the public sphere and then assuming an entirely laissez faire approach to how the research is used and interpreted, and on the other hand, attempting to monitor every use or application of the findings to ensure that no harms arise. In this chapter, however, we will seek to provide an intuitive (albeit partial) method (or, procedure) to help address this challenge of delineating or demarcating the scope and substance of a project team's responsibility for ongoing communication and engagement. Additional Guides Before we dicuss the method, however, we should note that there is significant overlap here with the content of our two other courses: Public Engagement and Communication of Science (PES) AI Ethics and Governance (AEG) The first of these two courses (PES) goes into more detail about the scientific and ethical rationale for public engagement, and also looks at how novel technologies are shaping the communication of science. The second course explores the normative foundations of public discourse, drawing upon work in discourse ethics and jurisprudence and extending their central themes to help address challenges of governing AI technologies that operate in a global and intercultural context. The topics and themes explored in these courses develop the notion of 'responsible communication' in important ways. And, arguably, an understanding of 'responsible communication' would be incomplete without reflecting on some of the conceptual issues that the above two courses consider. However, there is a limit to how much can be covered in any single course, so we won't delve into these issues here. We encourage you to look into these courses if you're interested. We will end this guide, therefore, by exploring a unifying framework that helps draw together the themes and activities we have already explored in this course, and which also provides a foundation for responsible communication. Unlike the topics and scope of the other courses, our present concern has a more (direct) pragmatic goal: the development of an assurance case that helps communicate and justify that you have exercised responsibility throughout a research or innovation project.","title":"Engaging, Communicating, Assuring"},{"location":"rri/chapter4/gpe/","text":"Goals, Properties, and Evidence \u00b6 In the previous section, we looked at the reflect, act, and justify procedure, which was used to help develop and draw together the constitutive elements of an assurance case. To summarise, reflect-act-justify presents a simple illustration of the above process with reference to the three core elements. But how do you know which goals are relevant for specific projects? How do you use these goals to identify salient properties ? And, how do you evaluate and determine if you have sufficient evidence to support your claims and overall case? By now, you will probably find it unsurprising to hear that a reflective, inclusive, and socially embedded process of stakeholder engagement is key. Consider again the original definition of responsible research and innovation presented in Chapter 2 : \u201cResponsible Research and Innovation is a transparent, interactive process by which societal actors and innovators become mutually responsive to each other with a view on the (ethical) acceptability, sustainability and societal desirability of the innovation process and its marketable products (in order to allow a proper embedding of scientific and technological advances in our society).\u201d[@vonschomberg2011] But even though stakeholder engagement is necessary, a project team still need a springboard to help facilitate communication and deliberation with stakeholders. That is, they need a starting point, grounded in an informed understanding of the potential harms that can arise from the misuse of data science and AI. This is why we introduced the SAFE-D principles in Chapter 2 . These principles were not plucked from thin air. They are rooted in and interconnected with wide-ranging and multi-disciplinary methods of enquiry that explore such social and ethical harms and benefits (e.g., moral philosophy, human rights law, and science and technology studies), and are also enhanced through diverse modes of knowledge production (e.g., citizen science, investigative journalism, quality assurance, and activism). As such, the SAFE-D principles represent significant normative goals and valuable starting points for a process of reflection, action and justification. While it is not possible to show how they can be operationalised in every possible data science or AI project, it is possible to expand upon the goals by discussing some of their core attributes. Sustainability \u00b6 Core Attribute Description Safety Safety is core to sustainability but goes beyond the mere operational safety of the system. It also includes an understanding of the long-term use context and impact of the system, and the resources needed to ensure the system continues to operate safely over time within its environment (i.e. is sustainable). For instance, safety may depend upon sufficient change monitoring processes that establish whether there has been any substantive drift in the underlying data distributions or social operating environment. Or, it could also involve engaging and involving users and stakeholders in the design and assessment of AI systems that could impact their human rights and fundamental freedoms. Security Security encompasses the protection of several operational dimensions of an AI system when confronted with possible adversarial attack. A secure system is capable of maintaining the integrity of its constitutive information. This includes protecting its architecture from the unauthorised modification or damage of any of its component parts. A secure system also remains continuously functional and accessible to its authorised users and keeps confidential and private information secure even under hostile or adversarial conditions. Robustness The objective of robustness can be thought of as the goal that an AI system functions reliably and accurately under harsh or uncertain conditions. These conditions may include adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). The measure of robustness is, therefore, the strength of a system\u2019s functional integrity and the soundness of its operation in response to difficult conditions, adversarial attacks, perturbations, data poisoning, or undesirable reinforcement learning behaviour. Reliability The objective of reliability is that an AI system behaves exactly as its designers intended and anticipated. A reliable system adheres to the specifications it was programmed to carry out. Reliability is therefore a measure of consistency and can establish confidence in the safety of a system based upon the dependability with which it conforms to its intended functionality. Accuracy and Performance The accuracy of a model is the proportion of examples for which it generates a correct output. This performance measure is also sometimes characterised conversely as an error rate or the fraction of cases for which the model produces an incorrect output. Specifying a reasonable performance level for the system may also require refining or exchanging the measure of accuracy. For instance, if certain errors are more significant or costly than others, a metric for total cost can be integrated into the model so that the cost of one class of errors can be weighed against that of another. Accountability \u00b6 Core Attribute Description Traceability Traceability refers to the process by which all stages of the data lifecycle from collection to deployment to system updating or deprovisioning are documented in a way that is accessible and easily understood. This may include not only the parties within the organisation involved but also the actions taken at each stage that may impact the individuals who use the system. Answerability Answerability depends upon a human chain of responsibility. Answerability responds to the question of who is accountable for an automation supported outcome. Auditability Whereas the property of answerability responds to the question of who is accountable for an automation supported outcome, the notion of auditability answers the question of how the designers and implementers of AI systems are to be held accountable. This aspect of accountability has to do with demonstrating and evidencing both the responsibility of design and use practices and the justifiability of outcomes. Clear Data Provenance and Lineage Clear provenance and data lineage consists of records that are accessible and simultaneously detail how data was collected and how it has been used and altered throughout the stages of pre-processing, modelling, training, testing, and deploying. Accessibility Accessibility involves ensuring that information about the processes that took place to design, develop, and deploy an AI system are easily accessible by individuals. This not only refers to suitable means of explanation (clear, understandable, and accessible language) but also the mediums for delivery. Reproducibility Related to and dependant on the above four properties, reproducibility refers to the ability for others to reproduce the steps you have taken throughout your project to achieve the desired outcomes and where necessary to replicate the same outcomes by following the same procedure. Fairness \u00b6 Core Attribute Description Bias Mitigation It is not possible to eliminate bias entirely. However, effective bias mitigation processes can minimise the unwanted and undesirable impact of systematic deviations, distortions, or disparate outcomes that arise to a project governance problem, interfering factor, or from insufficient reflection on historical social or structural discrimination. Diversity and Inclusiveness A significant component of fairness aware design is ensuring the inclusion of diverse voices and opinions in the design and development process through the participation of a more representative range of stakeholders. This includes considering whether values of civic participation, inclusion, and diversity been adequately considered in articulating the purpose and setting the goals of the project. Consulting with internal organisational stakeholders is also necessary to strengthen the openness, inclusiveness, and diversity of the project. Non-Discrimination A system or model should not create or contribute to circumstances whereby members of protected groups are treated differently or less favourably than other groups because of their respective protected characteristic. Equality the outcome or impact of a system should either maintain or promote a state of affairs in which every individual has equal rights and liberties, and equal access or opportunities to whatever good or service the AI system brings about. Explainability \u00b6 Core Attribute Description Interpretability Interpretability consists of the ability to know how and why a model performed the way it did in a specific context and, therefore, to understand the rationale behind its decision or behaviour. Responsible Model Selection The normal expectations of intelligibility and accessibility that accompany the function of the system, as fulfilled in the sector or domain in which it will operate. This can also necessitate the availability of more interpretable algorithmic models or techniques in cases where the selection of an opaque model poses risks to the physical, psychological, or moral integrity of rights-holders or to their human rights and fundamental freedoms. The availability of the resources and capacity that will be needed to provide responsible, supplementary methods of explanation (e.g. simpler surrogate models, sensitivity analysis, or relative feature important) in cases where an opaque model is deemed appropriate and selected. Accessible Rationale Explanation The reasons that led to a decision\u2014especially one that is automated\u2014delivered in an accessible and non-technical way. Implementation and User Training Training users to operate the AI system may include: a) conveying basic knowledge about the nature of machine learning, b) explaining the limitations of the system, c) educating users about the risks of AI-related biases, such as decision-automation bias or automation-distrust bias, and d) encouraging users to view the benefits and risks of deploying these systems in terms of their role in helping humans to come to judgements, rather than replacing that judgement. Data Quality, Integrity, Protection and Privacy \u00b6 Core Attribute Description Source Integrity and Measurement Accuracy Effective bias mitigation begins at the very commencement of data extraction and collection processes. Both the sources and instruments of measurement may introduce discriminatory factors into a dataset. When incorporated as inputs in the training data, biased prior human decisions and judgments\u2014such as prejudiced scoring, ranking, interview-data or evaluation\u2014will become the \u2018ground truth\u2019 of the model and replicate the bias in the outputs of the system in order to secure discriminatory non-harm, as well as ensuring that the data sample has optimal source integrity. This involves securing or confirming that the data gathering processes involved suitable, reliable, and impartial sources of measurement and sound methods of collection. Timeliness and Recency If datasets include outdated data then changes in the underlying data distribution may adversely affect the generalisability of the trained model. Provided these distributional drifts reflect changing social relationship or group dynamics, this loss of accuracy with regard to the actual characteristics of the underlying population may introduce bias into an AI system. In preventing discriminatory outcomes, timeliness and recency of all elements of the data that constitute the datasets must be scrutinised. Relevance, Appropriateness, and Domain Knowledge The understanding and utilisation of the most appropriate sources and types of data are crucial for building a robust and unbiased AI system. Solid domain knowledge of the underlying population distribution and of the predictive or classificatory goal of the project is instrumental for choosing optimally relevant measurement inputs that contribute to the reasonable determination of the defined solution. Domain experts should collaborate closely with the technical team to assist in the determination of the optimally appropriate categories and sources of measurement. Adequacy of Quantity and Quality This property involves assessing whether the data available is comprehensive enough to address the problem set at hand, as determined by the use case, domain, function, and purpose of the system. Adequate quantity and quality should address sample size, representativeness, and availability of features relevant to problem. Balance and Representativeness A balanced and representative dataset is one in which the distribution of features that are included, and the number of samples within each class is similar to the underlying distribution that exists in the overall population. Attributable Data should clearly demonstrate who observed and recorded it, when it was observed and recorded, and who it is about. Consistent, Legible and Accurate Data should be easy to understand, recorded permanently and original entities should be preserved. Data should be free from errors and conform with the protocol. Consistency includes ensuring data is chronological (e.g., has a date and time stamp that is in the expected sequence). Complete All recorded data requires an audit trail to show nothing has been deleted or lost. Contemporaneous Data should be recorded as it was observed, and at the time it was executed. Responsible Data Management Responsible data management ensures that the team has been trained on how to manage data responsibly and securely, identifying possible risks and threats to the system and assigning roles and responsibilities for how to deal with these risks if they were to occur. Policies on data storage and public dissemination of results should be discussed within the team and with stakeholders, as well as being clearly documented. Data Traceability and Auditability Any changes or revisions to the dataset (e.g., additions, augmentations, normalisation) that occur after the original collection should be clearly traceable and well-documented to support any auditing. Consent (or legitimate basis) for processing There must be demonstrable grounds that data processing can be carried out on the basis of the free, specific, informed and unambiguous consent of the data subject or of some other legitimate basis laid down by law. The data subject must be informed of risks that could arise in the absence of appropriate safeguards. Such consent must represent the free expression of an intentional choice, given either by a statement (which can be written, including by electronic means, or oral) or by a clear affirmative action and which clearly indicates in this specific context the acceptance of the proposed processing of personal data. Mere silence, inactivity or pre-validated forms or boxes should not, therefore, constitute consent. No undue influence or pressure (which can be of an economic or other nature) whether direct or indirect, may be exercised on the data subject and consent should not be regarded as freely given where the data subject has no genuine or free choice or is unable to refuse or withdraw consent without prejudice. The data subject has the right to withdraw the consent he or she gave at any time (which is to be distinguished from the separate right to object to processing). Data Security Each Party shall provide that the controller, and, where applicable the processor, takes appropriate security measures against risks such as accidental or unauthorised access to, destruction, loss, use, modification or disclosure of personal data. Each Party shall provide that the controller notifies, without delay, at least the competent supervisory authority within the meaning of Article 15 of this Convention, of those data breaches which may seriously interfere with the rights and fundamental freedoms of data subjects. Data Minimisation Personal data being processed is adequate (sufficient to properly fulfil the stated purpose), relevant (has a rational link to that purpose), and limited to what is necessary do not hold more data than needed for that purpose). Transparency The transparency of AI systems can refer to several features, both of their inner workings and behaviours, as well as the systems and processes that support them. An AI system is transparent when it is possible to determine how it was designed, developed, and deployed. This can include, among other things, a record of the data that were used to train the system, or the parameters of the model that transforms the input (e.g., an image) into an output (e.g, a description of the objects in the image). However, it can also refer to wider processes, such as whether there are legal barriers that prevent individuals from accessing information that may be necessary to understand fully how the system functions (e.g., intellectual property restrictions). Proportionality delivering a just outcome in ways that are proportionate to the cost, complexity, and resources available. In a similar vein, the term \u2018proportionality\u2019 can also be used as an evaluative notion, such as in the case of a data protection principle that states only personal data that are necessary and adequate for the purposes of the task are collected. Purpose Limitation The purposes for data processing must be outlined and documented from the beginning and made available to all individuals through privacy information. Personal data must adhere to the original purpose unless it is compatible with the original purpose, additional consent is received, or there is an obligation or function set out in law. The above goals and corresponding list of attributes may seem daunting. However, they do not represent a checklist that all research and innovation projects must provide documentation for (e.g. an assurance case). Instead, it is best to think of them as deliberative prompts\u2014a reflect-list rather than a checklist. If, in discussion as a team and in conjunction with stakeholders, it is determined that certain attributes are irrelevant to the project for justifiable reasons, then no activities may be necessary. A principle of proportionality can certianly be adopted here as a meta-principle of sorts, helping direct project governance decisions. But, if a goal and attribute is deemed important, then the next step is to consider what system or project properties need to be established, and what evidence must be gathered and documented, in order to demonstrate that the top-level goal (and corresponding attributes) have been obtained. Here, unfortunately, we can only provide some illustrative examples that link a goal and attribute with a possible system or property claim, and also suggest where in the project lifecycle such an activity would be undertaken. As these are examples, no evidential artefact is provided. Goal & Attribute Example Property Claim Project Lifecycle Stage Sustainability (Robustness) The model used in our system has been internally and externally validated. The external validation has been carried out across several varied environments to ensure robustness of the system. Model Training, Testing and Validation Accountability (Accessibility) All identified stakeholders were consulted prior to the development of our system to help critically evaluate our project plans and ensure they were intelligible. Project Planning and Problem Formulation Fairness (Equality) Persons affected by use of the system have avenues of recourse, ability to contest system outputs and demand human intervention. System Use & Monitoring Explainability (Responsible Model Selection) Features were hand-selected in conjunction with domain experts to optimise for both interpretability and predictive power. Preprocessing & Feature Engineering and Model Selection Data Quality (Timeliness & Recency) Only data that were collected within the previous 3 months were used to ensure the training data were up-to-date. Data Extraction or Procurement In the next activity, however, we will consider what properties and evidence ought to be included for a hypothetical assurance case for the projects we have been considering.","title":"Goals, Properties, and Evidence"},{"location":"rri/chapter4/gpe/#goals-properties-and-evidence","text":"In the previous section, we looked at the reflect, act, and justify procedure, which was used to help develop and draw together the constitutive elements of an assurance case. To summarise, reflect-act-justify presents a simple illustration of the above process with reference to the three core elements. But how do you know which goals are relevant for specific projects? How do you use these goals to identify salient properties ? And, how do you evaluate and determine if you have sufficient evidence to support your claims and overall case? By now, you will probably find it unsurprising to hear that a reflective, inclusive, and socially embedded process of stakeholder engagement is key. Consider again the original definition of responsible research and innovation presented in Chapter 2 : \u201cResponsible Research and Innovation is a transparent, interactive process by which societal actors and innovators become mutually responsive to each other with a view on the (ethical) acceptability, sustainability and societal desirability of the innovation process and its marketable products (in order to allow a proper embedding of scientific and technological advances in our society).\u201d[@vonschomberg2011] But even though stakeholder engagement is necessary, a project team still need a springboard to help facilitate communication and deliberation with stakeholders. That is, they need a starting point, grounded in an informed understanding of the potential harms that can arise from the misuse of data science and AI. This is why we introduced the SAFE-D principles in Chapter 2 . These principles were not plucked from thin air. They are rooted in and interconnected with wide-ranging and multi-disciplinary methods of enquiry that explore such social and ethical harms and benefits (e.g., moral philosophy, human rights law, and science and technology studies), and are also enhanced through diverse modes of knowledge production (e.g., citizen science, investigative journalism, quality assurance, and activism). As such, the SAFE-D principles represent significant normative goals and valuable starting points for a process of reflection, action and justification. While it is not possible to show how they can be operationalised in every possible data science or AI project, it is possible to expand upon the goals by discussing some of their core attributes.","title":"Goals, Properties, and Evidence"},{"location":"rri/chapter4/gpe/#sustainability","text":"Core Attribute Description Safety Safety is core to sustainability but goes beyond the mere operational safety of the system. It also includes an understanding of the long-term use context and impact of the system, and the resources needed to ensure the system continues to operate safely over time within its environment (i.e. is sustainable). For instance, safety may depend upon sufficient change monitoring processes that establish whether there has been any substantive drift in the underlying data distributions or social operating environment. Or, it could also involve engaging and involving users and stakeholders in the design and assessment of AI systems that could impact their human rights and fundamental freedoms. Security Security encompasses the protection of several operational dimensions of an AI system when confronted with possible adversarial attack. A secure system is capable of maintaining the integrity of its constitutive information. This includes protecting its architecture from the unauthorised modification or damage of any of its component parts. A secure system also remains continuously functional and accessible to its authorised users and keeps confidential and private information secure even under hostile or adversarial conditions. Robustness The objective of robustness can be thought of as the goal that an AI system functions reliably and accurately under harsh or uncertain conditions. These conditions may include adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). The measure of robustness is, therefore, the strength of a system\u2019s functional integrity and the soundness of its operation in response to difficult conditions, adversarial attacks, perturbations, data poisoning, or undesirable reinforcement learning behaviour. Reliability The objective of reliability is that an AI system behaves exactly as its designers intended and anticipated. A reliable system adheres to the specifications it was programmed to carry out. Reliability is therefore a measure of consistency and can establish confidence in the safety of a system based upon the dependability with which it conforms to its intended functionality. Accuracy and Performance The accuracy of a model is the proportion of examples for which it generates a correct output. This performance measure is also sometimes characterised conversely as an error rate or the fraction of cases for which the model produces an incorrect output. Specifying a reasonable performance level for the system may also require refining or exchanging the measure of accuracy. For instance, if certain errors are more significant or costly than others, a metric for total cost can be integrated into the model so that the cost of one class of errors can be weighed against that of another.","title":"Sustainability"},{"location":"rri/chapter4/gpe/#accountability","text":"Core Attribute Description Traceability Traceability refers to the process by which all stages of the data lifecycle from collection to deployment to system updating or deprovisioning are documented in a way that is accessible and easily understood. This may include not only the parties within the organisation involved but also the actions taken at each stage that may impact the individuals who use the system. Answerability Answerability depends upon a human chain of responsibility. Answerability responds to the question of who is accountable for an automation supported outcome. Auditability Whereas the property of answerability responds to the question of who is accountable for an automation supported outcome, the notion of auditability answers the question of how the designers and implementers of AI systems are to be held accountable. This aspect of accountability has to do with demonstrating and evidencing both the responsibility of design and use practices and the justifiability of outcomes. Clear Data Provenance and Lineage Clear provenance and data lineage consists of records that are accessible and simultaneously detail how data was collected and how it has been used and altered throughout the stages of pre-processing, modelling, training, testing, and deploying. Accessibility Accessibility involves ensuring that information about the processes that took place to design, develop, and deploy an AI system are easily accessible by individuals. This not only refers to suitable means of explanation (clear, understandable, and accessible language) but also the mediums for delivery. Reproducibility Related to and dependant on the above four properties, reproducibility refers to the ability for others to reproduce the steps you have taken throughout your project to achieve the desired outcomes and where necessary to replicate the same outcomes by following the same procedure.","title":"Accountability"},{"location":"rri/chapter4/gpe/#fairness","text":"Core Attribute Description Bias Mitigation It is not possible to eliminate bias entirely. However, effective bias mitigation processes can minimise the unwanted and undesirable impact of systematic deviations, distortions, or disparate outcomes that arise to a project governance problem, interfering factor, or from insufficient reflection on historical social or structural discrimination. Diversity and Inclusiveness A significant component of fairness aware design is ensuring the inclusion of diverse voices and opinions in the design and development process through the participation of a more representative range of stakeholders. This includes considering whether values of civic participation, inclusion, and diversity been adequately considered in articulating the purpose and setting the goals of the project. Consulting with internal organisational stakeholders is also necessary to strengthen the openness, inclusiveness, and diversity of the project. Non-Discrimination A system or model should not create or contribute to circumstances whereby members of protected groups are treated differently or less favourably than other groups because of their respective protected characteristic. Equality the outcome or impact of a system should either maintain or promote a state of affairs in which every individual has equal rights and liberties, and equal access or opportunities to whatever good or service the AI system brings about.","title":"Fairness"},{"location":"rri/chapter4/gpe/#explainability","text":"Core Attribute Description Interpretability Interpretability consists of the ability to know how and why a model performed the way it did in a specific context and, therefore, to understand the rationale behind its decision or behaviour. Responsible Model Selection The normal expectations of intelligibility and accessibility that accompany the function of the system, as fulfilled in the sector or domain in which it will operate. This can also necessitate the availability of more interpretable algorithmic models or techniques in cases where the selection of an opaque model poses risks to the physical, psychological, or moral integrity of rights-holders or to their human rights and fundamental freedoms. The availability of the resources and capacity that will be needed to provide responsible, supplementary methods of explanation (e.g. simpler surrogate models, sensitivity analysis, or relative feature important) in cases where an opaque model is deemed appropriate and selected. Accessible Rationale Explanation The reasons that led to a decision\u2014especially one that is automated\u2014delivered in an accessible and non-technical way. Implementation and User Training Training users to operate the AI system may include: a) conveying basic knowledge about the nature of machine learning, b) explaining the limitations of the system, c) educating users about the risks of AI-related biases, such as decision-automation bias or automation-distrust bias, and d) encouraging users to view the benefits and risks of deploying these systems in terms of their role in helping humans to come to judgements, rather than replacing that judgement.","title":"Explainability"},{"location":"rri/chapter4/gpe/#data-quality-integrity-protection-and-privacy","text":"Core Attribute Description Source Integrity and Measurement Accuracy Effective bias mitigation begins at the very commencement of data extraction and collection processes. Both the sources and instruments of measurement may introduce discriminatory factors into a dataset. When incorporated as inputs in the training data, biased prior human decisions and judgments\u2014such as prejudiced scoring, ranking, interview-data or evaluation\u2014will become the \u2018ground truth\u2019 of the model and replicate the bias in the outputs of the system in order to secure discriminatory non-harm, as well as ensuring that the data sample has optimal source integrity. This involves securing or confirming that the data gathering processes involved suitable, reliable, and impartial sources of measurement and sound methods of collection. Timeliness and Recency If datasets include outdated data then changes in the underlying data distribution may adversely affect the generalisability of the trained model. Provided these distributional drifts reflect changing social relationship or group dynamics, this loss of accuracy with regard to the actual characteristics of the underlying population may introduce bias into an AI system. In preventing discriminatory outcomes, timeliness and recency of all elements of the data that constitute the datasets must be scrutinised. Relevance, Appropriateness, and Domain Knowledge The understanding and utilisation of the most appropriate sources and types of data are crucial for building a robust and unbiased AI system. Solid domain knowledge of the underlying population distribution and of the predictive or classificatory goal of the project is instrumental for choosing optimally relevant measurement inputs that contribute to the reasonable determination of the defined solution. Domain experts should collaborate closely with the technical team to assist in the determination of the optimally appropriate categories and sources of measurement. Adequacy of Quantity and Quality This property involves assessing whether the data available is comprehensive enough to address the problem set at hand, as determined by the use case, domain, function, and purpose of the system. Adequate quantity and quality should address sample size, representativeness, and availability of features relevant to problem. Balance and Representativeness A balanced and representative dataset is one in which the distribution of features that are included, and the number of samples within each class is similar to the underlying distribution that exists in the overall population. Attributable Data should clearly demonstrate who observed and recorded it, when it was observed and recorded, and who it is about. Consistent, Legible and Accurate Data should be easy to understand, recorded permanently and original entities should be preserved. Data should be free from errors and conform with the protocol. Consistency includes ensuring data is chronological (e.g., has a date and time stamp that is in the expected sequence). Complete All recorded data requires an audit trail to show nothing has been deleted or lost. Contemporaneous Data should be recorded as it was observed, and at the time it was executed. Responsible Data Management Responsible data management ensures that the team has been trained on how to manage data responsibly and securely, identifying possible risks and threats to the system and assigning roles and responsibilities for how to deal with these risks if they were to occur. Policies on data storage and public dissemination of results should be discussed within the team and with stakeholders, as well as being clearly documented. Data Traceability and Auditability Any changes or revisions to the dataset (e.g., additions, augmentations, normalisation) that occur after the original collection should be clearly traceable and well-documented to support any auditing. Consent (or legitimate basis) for processing There must be demonstrable grounds that data processing can be carried out on the basis of the free, specific, informed and unambiguous consent of the data subject or of some other legitimate basis laid down by law. The data subject must be informed of risks that could arise in the absence of appropriate safeguards. Such consent must represent the free expression of an intentional choice, given either by a statement (which can be written, including by electronic means, or oral) or by a clear affirmative action and which clearly indicates in this specific context the acceptance of the proposed processing of personal data. Mere silence, inactivity or pre-validated forms or boxes should not, therefore, constitute consent. No undue influence or pressure (which can be of an economic or other nature) whether direct or indirect, may be exercised on the data subject and consent should not be regarded as freely given where the data subject has no genuine or free choice or is unable to refuse or withdraw consent without prejudice. The data subject has the right to withdraw the consent he or she gave at any time (which is to be distinguished from the separate right to object to processing). Data Security Each Party shall provide that the controller, and, where applicable the processor, takes appropriate security measures against risks such as accidental or unauthorised access to, destruction, loss, use, modification or disclosure of personal data. Each Party shall provide that the controller notifies, without delay, at least the competent supervisory authority within the meaning of Article 15 of this Convention, of those data breaches which may seriously interfere with the rights and fundamental freedoms of data subjects. Data Minimisation Personal data being processed is adequate (sufficient to properly fulfil the stated purpose), relevant (has a rational link to that purpose), and limited to what is necessary do not hold more data than needed for that purpose). Transparency The transparency of AI systems can refer to several features, both of their inner workings and behaviours, as well as the systems and processes that support them. An AI system is transparent when it is possible to determine how it was designed, developed, and deployed. This can include, among other things, a record of the data that were used to train the system, or the parameters of the model that transforms the input (e.g., an image) into an output (e.g, a description of the objects in the image). However, it can also refer to wider processes, such as whether there are legal barriers that prevent individuals from accessing information that may be necessary to understand fully how the system functions (e.g., intellectual property restrictions). Proportionality delivering a just outcome in ways that are proportionate to the cost, complexity, and resources available. In a similar vein, the term \u2018proportionality\u2019 can also be used as an evaluative notion, such as in the case of a data protection principle that states only personal data that are necessary and adequate for the purposes of the task are collected. Purpose Limitation The purposes for data processing must be outlined and documented from the beginning and made available to all individuals through privacy information. Personal data must adhere to the original purpose unless it is compatible with the original purpose, additional consent is received, or there is an obligation or function set out in law. The above goals and corresponding list of attributes may seem daunting. However, they do not represent a checklist that all research and innovation projects must provide documentation for (e.g. an assurance case). Instead, it is best to think of them as deliberative prompts\u2014a reflect-list rather than a checklist. If, in discussion as a team and in conjunction with stakeholders, it is determined that certain attributes are irrelevant to the project for justifiable reasons, then no activities may be necessary. A principle of proportionality can certianly be adopted here as a meta-principle of sorts, helping direct project governance decisions. But, if a goal and attribute is deemed important, then the next step is to consider what system or project properties need to be established, and what evidence must be gathered and documented, in order to demonstrate that the top-level goal (and corresponding attributes) have been obtained. Here, unfortunately, we can only provide some illustrative examples that link a goal and attribute with a possible system or property claim, and also suggest where in the project lifecycle such an activity would be undertaken. As these are examples, no evidential artefact is provided. Goal & Attribute Example Property Claim Project Lifecycle Stage Sustainability (Robustness) The model used in our system has been internally and externally validated. The external validation has been carried out across several varied environments to ensure robustness of the system. Model Training, Testing and Validation Accountability (Accessibility) All identified stakeholders were consulted prior to the development of our system to help critically evaluate our project plans and ensure they were intelligible. Project Planning and Problem Formulation Fairness (Equality) Persons affected by use of the system have avenues of recourse, ability to contest system outputs and demand human intervention. System Use & Monitoring Explainability (Responsible Model Selection) Features were hand-selected in conjunction with domain experts to optimise for both interpretability and predictive power. Preprocessing & Feature Engineering and Model Selection Data Quality (Timeliness & Recency) Only data that were collected within the previous 3 months were used to ensure the training data were up-to-date. Data Extraction or Procurement In the next activity, however, we will consider what properties and evidence ought to be included for a hypothetical assurance case for the projects we have been considering.","title":"Data Quality, Integrity, Protection and Privacy"},{"location":"rri/conclusion/","text":"Illustration by [Johnny Lighthands](https://www.johnnylighthands.co.uk)) 6 Conclusion (Looking Forward) \u00b6 At the start of this guidebook, several learning objectives were presented. They were to: Understand what is meant by the term \u2018responsible research and innovation\u2019, including the motivation and historical context for its increasing relevance. Identify and evaluate the ethical issues associated with the key stages of a typical data science or AI project lifecycle: (project) design, (model) development, (system) deployment. Explore practical tools and mechanisms for operationalising the concept of \u2018responsibility\u2019 within the context of data science and AI research and innovation. Gain an appreciation of shared goals and values across scientific disciplines and research domains through dialogue with other participants. By now, you should be in a position to determine whether these objectives have been met. You should also be in a better position to critically evaluate the material that has been presented, and to ask how well it meets these objectives. I encourage you to be critical! There is no shrinking away from the fact that this guidebook, unsurprisingly, has gaps and limitations. As such, there is plenty of room for improvement. This is one of the reasons why it is linked to a GitHub repository. Hopefully, others can contribute to its development and a community of interested participants can help improve the content to make it more useful for future visitors\u2014a true commons for those that care about ensuring data science and AI work for the benefit of society. There is one limitation that is worth highlighting though. At the time of writing this (November 2021), there is a lot of research underway in an area known as 'intercultural ethics',[@evanoff2020] with a specific focus on how it applies to data science and AI. 1 Much of this research explores how different normative values are represented and accommodated\u2014or, conversely poorly represented and accommodated\u2014in the design, development, and deployment of data-driven technologies. Because many technological systems or models are deployed in global and multicultural environments, there is a significant concern about the extent to which those values embedded into a system by its developers come into tension or conflict with different communities across the globe. Moreover, as these technologies are \"data-driven\", there are also concerns about the degree to which marginalised or vulnerable communities across the globe are empowered to use or take control of how their data is managed. These issues are not well represented in the current version of this guidebook. One reason for this is simply that there are two other guidebooks\u2014one on AI Ethics & Governance, and one on Public Engagement and Communication of Science\u2014for which these topics are more directly relevant. However, while these guidebooks are better suited to dealing with these topics directly, that does not preclude this guidebook from integrating some of the lessons about intercultural ethics or data justice into the current chapters. Therefore, subsequent iterations of this guidebook will aim to engage this literature more directly. In the meantime, the 'Further Reading' section has some starting points and references for those who may be interested. Taking Responsibility \u00b6 A core focus of this guidebook has been the idea that science and technology are inextricably interconnected with society, and help shape its norms and practices. In this context, the anticipatory and reflective elements of UKRI's AR EA framework , are about looking forward into the future and trying to take responsibility for the impact of the research or innovation that you have some direct control over. Doing this, presupposes a vital ethical value that is overlooked by the AREA framework: an attitude or disposition to care for others and the society in which you arte situated. As Leslie[@leslie2020] notes, this changes the AREA framework into a CARE and Act framework. Taking responsibility, therefore, is a reflection of your values and a reflection of what you choose to care about. By taking responsibility for your research and innovation you are helping to care for society and the future we will share. Thanks for taking part \ud83d\ude4f See the special issue by Aggarwal[@aggarwal2020]. \u21a9","title":"Conclusion"},{"location":"rri/conclusion/#6-conclusion-looking-forward","text":"At the start of this guidebook, several learning objectives were presented. They were to: Understand what is meant by the term \u2018responsible research and innovation\u2019, including the motivation and historical context for its increasing relevance. Identify and evaluate the ethical issues associated with the key stages of a typical data science or AI project lifecycle: (project) design, (model) development, (system) deployment. Explore practical tools and mechanisms for operationalising the concept of \u2018responsibility\u2019 within the context of data science and AI research and innovation. Gain an appreciation of shared goals and values across scientific disciplines and research domains through dialogue with other participants. By now, you should be in a position to determine whether these objectives have been met. You should also be in a better position to critically evaluate the material that has been presented, and to ask how well it meets these objectives. I encourage you to be critical! There is no shrinking away from the fact that this guidebook, unsurprisingly, has gaps and limitations. As such, there is plenty of room for improvement. This is one of the reasons why it is linked to a GitHub repository. Hopefully, others can contribute to its development and a community of interested participants can help improve the content to make it more useful for future visitors\u2014a true commons for those that care about ensuring data science and AI work for the benefit of society. There is one limitation that is worth highlighting though. At the time of writing this (November 2021), there is a lot of research underway in an area known as 'intercultural ethics',[@evanoff2020] with a specific focus on how it applies to data science and AI. 1 Much of this research explores how different normative values are represented and accommodated\u2014or, conversely poorly represented and accommodated\u2014in the design, development, and deployment of data-driven technologies. Because many technological systems or models are deployed in global and multicultural environments, there is a significant concern about the extent to which those values embedded into a system by its developers come into tension or conflict with different communities across the globe. Moreover, as these technologies are \"data-driven\", there are also concerns about the degree to which marginalised or vulnerable communities across the globe are empowered to use or take control of how their data is managed. These issues are not well represented in the current version of this guidebook. One reason for this is simply that there are two other guidebooks\u2014one on AI Ethics & Governance, and one on Public Engagement and Communication of Science\u2014for which these topics are more directly relevant. However, while these guidebooks are better suited to dealing with these topics directly, that does not preclude this guidebook from integrating some of the lessons about intercultural ethics or data justice into the current chapters. Therefore, subsequent iterations of this guidebook will aim to engage this literature more directly. In the meantime, the 'Further Reading' section has some starting points and references for those who may be interested.","title":"6 Conclusion (Looking Forward)"},{"location":"rri/conclusion/#taking-responsibility","text":"A core focus of this guidebook has been the idea that science and technology are inextricably interconnected with society, and help shape its norms and practices. In this context, the anticipatory and reflective elements of UKRI's AR EA framework , are about looking forward into the future and trying to take responsibility for the impact of the research or innovation that you have some direct control over. Doing this, presupposes a vital ethical value that is overlooked by the AREA framework: an attitude or disposition to care for others and the society in which you arte situated. As Leslie[@leslie2020] notes, this changes the AREA framework into a CARE and Act framework. Taking responsibility, therefore, is a reflection of your values and a reflection of what you choose to care about. By taking responsibility for your research and innovation you are helping to care for society and the future we will share. Thanks for taking part \ud83d\ude4f See the special issue by Aggarwal[@aggarwal2020]. \u21a9","title":"Taking Responsibility"},{"location":"rri/intro/","text":"1 Introduction \u00b6 Summary This introduction serves to motivate the significance of some of the topics that will be explored throughout this guidebook. It starts by discussing the development of Prozac, as an illustrative example of how it can be challenging to anticipate the consequences of scientific research, before moving on to explore the concept of 'uncertainty'. Although some topics and concepts are introduced, they are not explored fully in this section. Anticipating the consequences of scientific research and technological innovation can be an intricate and formidable task\u2014one that is made all the more challenging when the target domain is a complex system such as the human brain or society. In the early 1970s, a group of scientists working at Eli Lilly\u2014a pharmaceutical company\u2014were engaged in research that would go on to have profound effects on both of these complex systems. Primarily, the group were exploring the role of serotonin in depression, and their research led to the discovery of a drug known as fluoxetine hydrochloride , which affects the reuptake of serotonin in the brain [cite wong2005]. But the introduction of the drug also had a profound effect on society as it rapidly grew in popularity under its more common name, Prozac. Prozac is a technology in the broad sense of the term, but it is not the sort of technology that this course guide will typically focus on. However, in spite of this, the discovery and development of Prozac is an interesting case to begin this guide with. Unanticipated and Unintended Consequences \u00b6 Although the scientists who were involved in the discovery and development of fluoxetine hydrochloride were confident that it could be used in the treatment of depression, the company Eli Lilly initially tested the drug as a treatment for high blood pressure and as an anti-obesity drug. Part of the reason for this was due to market analysis forecasting a limited demand for another antidepressant drug to rival the class of medications known as 'tricyclic antidepressants' that were already in use. So, as one of the discovers of Prozac admits, Quote \"it would be presumptive to claim that we anticipated the wide acceptance of Prozac by both physicians and patients. Neither did we foresee that its pale-green and light-yellow capsule would appear on the cover of Newsweek (26 March 1990), which described it as: 'A breakthrough drug for depression'.\" As we will see throughout this course, anticipating the effects of your research or innovation project is a core part of what it means to take responsibility. But the discovery of Prozac shows us that there are limits to anticipation, and that sometimes unintended consequences may arise. For instance, antidepressants such as Prozac have received a large amount of critical scrutiny that focuses on their over-prescription and overuse in society, which can often lead to alternative (non-pharmaceutical treatments) being overlooked; the patchy state of our empirical understanding regarding how they affect and alter an individual's brain; and the range of side effects, including nausea, headaches, difficulty sleeping, dizziness, fatigue, and even increased suicidal ideation\u2014the very thing that the drugs are often prescribed to help alleviate [cite spence2013]. Should the developers of Prozac have anticipated these consequences? Does taking responsibility for research and innovation require researchers and innovators to have near-omniscient levels of anticipatory capabilities, in order to ensure no unintended consequences arise? Obviously, the answer is no. Any sensible moral theory that is intended to support practical decision-making must make room for the cognitive limitations of human individuals and teams, and we cannot ignore the many positive impacts that counteract the potentially negative and unintended consequences. Returning to our example, it should be recognised that Prozac and its kin\u2014a class of drugs called selective serotonin reuptake inhibitors (SSRIs)\u2014have had a profound and positive impact on the lives of many people who suffer with depressive mood disorders. These positive (and intended) consequences can't be dismissed in any evaluation of the social and individual benefits of SSRIs. Let's take a look at some of these trade-offs as they pertain to the topic of RRI. Dealing with Uncertainty \u00b6 Perhaps the most important is the ongoing empirical uncertainty surrounding how antidepressants operate. Given the complexity of the brain and the relationship between an individual's mind and their culture and society, there is, unsurprisingly, a vast amount that is not known about the mechanisms and causal processes by which SSRIs operate. The rationale behind a lot of their use is that there is a typical (or, normal) level of serotonin reuptake and that depressive mood disorders are characterised by a deviation from this level\u2014a homeostatic conception often referred to by the label 'chemical imbalance'. As such, SSRIs are intended to correct for this imbalance by returning (and holding) the levels to a set point. But, there are also a wide variety of other interventions that work for individuals, including cognitive behavioural therapy, herbal or dietary supplements (e.g., St John's Wort), sports and leisure activities, and also better sleep\u2014something we could probably all benefit from. In the course of assessing, diagnosing, and treating depression it is common for patients to be asked to evaluate whether the impact of their depression on their day-to-day activities and relationships is worse than the potential side effects of medication. This is a value-laden decision made under uncertainty, which cannot be made by the psychiatrist on behalf of their patient. Nor could it be fully accounted for in the initial course of developing the drug\u2014individuals respond in a remarkably diverse number of ways to treatment. Rather, it can only be rationally decided on by the user of the treatment, following a process of informed consent and understanding. This brings us to another significant aspect of RRI, which we will discuss in depth during later sections\u2014 reflecting on how a research or innovation project will impact upon the lives, rights, and freedoms of users or stakeholders must be a participatory activity that is inclusive of the users or stakeholders themselves. We can assume that the development and use of SSRIs was (and is) guided by noble and beneficent intentions to help people and improve public health outcomes. But as the well-known saying goes, 'the road to hell is paved with good intentions'. Good intentions are not sufficient for acting responsibly. Let's look at an example. Cathy\u2014a data scientist with an interest in machine learning\u2014may believe that she is doing a good act by developing an automated system that uses natural language processing to secretly monitor the tweets or comments of her friends on social media and then alerts her if it detects negative language that could be indicative of depression or suicidal ideation. If it detects a message, it notifies Cathy so she can reach out to her friends and try to offer some help or support. But is she acting responsibly in doing so? Your immediate reaction is probably one of discomfort, leading you to think the answer to this question should be 'no'. Let's explore some of the relevant factors to see why this is likely the case. An Example First of all, you may think that in spite of the fact that Cathy may be trying to help, if she were to explain to her friend that she had reached out on the basis of an automated monitoring tool, they would probably feel as though she were violating an important expectation of trust by operating this tool in secrecy. Arguably, this is an outcome that she should have been able to anticipate had she reflected on the consequences of her project. Although Cathy's friend's messages and comments were made in public, they would have a reasonable expectation that Cathy would not use their posts or data for purposes that could be perceived as some form of surveillance. Had Cathy reached out to her friends and included them in her initial idea formation, they would probably have been able to save her a lot of wasted effort and prevented the unnecessary harms to their friendship. Second, let's assume that Cathy has developed this tool and hosted the code on a public GitHub repository. Her intention is to allow others to help and support their friends in the same way, and is operating under the belief that by making her work open, accessible, and transparent\u2014several principles that are commonly found in ethical frameworks\u2014she is contributing to the public good. But what if the tool becomes incredibly popular and widely used? A possible unintended consequence of this is that many users of social media begin to feel unsafe posting on platforms that they had, hitherto, used for the purpose of seeking help and advice from a supportive community? Unfortunately, Cathy did not properly reflect on how these principles would operate within the context of her project, and falsely assumed they were unconditional goods. Finally, let's pretend that Cathy has genuinely overlooked both of these consequences and, therefore, has made no plans for rectifying the potential harm her tool has caused. By now, she is unable to respond to the emergence of the unintended consequences except by apologising and removing the tool, which could have already been copied to other user's computers. It should be clear that this example is a case of irresponsible technological innovation that gives rise to several socially undesirable outcomes and harms. However, what is not necessarily clear at present is that Cathy's project can also be seen to contravene several principles of RRI: anticipation, reflexivity, inclusiveness, and responsiveness. We will explore these principles in more detail in a later section. However, this introduction has already indirectly introduced you to most of the themes, topics, and principles that are covered in this guide. In the remainder of the guide we will approach each of them in a more systematic and structured manner, taking time to discuss and explore how they can help you conduct more responsible research and innovation in data science and AI.","title":"Introduction"},{"location":"rri/intro/#1-introduction","text":"Summary This introduction serves to motivate the significance of some of the topics that will be explored throughout this guidebook. It starts by discussing the development of Prozac, as an illustrative example of how it can be challenging to anticipate the consequences of scientific research, before moving on to explore the concept of 'uncertainty'. Although some topics and concepts are introduced, they are not explored fully in this section. Anticipating the consequences of scientific research and technological innovation can be an intricate and formidable task\u2014one that is made all the more challenging when the target domain is a complex system such as the human brain or society. In the early 1970s, a group of scientists working at Eli Lilly\u2014a pharmaceutical company\u2014were engaged in research that would go on to have profound effects on both of these complex systems. Primarily, the group were exploring the role of serotonin in depression, and their research led to the discovery of a drug known as fluoxetine hydrochloride , which affects the reuptake of serotonin in the brain [cite wong2005]. But the introduction of the drug also had a profound effect on society as it rapidly grew in popularity under its more common name, Prozac. Prozac is a technology in the broad sense of the term, but it is not the sort of technology that this course guide will typically focus on. However, in spite of this, the discovery and development of Prozac is an interesting case to begin this guide with.","title":"1 Introduction"},{"location":"rri/intro/#unanticipated-and-unintended-consequences","text":"Although the scientists who were involved in the discovery and development of fluoxetine hydrochloride were confident that it could be used in the treatment of depression, the company Eli Lilly initially tested the drug as a treatment for high blood pressure and as an anti-obesity drug. Part of the reason for this was due to market analysis forecasting a limited demand for another antidepressant drug to rival the class of medications known as 'tricyclic antidepressants' that were already in use. So, as one of the discovers of Prozac admits, Quote \"it would be presumptive to claim that we anticipated the wide acceptance of Prozac by both physicians and patients. Neither did we foresee that its pale-green and light-yellow capsule would appear on the cover of Newsweek (26 March 1990), which described it as: 'A breakthrough drug for depression'.\" As we will see throughout this course, anticipating the effects of your research or innovation project is a core part of what it means to take responsibility. But the discovery of Prozac shows us that there are limits to anticipation, and that sometimes unintended consequences may arise. For instance, antidepressants such as Prozac have received a large amount of critical scrutiny that focuses on their over-prescription and overuse in society, which can often lead to alternative (non-pharmaceutical treatments) being overlooked; the patchy state of our empirical understanding regarding how they affect and alter an individual's brain; and the range of side effects, including nausea, headaches, difficulty sleeping, dizziness, fatigue, and even increased suicidal ideation\u2014the very thing that the drugs are often prescribed to help alleviate [cite spence2013]. Should the developers of Prozac have anticipated these consequences? Does taking responsibility for research and innovation require researchers and innovators to have near-omniscient levels of anticipatory capabilities, in order to ensure no unintended consequences arise? Obviously, the answer is no. Any sensible moral theory that is intended to support practical decision-making must make room for the cognitive limitations of human individuals and teams, and we cannot ignore the many positive impacts that counteract the potentially negative and unintended consequences. Returning to our example, it should be recognised that Prozac and its kin\u2014a class of drugs called selective serotonin reuptake inhibitors (SSRIs)\u2014have had a profound and positive impact on the lives of many people who suffer with depressive mood disorders. These positive (and intended) consequences can't be dismissed in any evaluation of the social and individual benefits of SSRIs. Let's take a look at some of these trade-offs as they pertain to the topic of RRI.","title":"Unanticipated and Unintended Consequences"},{"location":"rri/intro/#dealing-with-uncertainty","text":"Perhaps the most important is the ongoing empirical uncertainty surrounding how antidepressants operate. Given the complexity of the brain and the relationship between an individual's mind and their culture and society, there is, unsurprisingly, a vast amount that is not known about the mechanisms and causal processes by which SSRIs operate. The rationale behind a lot of their use is that there is a typical (or, normal) level of serotonin reuptake and that depressive mood disorders are characterised by a deviation from this level\u2014a homeostatic conception often referred to by the label 'chemical imbalance'. As such, SSRIs are intended to correct for this imbalance by returning (and holding) the levels to a set point. But, there are also a wide variety of other interventions that work for individuals, including cognitive behavioural therapy, herbal or dietary supplements (e.g., St John's Wort), sports and leisure activities, and also better sleep\u2014something we could probably all benefit from. In the course of assessing, diagnosing, and treating depression it is common for patients to be asked to evaluate whether the impact of their depression on their day-to-day activities and relationships is worse than the potential side effects of medication. This is a value-laden decision made under uncertainty, which cannot be made by the psychiatrist on behalf of their patient. Nor could it be fully accounted for in the initial course of developing the drug\u2014individuals respond in a remarkably diverse number of ways to treatment. Rather, it can only be rationally decided on by the user of the treatment, following a process of informed consent and understanding. This brings us to another significant aspect of RRI, which we will discuss in depth during later sections\u2014 reflecting on how a research or innovation project will impact upon the lives, rights, and freedoms of users or stakeholders must be a participatory activity that is inclusive of the users or stakeholders themselves. We can assume that the development and use of SSRIs was (and is) guided by noble and beneficent intentions to help people and improve public health outcomes. But as the well-known saying goes, 'the road to hell is paved with good intentions'. Good intentions are not sufficient for acting responsibly. Let's look at an example. Cathy\u2014a data scientist with an interest in machine learning\u2014may believe that she is doing a good act by developing an automated system that uses natural language processing to secretly monitor the tweets or comments of her friends on social media and then alerts her if it detects negative language that could be indicative of depression or suicidal ideation. If it detects a message, it notifies Cathy so she can reach out to her friends and try to offer some help or support. But is she acting responsibly in doing so? Your immediate reaction is probably one of discomfort, leading you to think the answer to this question should be 'no'. Let's explore some of the relevant factors to see why this is likely the case. An Example First of all, you may think that in spite of the fact that Cathy may be trying to help, if she were to explain to her friend that she had reached out on the basis of an automated monitoring tool, they would probably feel as though she were violating an important expectation of trust by operating this tool in secrecy. Arguably, this is an outcome that she should have been able to anticipate had she reflected on the consequences of her project. Although Cathy's friend's messages and comments were made in public, they would have a reasonable expectation that Cathy would not use their posts or data for purposes that could be perceived as some form of surveillance. Had Cathy reached out to her friends and included them in her initial idea formation, they would probably have been able to save her a lot of wasted effort and prevented the unnecessary harms to their friendship. Second, let's assume that Cathy has developed this tool and hosted the code on a public GitHub repository. Her intention is to allow others to help and support their friends in the same way, and is operating under the belief that by making her work open, accessible, and transparent\u2014several principles that are commonly found in ethical frameworks\u2014she is contributing to the public good. But what if the tool becomes incredibly popular and widely used? A possible unintended consequence of this is that many users of social media begin to feel unsafe posting on platforms that they had, hitherto, used for the purpose of seeking help and advice from a supportive community? Unfortunately, Cathy did not properly reflect on how these principles would operate within the context of her project, and falsely assumed they were unconditional goods. Finally, let's pretend that Cathy has genuinely overlooked both of these consequences and, therefore, has made no plans for rectifying the potential harm her tool has caused. By now, she is unable to respond to the emergence of the unintended consequences except by apologising and removing the tool, which could have already been copied to other user's computers. It should be clear that this example is a case of irresponsible technological innovation that gives rise to several socially undesirable outcomes and harms. However, what is not necessarily clear at present is that Cathy's project can also be seen to contravene several principles of RRI: anticipation, reflexivity, inclusiveness, and responsiveness. We will explore these principles in more detail in a later section. However, this introduction has already indirectly introduced you to most of the themes, topics, and principles that are covered in this guide. In the remainder of the guide we will approach each of them in a more systematic and structured manner, taking time to discuss and explore how they can help you conduct more responsible research and innovation in data science and AI.","title":"Dealing with Uncertainty"},{"location":"rri/resources/","text":"Additional Resources \u00b6 In this section, you will find the following resources: Further Reading : a list of further reading, organised by the topics covered in the guidebook. Synthetic Data Generation : a Jupyter Notebook explaining how the synthetic data were generated for the ' Data Analysis ' section in Chapter 3 .","title":"Additional Resources"},{"location":"rri/resources/#additional-resources","text":"In this section, you will find the following resources: Further Reading : a list of further reading, organised by the topics covered in the guidebook. Synthetic Data Generation : a Jupyter Notebook explaining how the synthetic data were generated for the ' Data Analysis ' section in Chapter 3 .","title":"Additional Resources"},{"location":"rri/resources/further-reading/","text":"Further Reading \u00b6 This page is still under development!","title":"Further Reading"},{"location":"rri/resources/further-reading/#further-reading","text":"This page is still under development!","title":"Further Reading"}]}